{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5febd8e2",
   "metadata": {},
   "source": [
    "# Tucker Decomposition on CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb68be",
   "metadata": {},
   "source": [
    "## 1. Phương pháp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11357f49",
   "metadata": {},
   "source": [
    "### Phương pháp 1:Tucker-2 Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67fcaa3",
   "metadata": {},
   "source": [
    "**Ý tưởng:**\n",
    "Chỉ phân rã theo hai mode kênh (input channels và output channels) — thường dùng nhất để nén convolution kernels trong CNN: giảm số channel đầu vào/đầu ra, giữ nguyên không gian (k×k).\n",
    "\n",
    "**Công thức**\n",
    "$$\n",
    "W \\in \\mathbb{R}^{k \\times k \\times C_{in} \\times C_{out}} \\approx\n",
    "G \\times_3 U^{(3)} \\times_4 U^{(4)}\n",
    "$$\n",
    "Trong đó:\n",
    "$G \\in \\mathbb{R}^{k \\times k \\times r_{in} \\times r_{out}}$,\n",
    "$U^{(3)} \\in \\mathbb{R}^{C_{in} \\times r_{in}}$, $U^{(4)} \\in \\mathbb{R}^{C_{out} \\times r_{out}}$.\n",
    "\n",
    "**Thuật toán / Triển khai (CNN):**\n",
    "\n",
    "1.Tính HOSVD/HOOI cho mode-3 và mode-4 để lấy $U^{(3)},U^{(4)}$ và core $G$.\n",
    "\n",
    "2.Thay 1 conv ($k\\times k$, $C_{in}\\to C_{out}$) bằng chuỗi:\n",
    "\n",
    "    1×1 conv: giảm $C_{in}\\to r_{in}$ (dùng $U^{(3)}$),\n",
    "\n",
    "    k×k conv: $r_{in}\\to r_{out}$ (dùng core $G$),\n",
    "\n",
    "    1×1 conv: tăng $r_{out}\\to C_{out}$ (dùng $U^{(4)}$).\n",
    "\n",
    "3.Fine-tune toàn mạng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1766fec",
   "metadata": {},
   "source": [
    "### Phương pháp 2:Full Tucker Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95aa89",
   "metadata": {},
   "source": [
    "**Ý tưởng:**\n",
    "Phân rã tensor trọng số theo cả 4 mode (spatial + input channel + output channel). Nén mạnh nhưng phức tạp hơn.\n",
    "**Công thức**\n",
    "$$\n",
    "W \\in \\mathbb{R}^{k \\times k \\times C_{in} \\times C_{out}} \\approx\n",
    "G \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)} \\times_4 U^{(4)}\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "$U^{(1)}\\in\\mathbb{R}^{k\\times r_1}, U^{(2)}\\in\\mathbb{R}^{k\\times r_2}, U^{(3)}\\in\\mathbb{R}^{C_{in}\\times r_3}, U^{(4)}\\in\\mathbb{R}^{C_{out}\\times r_4}$ và $G\\in\\mathbb{R}^{r_1\\times r_2\\times r_3\\times r_4}$.\n",
    "\n",
    "**Thuật toán / Triển khai:**\n",
    "\n",
    "Chọn ranks $(r_1,r_2,r_3,r_4)$.\n",
    "\n",
    "Tính HOSVD/HOOI để thu $U^{(i)}$ và core $G$.\n",
    "\n",
    "Thay conv gốc bằng chuỗi layer tương ứng (có thể chuyển các factor thành conv 1D/1×k/ k×1/1×1 tùy thiết kế).\n",
    "\n",
    "Fine-tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8437f",
   "metadata": {},
   "source": [
    "### Phương pháp 3:Asymmetric Tucker Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d6c8c",
   "metadata": {},
   "source": [
    "**Ý tưởng:**\n",
    "\n",
    "Trong Tucker truyền thống (Tucker-2 hoặc Full Tucker), chúng ta thường giảm cả input channels và output channels đối xứng.\n",
    "\n",
    "Nhưng thực tế, CNN layers thường có số lượng input và output channel khác nhau, và mức độ dư thừa cũng khác nhau.\n",
    "\n",
    "Asymmetric Tucker cho phép chỉ giảm ở một phía (chỉ input hoặc output) → linh hoạt hơn, ít ảnh hưởng đến accuracy.\n",
    "\n",
    "**Công thức(Nếu giảm input channels)**\n",
    "$$\n",
    "W \\in \\mathbb{R}^{k \\times k \\times C_{in} \\times C_{out}}\n",
    "\\;\\approx\\;\n",
    "G \\times_3 U^{(3)}\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "$U^{(3)} \\in \\mathbb{R}^{C_{in} \\times r_{in}}$ nén số chiều input từ $C_{in}$ xuống $r_{in}$.\n",
    "\n",
    "$G \\in \\mathbb{R}^{k \\times k \\times r_{in} \\times C_{out}}$ là core tensor.\n",
    "\n",
    "**Công thức(Nếu giảm output channels)**\n",
    "$$\n",
    "W \\;\\approx\\; G \\times_4 U^{(4)}, \\quad U^{(4)} \\in \\mathbb{R}^{C_{out} \\times r_{out}}\n",
    "$$\n",
    "\n",
    "T**riển khai trong CNN:**\n",
    "\n",
    "Nếu giảm input → thay conv layer bằng 1×1 conv (giảm $C_{in} \\to r_{in}$) rồi đến k×k conv ($r_{in} \\to C_{out}$).\n",
    "\n",
    "Nếu giảm output → thay conv layer bằng k×k conv ($C_{in} \\to r_{out}$) rồi đến 1×1 conv ($r_{out} \\to C_{out}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a713b",
   "metadata": {},
   "source": [
    "### Phương pháp 4:Data-driven Tucker (Adaptive Tucker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339019dd",
   "metadata": {},
   "source": [
    "**Ý tưởng:**\n",
    "\n",
    "Thay vì decomposition hoàn toàn dựa vào trọng số $W$ hiện tại, ta dùng dữ liệu huấn luyện để tìm decomposition tối ưu sao cho loss trên dữ liệu gốc nhỏ nhất.\n",
    "\n",
    "Tức là decomposition được học end-to-end với dữ liệu, chứ không chỉ là phân tích tuyến tính.\n",
    "\n",
    "→ Hướng nâng cao, kết hợp compression với training.\n",
    "\n",
    "**Công thức**\n",
    "$$\n",
    "\\min_{G, \\{U^{(i)}\\}} \\;\n",
    "\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}\n",
    "\\Big[ \\mathcal{L}\\big(f_{G,U}(x), y\\big) \\Big]\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "$f_{G,U}$ là mạng sau khi thay các layer gốc bằng Tucker decomposition với core $G$ và factor ${U^{(i)}}$.\n",
    "\n",
    "Ta tối ưu trực tiếp $G$ và ${U^{(i)}}$ bằng gradient descent, thay vì tính bằng HOSVD/HOOI như Tucker thường.\n",
    "\n",
    "**Triển khai trong CNN:**\n",
    "\n",
    "Khởi tạo decomposition bằng Tucker-2 hoặc Tucker đầy đủ.\n",
    "\n",
    "Chèn vào mạng → coi core $G$ và factor $U^{(i)}$ như tham số learnable.\n",
    "\n",
    "Train/fine-tune lại trên dữ liệu gốc → decomposition thích nghi với dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8b655",
   "metadata": {},
   "source": [
    "# 2.Đánh giá"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
