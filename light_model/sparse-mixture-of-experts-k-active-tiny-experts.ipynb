{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Light Model: Sparse Mixture-of-Experts (k-active tiny experts)\n\n**Mục tiêu:** Giảm **FLOPs/latency** trong khi giữ chất lượng bằng cách chỉ kích hoạt **k** chuyên gia cho mỗi token/mẫu. Ở đây ta khảo sát **3 giải thuật**:\n\n### 1) Switch-MoE (Top-1 hard routing)\n- **Ý tưởng:** với mỗi mẫu, router chọn **1 expert duy nhất** (Top-1) → cực nhẹ khi suy luận.\n- **Cơ chế:** `g = Linear(GAP(x))` → `argmax` → one-hot → chỉ tính 1 expert.  \n- **Cân bằng tải:** auxiliary loss khuyến khích phân phối chọn expert gần **đồng đều**.\n\n### 2) Noisy Top-k MoE (Shazeer-style)\n- **Ý tưởng:** router **thêm nhiễu** vào logits để tăng khám phá, rồi **Top-k** (vd. k=2), **chuẩn hoá softmax trên top-k**.\n- **Ưu điểm:** mềm dẻo hơn Top-1, thường **ổn định** hơn với dữ liệu đa dạng.\n- **Cân bằng tải:** loss cân bằng (KL/entropy) lên **trung bình gate**.\n\n### 3) Hash-Routed MoE (Fixed random router)\n- **Ý tưởng:** **không học router**; dùng **phép chiếu ngẫu nhiên** cố định để ánh xạ mẫu → expert (gần với “hash”).  \n- **Ưu điểm:** **siêu nhẹ** (không thêm params), latency thấp, dễ triển khai ở **edge**.\n- **Nhược:** router không thích nghi → độ chính xác có thể thấp hơn router học được.\n\n**Các so sánh sẽ báo cáo:** #Params, FLOPs, latency, Val@1/Val@5.  \n**Checkpoint:** `checkpoints/lm_smoe_switch.pth`, `checkpoints/lm_smoe_noisy_k2.pth`, `checkpoints/lm_smoe_hash.pth`.\n","metadata":{}},{"cell_type":"code","source":"import os, math, time, random, itertools, json\nfrom pathlib import Path\nfrom typing import Tuple, List, Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, Subset, Sampler\nfrom torchvision import datasets, transforms, models\nfrom tqdm import tqdm\n\nSEED = 1337\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Control quick demo vs fuller training\nQUICKRUN = True  # set False for better accuracy (longer runs)\n\nDATA_ROOT = \"./data\"\nCKPT_DIR = Path(\"checkpoints\")\nCKPT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Simple helpers\ndef set_seed(seed=SEED):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef count_params(model: nn.Module):\n    return sum(p.numel() for p in model.parameters())\n\ndef try_flops(model, input_size=(1,3,32,32)):\n    # Attempt FLOPs via thop; return 'N/A' if unavailable.\n    try:\n        from thop import profile\n        inp = torch.randn(*input_size).to(next(model.parameters()).device)\n        macs, params = profile(model, inputs=(inp,), verbose=False)\n        return int(macs*2)  # MACs*2 ~ FLOPs\n    except Exception as e:\n        return \"N/A\"\n\n@torch.no_grad()\ndef benchmark_latency(model, input_size=(1,3,32,32), nwarm=20, niter=50, use_jit=False):\n    model.eval()\n    x = torch.randn(*input_size).to(next(model.parameters()).device)\n    if use_jit:\n        try:\n            ts = torch.jit.trace(model, x)\n            model = ts\n        except Exception as e:\n            pass\n    # Warmup\n    for _ in range(nwarm):\n        _ = model(x)\n    # Measure\n    times = []\n    for _ in range(niter):\n        t0 = time.time()\n        _ = model(x)\n        times.append(time.time()-t0)\n    return 1000*np.mean(times)  # ms\n\n# Data transforms\nMEAN = (0.4914, 0.4822, 0.4465)\nSTD  = (0.2023, 0.1994, 0.2010)\ntrain_tf = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(MEAN, STD),\n])\nval_tf = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(MEAN, STD),\n])\n\n# CIFAR-10 dataset and splits\nfull_train = datasets.CIFAR10(DATA_ROOT, train=True, download=True, transform=train_tf)\ntest_set   = datasets.CIFAR10(DATA_ROOT, train=False, download=True, transform=val_tf)\n\n# create a small validation split from train\nindices = list(range(len(full_train)))\nrandom.shuffle(indices)\nval_ratio = 0.1\nval_count = int(len(indices)*val_ratio)\nval_idx   = indices[:val_count]\ntrain_idx = indices[val_count:]\ntrain_set = Subset(full_train, train_idx)\nval_set   = Subset(datasets.CIFAR10(DATA_ROOT, train=True, download=False, transform=val_tf), val_idx)\n\nBATCH = 128 if not QUICKRUN else 64\ntrain_loader = DataLoader(train_set, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader  = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n\nNUM_CLASSES = 10\n\ndef accuracy(logits, y, topk=(1,5)):\n    maxk = max(topk)\n    _, pred = logits.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(y.view(1,-1).expand_as(pred))\n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        res.append((correct_k.item()/y.size(0))*100.0)\n    return res  # top1, top5\n\nclass AverageMeter:\n    def __init__(self):\n        self.reset()\n    def reset(self):\n        self.sum = 0.0; self.n = 0\n    def update(self, val, n=1):\n        self.sum += val*n; self.n += n\n    @property\n    def avg(self):\n        return self.sum/max(1,self.n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:49:26.119393Z","iopub.execute_input":"2025-09-20T13:49:26.119997Z","iopub.status.idle":"2025-09-20T13:49:28.406318Z","shell.execute_reply.started":"2025-09-20T13:49:26.119965Z","shell.execute_reply":"2025-09-20T13:49:28.405505Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ========= SHARED FOR MoE =========\nimport math, random, time, numpy as np, torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pathlib import Path\n\n# ---- Env & folders ----\nif 'device' not in globals():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif 'CKPT_DIR' not in globals():\n    CKPT_DIR = Path(\"checkpoints\"); CKPT_DIR.mkdir(exist_ok=True, parents=True)\n\n# ---- Check loaders ----\nneeded = ['train_loader','val_loader','test_loader','NUM_CLASSES']\nmissing = [n for n in needed if n not in globals()]\nif missing:\n    raise RuntimeError(f\"Thiếu biến: {missing}. Hãy chạy phần setup/dataloader trước.\")\n\n# ---- Utils ----\nif 'AverageMeter' not in globals():\n    class AverageMeter:\n        def __init__(self): self.sum=0.0; self.n=0\n        def update(self,v,n=1): self.sum+=v*n; self.n+=n\n        @property\n        def avg(self): return self.sum/max(1,self.n)\n\nif 'accuracy' not in globals():\n    def accuracy(logits, y, topk=(1,5)):\n        maxk = max(topk)\n        _, pred = logits.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(y.view(1,-1).expand_as(pred))\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append((correct_k.item()/y.size(0))*100.0)\n        return res\n\nif 'try_flops' not in globals():\n    def try_flops(model, input_size=(1,3,32,32)):\n        try:\n            from thop import profile\n            dummy = torch.randn(*input_size).to(next(model.parameters()).device)\n            macs,_ = profile(model, inputs=(dummy,), verbose=False)\n            return int(macs*2)\n        except Exception:\n            return \"N/A\"\n\nif 'benchmark_latency' not in globals():\n    @torch.no_grad()\n    def benchmark_latency(model, input_size=(1,3,32,32), nwarm=10, niter=30):\n        model.eval()\n        x = torch.randn(*input_size).to(next(model.parameters()).device)\n        for _ in range(nwarm): _ = model(x)\n        ts=[]\n        for _ in range(niter):\n            t0=time.time(); _=model(x); ts.append(time.time()-t0)\n        return 1000*np.mean(ts)\n\n@torch.no_grad()\ndef eval_topk(model, loader, topk=(1,5)):\n    model.eval()\n    n, t1_sum, t5_sum = 0, 0, 0\n    for x,y in loader:\n        x,y = x.to(device), y.to(device)\n        out = model(x)\n        logits = out[0] if isinstance(out, tuple) else out\n        _, pred = logits.topk(max(topk), 1, True, True)\n        correct = pred.eq(y.view(-1,1))\n        n += y.size(0)\n        t1_sum += correct[:, :1].sum().item()\n        t5_sum += correct[:, :5].sum().item()\n    return 100.0*t1_sum/n, 100.0*t5_sum/n\n\ndef train_moe(model, label, epochs=10, lr=0.1, lb_weight=0.02, print_every=1):\n    \"\"\"Train + log mỗi epoch (Train loss/@k, Val@k, Test@k). Save best theo Val@1.\"\"\"\n    model = model.to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n    best_val = -1.0\n    ckpt = CKPT_DIR/f\"{label}.pth\"\n\n    print(f\"[Start] {label} | epochs={epochs} lr={lr} lb_w={lb_weight}\")\n    for ep in range(1, epochs+1):\n        # ---- Train ----\n        model.train()\n        loss_meter = AverageMeter()\n        tr_t1, tr_t5 = AverageMeter(), AverageMeter()\n        for x,y in train_loader:\n            x,y = x.to(device), y.to(device)\n            opt.zero_grad()\n            out = model(x)\n            logits, lb = out if isinstance(out, tuple) else (out, None)\n            ce = F.cross_entropy(logits, y)\n            loss = ce + (lb_weight*lb if lb is not None else 0.0)\n            loss.backward(); opt.step()\n            t1,t5 = accuracy(logits.detach(), y, topk=(1,5))\n            loss_meter.update(loss.item(), x.size(0))\n            tr_t1.update(t1, x.size(0)); tr_t5.update(t5, x.size(0))\n        sch.step()\n\n        # ---- Val & Test ----\n        val_t1, val_t5 = eval_topk(model, val_loader, topk=(1,5))\n        test_t1, test_t5 = eval_topk(model, test_loader, topk=(1,5))\n\n        # ---- Save best on Val@1 ----\n        is_best = val_t1 > best_val\n        if is_best:\n            best_val = val_t1\n            torch.save({\"model_state\": model.state_dict(),\n                        \"meta\":{\"label\":label,\"val_top1\":best_val}}, ckpt)\n\n        if ep % print_every == 0:\n            print(f\"[{label}] Ep {ep:03d}/{epochs} | \"\n                  f\"TrainLoss {loss_meter.avg:.4f} | Train@1 {tr_t1.avg:.2f} | Train@5 {tr_t5.avg:.2f} || \"\n                  f\"Val@1 {val_t1:.2f} | Val@5 {val_t5:.2f} || \"\n                  f\"Test@1 {test_t1:.2f} | Test@5 {test_t5:.2f} || \"\n                  f\"Best Val@1 {best_val:.2f} {'*' if is_best else ''}\")\n\n    # ---- Summary ----\n    params = sum(p.numel() for p in model.parameters())\n    flops  = try_flops(model)\n    lat    = benchmark_latency(model, niter=20)\n    print(f\"[Done:{label}] Params:{params:,} | FLOPs:{flops} | Lat(ms):{lat:.2f} | Best Val@1:{best_val:.2f} | ckpt={ckpt}\")\n    return ckpt\n\n# ---- Tiny expert shared by all MoE variants ----\nclass DWConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.dw = nn.Conv2d(in_ch, in_ch, 3, padding=1, groups=in_ch, bias=False)\n        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n        self.bn = nn.BatchNorm2d(out_ch)\n        self.act = nn.ReLU(inplace=True)\n    def forward(self, x): return self.act(self.bn(self.pw(self.dw(x))))\n\nclass TinyExpert(nn.Module):\n    def __init__(self, ch):\n        super().__init__()\n        self.block = nn.Sequential(DWConvBlock(ch, ch), DWConvBlock(ch, ch))\n    def forward(self, x): return self.block(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:49:28.407814Z","iopub.execute_input":"2025-09-20T13:49:28.408131Z","iopub.status.idle":"2025-09-20T13:49:28.432227Z","shell.execute_reply.started":"2025-09-20T13:49:28.408103Z","shell.execute_reply":"2025-09-20T13:49:28.431519Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"\n\n## Ký hiệu\n\n* Số expert: $E$.\n* Số expert hoạt động/token: $k \\le E$.\n* Chiều ẩn đầu vào: $d$.\n* Chiều ẩn FFN của **mỗi expert**: $d_{\\text{ff}}$.\n* Ma trận router: $W_g \\in \\mathbb{R}^{d \\times E}$, bias $b_g \\in \\mathbb{R}^{E}$.\n* Đầu vào một token: $x \\in \\mathbb{R}^{d}$.\n* Logits router: $z = W_g^\\top x + b_g \\in \\mathbb{R}^{E}$.\n* Softmax: $\\operatorname{softmax}(z)_i = \\dfrac{e^{z_i}}{\\sum_{j=1}^E e^{z_j}}$.\n* Expert $i$: hàm $F_i: \\mathbb{R}^{d} \\to \\mathbb{R}^{d}$ (ví dụ FFN hai lớp).\n* Hệ số capacity: $c \\ge 1$; **capacity/ expert**:  \n  $$\n  C = \\left\\lceil c \\cdot \\frac{N_{\\text{tokens}} \\cdot k}{E} \\right\\rceil.\n  $$\n\n**MoE layer (dạng tổng quát).** Gọi $S \\in \\{0,1\\}^{N_{\\text{tokens}} \\times E}$ là ma trận **dispatch** (token→expert) và $G \\in \\mathbb{R}_{\\ge 0}^{N_{\\text{tokens}} \\times E}$ là ma trận **kết hợp** (weight sau gate, chỉ khác 0 ở top-$k$). Với token $t$:\n\n$$\n\\tilde{y}_t = \\sum_{i=1}^{E} S_{t,i} \\cdot F_i(x_t), \\quad \ny_t = x_t + \\sum_{i=1}^E G_{t,i} \\cdot F_i(x_t).\n$$\n\nRàng buộc capacity: mỗi cột $i$ của $S$ có nhiều nhất $C$ phần tử bằng 1 (token vượt ngưỡng có thể **drop** hoặc đi qua **residual**).\n\n---\n\n## 1) Switch-MoE (Top-1, hard routing)\n\n**Router.** Tính $z = W_g^\\top x + b_g$. Với nhiệt độ $\\tau>0$:\n\n$$\n\\pi = \\operatorname{softmax}(z/\\tau), \\quad \ni^* = \\operatorname*{argmax}_{i \\in [E]} z_i, \\quad \nS_{t,i} = \\mathbb{1}[i=i^*].\n$$\n\n**Kết hợp.** $k=1$ nên $G_{t,i}=\\mathbb{1}[i=i^*]$ (hoặc $G_{t,i}=\\pi_i \\cdot \\mathbb{1}[i=i^*]$ nếu muốn scale theo xác suất).\n\n**Đầu ra.** $y_t = x_t + F_{i^*}(x_t)$.\n\n**Auxiliary load-balancing loss.** Gọi:\n\n* $\\text{load}_i = \\dfrac{1}{N_{\\text{tokens}}} \\sum_t \\mathbb{1}[i=i^*(t)]$ (tỉ lệ **token** định tuyến tới expert $i$),\n* $\\text{imp}_i = \\dfrac{1}{N_{\\text{tokens}}} \\sum_t \\pi_i(t)$ (tỉ lệ **khối xác suất** router dành cho expert $i$).\n\nHàm phạt cân bằng (dạng Switch) với hệ số $\\lambda_{\\text{bal}}$:\n\n$$\n\\mathcal{L}_{\\text{bal}} = \\lambda_{\\text{bal}} \\cdot E \\cdot \\sum_{i=1}^{E} \\text{load}_i \\, \\text{imp}_i.\n$$\n\n**Z-loss (ổn định router).**\n\n$$\n\\mathcal{L}_{z} = \\lambda_{z} \\cdot \\frac{1}{N_{\\text{tokens}}} \\sum_t \\big( \\operatorname{logsumexp}(z(t)) \\big)^2.\n$$\n\n---\n\n## Loss tổng\n\n$$\n\\mathcal{L} = \\mathcal{L}_{\\text{task}} + \\mathcal{L}_{\\text{bal}} + \\mathcal{L}_{z}.\n$$\n\n---\n\n## Đo đạc & đánh giá\n\n* **#Params**: số tham số.\n* **FLOPs**: /token hoặc /chuỗi.\n* **Latency**: p50/p95/p99 trên phần cứng.\n* **Val@1 / Val@5**:\n\n$$\n\\text{Top-1} = \\frac{1}{N}\\sum_{n} \\mathbb{1}[y_n^{\\text{pred}} = y_n^{\\text{true}}], \\quad\n\\text{Top-5} = \\frac{1}{N}\\sum_{n} \\mathbb{1}[y_n^{\\text{true}} \\in \\text{Top5}(p_n)].\n$$\n\n* **Expert utilization entropy**:\n\n$$\nH(u) = -\\sum_{i=1}^E u_i \\log u_i.\n$$\n\n---\n\n## Checkpoints\n\n* `checkpoints/lm_smoe_switch.pth`\n* `checkpoints/lm_smoe_noisy_k2.pth`\n* `checkpoints/lm_smoe_hash.pth`\n","metadata":{}},{"cell_type":"code","source":"# ========= SWITCH-MoE =========\nclass SwitchMoE(nn.Module):\n    def __init__(self, channels, num_experts=4, use_load_balance=True):\n        super().__init__()\n        self.E = num_experts\n        self.use_lb = use_load_balance\n        self.experts = nn.ModuleList([TinyExpert(channels) for _ in range(self.E)])\n        self.gate = nn.Linear(channels, self.E)\n\n    def forward(self, x):\n        B,C,H,W = x.shape\n        gap = F.adaptive_avg_pool2d(x,1).view(B,C)\n        probs = F.softmax(self.gate(gap), dim=1)   # [B,E]\n        top1 = probs.argmax(dim=1)                 # [B]\n        onehot = F.one_hot(top1, num_classes=self.E).float()  # [B,E]\n        out = 0.0\n        for e_id, expert in enumerate(self.experts):\n            out = out + expert(x) * onehot[:, e_id].view(B,1,1,1)\n        lb = None\n        if self.use_lb and self.training:\n            mean_usage = probs.mean(dim=0)\n            prior = torch.full_like(mean_usage, 1.0/self.E)\n            lb = F.kl_div(mean_usage.log(), prior, reduction=\"batchmean\")\n        return out, lb\n\nclass TinyCNN_SwitchMoE(nn.Module):\n    def __init__(self, num_classes=10, base_ch=32, num_experts=4, use_lb=True):\n        super().__init__()\n        self.stem = nn.Sequential(nn.Conv2d(3, base_ch, 3, padding=1, bias=False),\n                                  nn.BatchNorm2d(base_ch), nn.ReLU(True))\n        self.m1 = SwitchMoE(base_ch, num_experts=num_experts, use_load_balance=use_lb)\n        self.down = nn.Conv2d(base_ch, base_ch*2, 3, 2, 1)\n        self.m2 = SwitchMoE(base_ch*2, num_experts=num_experts, use_load_balance=use_lb)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(base_ch*2, num_classes)\n    def forward(self, x):\n        x, lb_sum = self.stem(x), 0.0\n        x, lb1 = self.m1(x); x = F.relu(self.down(x))\n        x, lb2 = self.m2(x)\n        if lb1 is not None: lb_sum += lb1\n        if lb2 is not None: lb_sum += lb2\n        x = self.pool(x).flatten(1); logits = self.fc(x)\n        return logits, (lb_sum if lb_sum!=0.0 else None)\n\n# ---- Train Switch-MoE ----\nEPOCHS = 30\nmodel_sw = TinyCNN_SwitchMoE(num_classes=NUM_CLASSES, base_ch=(24 if 'QUICKRUN' in globals() and QUICKRUN else 32),\n                             num_experts=4, use_lb=True)\nckpt_switch = train_moe(model_sw, label=\"lm_smoe_switch\",\n                        epochs=EPOCHS, lr=(0.05 if 'QUICKRUN' in globals() and QUICKRUN else 0.1), lb_weight=0.02)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:49:28.433179Z","iopub.execute_input":"2025-09-20T13:49:28.433462Z","iopub.status.idle":"2025-09-20T13:58:03.040898Z","shell.execute_reply.started":"2025-09-20T13:49:28.433436Z","shell.execute_reply":"2025-09-20T13:58:03.040152Z"}},"outputs":[{"name":"stdout","text":"[Start] lm_smoe_switch | epochs=30 lr=0.05 lb_w=0.02\n[lm_smoe_switch] Ep 001/30 | TrainLoss 1.8189 | Train@1 30.29 | Train@5 84.26 || Val@1 36.60 | Val@5 89.12 || Test@1 37.27 | Test@5 89.39 || Best Val@1 36.60 *\n[lm_smoe_switch] Ep 002/30 | TrainLoss 1.6079 | Train@1 39.44 | Train@5 90.01 || Val@1 39.36 | Val@5 89.92 || Test@1 38.43 | Test@5 90.41 || Best Val@1 39.36 *\n[lm_smoe_switch] Ep 003/30 | TrainLoss 1.4892 | Train@1 44.81 | Train@5 91.99 || Val@1 42.62 | Val@5 92.30 || Test@1 43.43 | Test@5 92.14 || Best Val@1 42.62 *\n[lm_smoe_switch] Ep 004/30 | TrainLoss 1.3850 | Train@1 49.21 | Train@5 93.24 || Val@1 48.98 | Val@5 93.02 || Test@1 48.12 | Test@5 92.87 || Best Val@1 48.98 *\n[lm_smoe_switch] Ep 005/30 | TrainLoss 1.3239 | Train@1 52.10 | Train@5 93.80 || Val@1 48.18 | Val@5 93.54 || Test@1 48.72 | Test@5 93.95 || Best Val@1 48.98 \n[lm_smoe_switch] Ep 006/30 | TrainLoss 1.2745 | Train@1 54.01 | Train@5 94.09 || Val@1 43.92 | Val@5 90.04 || Test@1 44.55 | Test@5 89.86 || Best Val@1 48.98 \n[lm_smoe_switch] Ep 007/30 | TrainLoss 1.2215 | Train@1 56.19 | Train@5 94.83 || Val@1 52.18 | Val@5 91.74 || Test@1 51.28 | Test@5 91.99 || Best Val@1 52.18 *\n[lm_smoe_switch] Ep 008/30 | TrainLoss 1.1975 | Train@1 57.02 | Train@5 95.19 || Val@1 47.80 | Val@5 92.34 || Test@1 47.77 | Test@5 91.96 || Best Val@1 52.18 \n[lm_smoe_switch] Ep 009/30 | TrainLoss 1.1474 | Train@1 58.93 | Train@5 95.72 || Val@1 58.40 | Val@5 94.56 || Test@1 58.48 | Test@5 94.65 || Best Val@1 58.40 *\n[lm_smoe_switch] Ep 010/30 | TrainLoss 1.1243 | Train@1 59.50 | Train@5 95.81 || Val@1 56.30 | Val@5 92.92 || Test@1 55.24 | Test@5 92.79 || Best Val@1 58.40 \n[lm_smoe_switch] Ep 011/30 | TrainLoss 1.0845 | Train@1 61.40 | Train@5 96.24 || Val@1 58.34 | Val@5 95.54 || Test@1 58.27 | Test@5 95.67 || Best Val@1 58.40 \n[lm_smoe_switch] Ep 012/30 | TrainLoss 1.0669 | Train@1 62.09 | Train@5 96.34 || Val@1 61.38 | Val@5 96.02 || Test@1 61.34 | Test@5 96.11 || Best Val@1 61.38 *\n[lm_smoe_switch] Ep 013/30 | TrainLoss 1.0446 | Train@1 62.84 | Train@5 96.39 || Val@1 60.08 | Val@5 95.56 || Test@1 58.81 | Test@5 95.94 || Best Val@1 61.38 \n[lm_smoe_switch] Ep 014/30 | TrainLoss 1.0197 | Train@1 63.94 | Train@5 96.60 || Val@1 52.80 | Val@5 93.18 || Test@1 52.27 | Test@5 93.81 || Best Val@1 61.38 \n[lm_smoe_switch] Ep 015/30 | TrainLoss 0.9991 | Train@1 64.60 | Train@5 96.69 || Val@1 57.20 | Val@5 95.56 || Test@1 57.79 | Test@5 95.55 || Best Val@1 61.38 \n[lm_smoe_switch] Ep 016/30 | TrainLoss 0.9710 | Train@1 65.58 | Train@5 96.94 || Val@1 61.30 | Val@5 96.10 || Test@1 61.32 | Test@5 96.58 || Best Val@1 61.38 \n[lm_smoe_switch] Ep 017/30 | TrainLoss 0.9538 | Train@1 66.17 | Train@5 97.14 || Val@1 61.00 | Val@5 96.48 || Test@1 60.92 | Test@5 96.10 || Best Val@1 61.38 \n[lm_smoe_switch] Ep 018/30 | TrainLoss 0.9224 | Train@1 67.50 | Train@5 97.32 || Val@1 65.08 | Val@5 97.24 || Test@1 64.17 | Test@5 97.05 || Best Val@1 65.08 *\n[lm_smoe_switch] Ep 019/30 | TrainLoss 0.9040 | Train@1 67.99 | Train@5 97.48 || Val@1 63.20 | Val@5 96.48 || Test@1 63.33 | Test@5 96.48 || Best Val@1 65.08 \n[lm_smoe_switch] Ep 020/30 | TrainLoss 0.8786 | Train@1 69.09 | Train@5 97.60 || Val@1 64.86 | Val@5 97.02 || Test@1 64.59 | Test@5 96.95 || Best Val@1 65.08 \n[lm_smoe_switch] Ep 021/30 | TrainLoss 0.8630 | Train@1 69.70 | Train@5 97.73 || Val@1 66.56 | Val@5 97.44 || Test@1 67.36 | Test@5 97.51 || Best Val@1 66.56 *\n[lm_smoe_switch] Ep 022/30 | TrainLoss 0.8292 | Train@1 70.69 | Train@5 97.90 || Val@1 69.32 | Val@5 97.50 || Test@1 68.71 | Test@5 97.66 || Best Val@1 69.32 *\n[lm_smoe_switch] Ep 023/30 | TrainLoss 0.8138 | Train@1 71.29 | Train@5 98.00 || Val@1 69.20 | Val@5 97.90 || Test@1 69.32 | Test@5 97.82 || Best Val@1 69.32 \n[lm_smoe_switch] Ep 024/30 | TrainLoss 0.7809 | Train@1 72.44 | Train@5 98.18 || Val@1 70.94 | Val@5 98.30 || Test@1 70.80 | Test@5 98.12 || Best Val@1 70.94 *\n[lm_smoe_switch] Ep 025/30 | TrainLoss 0.7562 | Train@1 73.67 | Train@5 98.21 || Val@1 70.94 | Val@5 98.08 || Test@1 71.43 | Test@5 98.10 || Best Val@1 70.94 \n[lm_smoe_switch] Ep 026/30 | TrainLoss 0.7393 | Train@1 74.10 | Train@5 98.38 || Val@1 71.92 | Val@5 97.96 || Test@1 72.04 | Test@5 98.22 || Best Val@1 71.92 *\n[lm_smoe_switch] Ep 027/30 | TrainLoss 0.7202 | Train@1 74.66 | Train@5 98.45 || Val@1 72.78 | Val@5 98.04 || Test@1 73.00 | Test@5 98.16 || Best Val@1 72.78 *\n[lm_smoe_switch] Ep 028/30 | TrainLoss 0.7013 | Train@1 75.49 | Train@5 98.59 || Val@1 73.80 | Val@5 98.24 || Test@1 73.86 | Test@5 98.27 || Best Val@1 73.80 *\n[lm_smoe_switch] Ep 029/30 | TrainLoss 0.6916 | Train@1 75.79 | Train@5 98.63 || Val@1 73.78 | Val@5 98.40 || Test@1 74.26 | Test@5 98.25 || Best Val@1 73.80 \n[lm_smoe_switch] Ep 030/30 | TrainLoss 0.6846 | Train@1 76.28 | Train@5 98.66 || Val@1 73.90 | Val@5 98.40 || Test@1 74.29 | Test@5 98.29 || Best Val@1 73.90 *\n[Done:lm_smoe_switch] Params:41,274 | FLOPs:N/A | Lat(ms):3.26 | Best Val@1:73.90 | ckpt=checkpoints/lm_smoe_switch.pth\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"---\n\n## 2) Noisy Top-$k$ MoE (kiểu Shazeer)\n\n**Router có nhiễu.** Với $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$:\n\n$$\n\\tilde{z} = z + \\epsilon, \\quad \n\\mathcal{I}_k = \\operatorname{TopK}(\\tilde{z}, k).\n$$\n\n**Chuẩn hoá trên top-$k$.** Với nhiệt độ $\\tau$:\n\n$$\nG_{t,i} = \\begin{cases}\n\\dfrac{\\exp(\\tilde{z}_i/\\tau)}{\\sum_{j\\in \\mathcal{I}_k} \\exp(\\tilde{z}_j/\\tau)} & i\\in \\mathcal{I}_k, \\\\\n0 & \\text{ngược lại.}\n\\end{cases}\n$$\n\n**Dispatch.** $S_{t,i}=\\mathbb{1}[i\\in \\mathcal{I}_k]$ (sau khi áp dụng capacity $C$).\n\n**Đầu ra.**\n\n$$\ny_t = x_t + \\sum_{i\\in\\mathcal{I}_k} G_{t,i} \\, F_i(x_t).\n$$\n\n**Cân bằng tải.** Với\n\n$$\n\\text{load}_i = \\frac{1}{N_{\\text{tokens}}}\\sum_t \\mathbb{1}[i\\in\\mathcal{I}_k(t)], \\quad\n\\text{imp}_i = \\frac{1}{N_{\\text{tokens}}}\\sum_t G_{t,i},\n$$\n\nđặt\n\n$$\n\\mathcal{L}_{\\text{bal}} = \\lambda_{\\text{bal}} \\cdot E \\cdot \\sum_{i=1}^{E} \\text{load}_i \\, \\text{imp}_i.\n$$\n\n---","metadata":{}},{"cell_type":"code","source":"# ========= NOISY TOP-k MoE =========\nclass NoisyTopKMoE(nn.Module):\n    def __init__(self, channels, num_experts=8, k=2, noise_std=1.0, use_load_balance=True):\n        super().__init__()\n        assert 1 <= k <= num_experts\n        self.E, self.k, self.noise_std = num_experts, k, noise_std\n        self.use_lb = use_load_balance\n        self.experts = nn.ModuleList([TinyExpert(channels) for _ in range(self.E)])\n        self.gate = nn.Linear(channels, self.E)\n\n    def forward(self, x):\n        B,C,H,W = x.shape\n        gap = F.adaptive_avg_pool2d(x,1).view(B,C)\n        logits = self.gate(gap)\n        if self.training and self.noise_std>0:\n            logits = logits + torch.randn_like(logits)*self.noise_std\n        vals, idx = torch.topk(logits, self.k, dim=1)      # [B,k]\n        w = F.softmax(vals, dim=1)                         # [B,k]\n        out = 0.0\n        for i in range(self.k):\n            pick = idx[:, i]\n            contrib = 0.0\n            for e_id, expert in enumerate(self.experts):\n                contrib = contrib + expert(x) * (pick==e_id).float().view(B,1,1,1)\n            out = out + contrib * w[:, i].view(B,1,1,1)\n        lb = None\n        if self.use_lb and self.training:\n            probs = F.softmax(logits, dim=1)\n            mean_usage = probs.mean(dim=0)\n            prior = torch.full_like(mean_usage, 1.0/self.E)\n            lb = F.kl_div(mean_usage.log(), prior, reduction=\"batchmean\")\n        return out, lb\n\nclass TinyCNN_NoisyTopK(nn.Module):\n    def __init__(self, num_classes=10, base_ch=32, num_experts=8, k=2, noise_std=1.0, use_lb=True):\n        super().__init__()\n        self.stem = nn.Sequential(nn.Conv2d(3, base_ch, 3, padding=1, bias=False),\n                                  nn.BatchNorm2d(base_ch), nn.ReLU(True))\n        self.m1 = NoisyTopKMoE(base_ch, num_experts=num_experts, k=k, noise_std=noise_std, use_load_balance=use_lb)\n        self.down = nn.Conv2d(base_ch, base_ch*2, 3, 2, 1)\n        self.m2 = NoisyTopKMoE(base_ch*2, num_experts=num_experts, k=k, noise_std=noise_std, use_load_balance=use_lb)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(base_ch*2, num_classes)\n    def forward(self, x):\n        x, lb_sum = self.stem(x), 0.0\n        x, lb1 = self.m1(x); x = F.relu(self.down(x))\n        x, lb2 = self.m2(x)\n        if lb1 is not None: lb_sum += lb1\n        if lb2 is not None: lb_sum += lb2\n        x = self.pool(x).flatten(1); logits = self.fc(x)\n        return logits, (lb_sum if lb_sum!=0.0 else None)\n\n# ---- Train Noisy Top-k ----\nmodel_nk = TinyCNN_NoisyTopK(num_classes=NUM_CLASSES, base_ch=(24 if 'QUICKRUN' in globals() and QUICKRUN else 32),\n                             num_experts=8, k=2, noise_std=1.0, use_lb=True)\nckpt_noisy = train_moe(model_nk, label=\"lm_smoe_noisy_k2\",\n                       epochs=EPOCHS, lr=(0.05 if 'QUICKRUN' in globals() and QUICKRUN else 0.1), lb_weight=0.02)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:58:03.042706Z","iopub.execute_input":"2025-09-20T13:58:03.042976Z","iopub.status.idle":"2025-09-20T14:23:53.900163Z","shell.execute_reply.started":"2025-09-20T13:58:03.042953Z","shell.execute_reply":"2025-09-20T14:23:53.899427Z"}},"outputs":[{"name":"stdout","text":"[Start] lm_smoe_noisy_k2 | epochs=30 lr=0.05 lb_w=0.02\n[lm_smoe_noisy_k2] Ep 001/30 | TrainLoss 1.8226 | Train@1 29.93 | Train@5 83.62 || Val@1 31.92 | Val@5 85.42 || Test@1 32.63 | Test@5 86.77 || Best Val@1 31.92 *\n[lm_smoe_noisy_k2] Ep 002/30 | TrainLoss 1.4227 | Train@1 48.04 | Train@5 92.99 || Val@1 47.90 | Val@5 94.08 || Test@1 48.02 | Test@5 93.59 || Best Val@1 47.90 *\n[lm_smoe_noisy_k2] Ep 003/30 | TrainLoss 1.2050 | Train@1 57.15 | Train@5 95.31 || Val@1 54.64 | Val@5 95.46 || Test@1 54.39 | Test@5 95.38 || Best Val@1 54.64 *\n[lm_smoe_noisy_k2] Ep 004/30 | TrainLoss 1.1046 | Train@1 60.89 | Train@5 96.08 || Val@1 55.08 | Val@5 92.10 || Test@1 55.16 | Test@5 92.27 || Best Val@1 55.08 *\n[lm_smoe_noisy_k2] Ep 005/30 | TrainLoss 1.0425 | Train@1 63.23 | Train@5 96.67 || Val@1 63.30 | Val@5 96.62 || Test@1 63.99 | Test@5 97.07 || Best Val@1 63.30 *\n[lm_smoe_noisy_k2] Ep 006/30 | TrainLoss 0.9905 | Train@1 65.40 | Train@5 97.00 || Val@1 62.36 | Val@5 96.74 || Test@1 62.04 | Test@5 96.78 || Best Val@1 63.30 \n[lm_smoe_noisy_k2] Ep 007/30 | TrainLoss 0.9604 | Train@1 66.86 | Train@5 97.14 || Val@1 61.24 | Val@5 96.26 || Test@1 60.99 | Test@5 96.58 || Best Val@1 63.30 \n[lm_smoe_noisy_k2] Ep 008/30 | TrainLoss 0.9336 | Train@1 67.63 | Train@5 97.37 || Val@1 62.10 | Val@5 96.64 || Test@1 62.17 | Test@5 97.04 || Best Val@1 63.30 \n[lm_smoe_noisy_k2] Ep 009/30 | TrainLoss 0.9093 | Train@1 68.59 | Train@5 97.64 || Val@1 68.36 | Val@5 97.26 || Test@1 68.19 | Test@5 97.59 || Best Val@1 68.36 *\n[lm_smoe_noisy_k2] Ep 010/30 | TrainLoss 0.8800 | Train@1 69.61 | Train@5 97.72 || Val@1 68.08 | Val@5 97.88 || Test@1 68.83 | Test@5 97.89 || Best Val@1 68.36 \n[lm_smoe_noisy_k2] Ep 011/30 | TrainLoss 0.8677 | Train@1 70.22 | Train@5 97.65 || Val@1 63.02 | Val@5 96.76 || Test@1 63.16 | Test@5 97.20 || Best Val@1 68.36 \n[lm_smoe_noisy_k2] Ep 012/30 | TrainLoss 0.8479 | Train@1 70.82 | Train@5 97.91 || Val@1 57.76 | Val@5 97.32 || Test@1 57.94 | Test@5 98.06 || Best Val@1 68.36 \n[lm_smoe_noisy_k2] Ep 013/30 | TrainLoss 0.8303 | Train@1 71.43 | Train@5 97.97 || Val@1 66.82 | Val@5 96.74 || Test@1 66.88 | Test@5 96.65 || Best Val@1 68.36 \n[lm_smoe_noisy_k2] Ep 014/30 | TrainLoss 0.8152 | Train@1 72.04 | Train@5 98.01 || Val@1 70.14 | Val@5 97.70 || Test@1 69.44 | Test@5 97.68 || Best Val@1 70.14 *\n[lm_smoe_noisy_k2] Ep 015/30 | TrainLoss 0.7988 | Train@1 72.71 | Train@5 98.09 || Val@1 68.12 | Val@5 97.00 || Test@1 68.04 | Test@5 96.92 || Best Val@1 70.14 \n[lm_smoe_noisy_k2] Ep 016/30 | TrainLoss 0.7787 | Train@1 73.29 | Train@5 98.23 || Val@1 65.76 | Val@5 97.72 || Test@1 67.39 | Test@5 97.52 || Best Val@1 70.14 \n[lm_smoe_noisy_k2] Ep 017/30 | TrainLoss 0.7629 | Train@1 73.98 | Train@5 98.36 || Val@1 66.86 | Val@5 96.88 || Test@1 67.39 | Test@5 97.22 || Best Val@1 70.14 \n[lm_smoe_noisy_k2] Ep 018/30 | TrainLoss 0.7473 | Train@1 74.69 | Train@5 98.40 || Val@1 72.28 | Val@5 98.16 || Test@1 72.10 | Test@5 98.25 || Best Val@1 72.28 *\n[lm_smoe_noisy_k2] Ep 019/30 | TrainLoss 0.7252 | Train@1 75.12 | Train@5 98.47 || Val@1 69.98 | Val@5 98.10 || Test@1 70.38 | Test@5 98.41 || Best Val@1 72.28 \n[lm_smoe_noisy_k2] Ep 020/30 | TrainLoss 0.7146 | Train@1 75.49 | Train@5 98.59 || Val@1 72.90 | Val@5 98.10 || Test@1 74.09 | Test@5 98.27 || Best Val@1 72.90 *\n[lm_smoe_noisy_k2] Ep 021/30 | TrainLoss 0.6955 | Train@1 76.29 | Train@5 98.60 || Val@1 71.18 | Val@5 98.24 || Test@1 72.13 | Test@5 98.18 || Best Val@1 72.90 \n[lm_smoe_noisy_k2] Ep 022/30 | TrainLoss 0.6758 | Train@1 76.97 | Train@5 98.71 || Val@1 72.68 | Val@5 98.20 || Test@1 73.03 | Test@5 98.10 || Best Val@1 72.90 \n[lm_smoe_noisy_k2] Ep 023/30 | TrainLoss 0.6623 | Train@1 77.46 | Train@5 98.74 || Val@1 74.02 | Val@5 98.06 || Test@1 74.19 | Test@5 98.15 || Best Val@1 74.02 *\n[lm_smoe_noisy_k2] Ep 024/30 | TrainLoss 0.6417 | Train@1 78.25 | Train@5 98.85 || Val@1 75.98 | Val@5 98.74 || Test@1 76.00 | Test@5 98.71 || Best Val@1 75.98 *\n[lm_smoe_noisy_k2] Ep 025/30 | TrainLoss 0.6273 | Train@1 78.85 | Train@5 98.89 || Val@1 77.06 | Val@5 98.70 || Test@1 76.45 | Test@5 98.77 || Best Val@1 77.06 *\n[lm_smoe_noisy_k2] Ep 026/30 | TrainLoss 0.6111 | Train@1 79.29 | Train@5 98.93 || Val@1 77.28 | Val@5 98.96 || Test@1 78.14 | Test@5 98.87 || Best Val@1 77.28 *\n[lm_smoe_noisy_k2] Ep 027/30 | TrainLoss 0.5990 | Train@1 79.74 | Train@5 99.00 || Val@1 77.86 | Val@5 98.70 || Test@1 78.32 | Test@5 98.71 || Best Val@1 77.86 *\n[lm_smoe_noisy_k2] Ep 028/30 | TrainLoss 0.5888 | Train@1 80.15 | Train@5 99.08 || Val@1 78.18 | Val@5 98.90 || Test@1 78.44 | Test@5 98.85 || Best Val@1 78.18 *\n[lm_smoe_noisy_k2] Ep 029/30 | TrainLoss 0.5787 | Train@1 80.39 | Train@5 99.10 || Val@1 78.74 | Val@5 98.86 || Test@1 79.06 | Test@5 98.85 || Best Val@1 78.74 *\n[lm_smoe_noisy_k2] Ep 030/30 | TrainLoss 0.5723 | Train@1 80.66 | Train@5 99.15 || Val@1 78.80 | Val@5 98.94 || Test@1 78.88 | Test@5 98.94 || Best Val@1 78.80 *\n[Done:lm_smoe_noisy_k2] Params:70,946 | FLOPs:N/A | Lat(ms):12.36 | Best Val@1:78.80 | ckpt=checkpoints/lm_smoe_noisy_k2.pth\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"---\n\n## 3) Hash-Routed MoE (router cố định/ngẫu nhiên)\n\n**Router cố định.** Lấy phép chiếu ngẫu nhiên cố định $R \\in \\mathbb{R}^{d \\times h}$. Với token $x$:\n\n$$\nq = R^\\top x \\in \\mathbb{R}^{h}, \\quad \ns = \\operatorname{sign}(q) \\in \\{-1,+1\\}^{h}.\n$$\n\nÁnh xạ băm tới chỉ số expert:\n\n$$\n\\phi(s) = \\Big(\\sum_{m=1}^{h} \\mathbb{1}[s_m=+1] \\cdot 2^{m-1}\\Big) \\bmod E, \\quad \ni^* = \\phi(s).\n$$\n\n**Routing/Kết hợp.** $k=1$, $S_{t,i}=\\mathbb{1}[i=i^*]$, $G_{t,i}=\\mathbb{1}[i=i^*]$.\n\n---\n","metadata":{}},{"cell_type":"code","source":"# ========= HASH-ROUTED MoE =========\nclass HashMoE(nn.Module):\n    def __init__(self, channels, num_experts=4, k=1, seed=42):\n        super().__init__()\n        assert k==1\n        self.E = num_experts\n        g = torch.Generator(); g.manual_seed(seed)\n        self.register_buffer('proj', torch.randn(channels, num_experts, generator=g))  # fixed router\n        self.experts = nn.ModuleList([TinyExpert(channels) for _ in range(self.E)])\n\n    def forward(self, x):\n        B,C,H,W = x.shape\n        gap = F.adaptive_avg_pool2d(x,1).view(B,C)\n        top1 = (gap @ self.proj).argmax(dim=1)            # [B]\n        onehot = F.one_hot(top1, num_classes=self.E).float()\n        out = 0.0\n        for e_id, expert in enumerate(self.experts):\n            out = out + expert(x) * onehot[:, e_id].view(B,1,1,1)\n        return out, None\n\nclass TinyCNN_HashMoE(nn.Module):\n    def __init__(self, num_classes=10, base_ch=32, num_experts=4, seed=123):\n        super().__init__()\n        self.stem = nn.Sequential(nn.Conv2d(3, base_ch, 3, padding=1, bias=False),\n                                  nn.BatchNorm2d(base_ch), nn.ReLU(True))\n        self.m1 = HashMoE(base_ch, num_experts=num_experts, k=1, seed=seed)\n        self.down = nn.Conv2d(base_ch, base_ch*2, 3, 2, 1)\n        self.m2 = HashMoE(base_ch*2, num_experts=num_experts, k=1, seed=seed+1)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(base_ch*2, num_classes)\n    def forward(self, x):\n        x, _ = self.m1(self.stem(x)); x = F.relu(self.down(x))\n        x, _ = self.m2(x)\n        x = self.pool(x).flatten(1)\n        return self.fc(x), None\n\n# ---- Train Hash-Routed ----\nmodel_hash = TinyCNN_HashMoE(num_classes=NUM_CLASSES, base_ch=(24 if 'QUICKRUN' in globals() and QUICKRUN else 32),\n                             num_experts=4, seed=123)\nckpt_hash = train_moe(model_hash, label=\"lm_smoe_hash\",\n                      epochs=EPOCHS, lr=(0.05 if 'QUICKRUN' in globals() and QUICKRUN else 0.1), lb_weight=0.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:23:53.901621Z","iopub.execute_input":"2025-09-20T14:23:53.901883Z","iopub.status.idle":"2025-09-20T14:32:03.890330Z","shell.execute_reply.started":"2025-09-20T14:23:53.901858Z","shell.execute_reply":"2025-09-20T14:32:03.889573Z"}},"outputs":[{"name":"stdout","text":"[Start] lm_smoe_hash | epochs=30 lr=0.05 lb_w=0.0\n[lm_smoe_hash] Ep 001/30 | TrainLoss 1.7471 | Train@1 33.71 | Train@5 86.19 || Val@1 35.20 | Val@5 88.50 || Test@1 35.11 | Test@5 89.12 || Best Val@1 35.20 *\n[lm_smoe_hash] Ep 002/30 | TrainLoss 1.4353 | Train@1 47.08 | Train@5 92.51 || Val@1 38.44 | Val@5 84.60 || Test@1 37.69 | Test@5 85.36 || Best Val@1 38.44 *\n[lm_smoe_hash] Ep 003/30 | TrainLoss 1.2741 | Train@1 53.55 | Train@5 94.04 || Val@1 47.52 | Val@5 90.96 || Test@1 46.21 | Test@5 90.24 || Best Val@1 47.52 *\n[lm_smoe_hash] Ep 004/30 | TrainLoss 1.1783 | Train@1 57.23 | Train@5 95.48 || Val@1 56.34 | Val@5 95.42 || Test@1 56.62 | Test@5 95.68 || Best Val@1 56.34 *\n[lm_smoe_hash] Ep 005/30 | TrainLoss 1.1252 | Train@1 59.58 | Train@5 95.94 || Val@1 52.72 | Val@5 94.26 || Test@1 52.07 | Test@5 94.29 || Best Val@1 56.34 \n[lm_smoe_hash] Ep 006/30 | TrainLoss 1.0851 | Train@1 61.27 | Train@5 96.25 || Val@1 52.54 | Val@5 93.74 || Test@1 52.56 | Test@5 93.29 || Best Val@1 56.34 \n[lm_smoe_hash] Ep 007/30 | TrainLoss 1.0463 | Train@1 62.58 | Train@5 96.52 || Val@1 51.00 | Val@5 95.04 || Test@1 51.11 | Test@5 95.30 || Best Val@1 56.34 \n[lm_smoe_hash] Ep 008/30 | TrainLoss 1.0232 | Train@1 63.22 | Train@5 96.68 || Val@1 53.42 | Val@5 94.38 || Test@1 53.26 | Test@5 94.39 || Best Val@1 56.34 \n[lm_smoe_hash] Ep 009/30 | TrainLoss 1.0085 | Train@1 63.96 | Train@5 96.71 || Val@1 56.80 | Val@5 94.60 || Test@1 56.52 | Test@5 94.57 || Best Val@1 56.80 *\n[lm_smoe_hash] Ep 010/30 | TrainLoss 0.9855 | Train@1 64.81 | Train@5 96.81 || Val@1 56.92 | Val@5 95.46 || Test@1 55.52 | Test@5 95.58 || Best Val@1 56.92 *\n[lm_smoe_hash] Ep 011/30 | TrainLoss 0.9626 | Train@1 65.59 | Train@5 97.02 || Val@1 55.52 | Val@5 94.14 || Test@1 55.27 | Test@5 94.22 || Best Val@1 56.92 \n[lm_smoe_hash] Ep 012/30 | TrainLoss 0.9496 | Train@1 65.98 | Train@5 97.19 || Val@1 55.42 | Val@5 94.10 || Test@1 55.04 | Test@5 94.29 || Best Val@1 56.92 \n[lm_smoe_hash] Ep 013/30 | TrainLoss 0.9298 | Train@1 67.11 | Train@5 97.26 || Val@1 45.54 | Val@5 94.68 || Test@1 46.37 | Test@5 94.55 || Best Val@1 56.92 \n[lm_smoe_hash] Ep 014/30 | TrainLoss 0.9137 | Train@1 67.60 | Train@5 97.41 || Val@1 63.84 | Val@5 96.48 || Test@1 63.00 | Test@5 96.81 || Best Val@1 63.84 *\n[lm_smoe_hash] Ep 015/30 | TrainLoss 0.8915 | Train@1 68.40 | Train@5 97.48 || Val@1 65.50 | Val@5 97.34 || Test@1 65.77 | Test@5 97.33 || Best Val@1 65.50 *\n[lm_smoe_hash] Ep 016/30 | TrainLoss 0.8679 | Train@1 69.39 | Train@5 97.64 || Val@1 65.02 | Val@5 96.80 || Test@1 65.43 | Test@5 97.28 || Best Val@1 65.50 \n[lm_smoe_hash] Ep 017/30 | TrainLoss 0.8479 | Train@1 70.18 | Train@5 97.78 || Val@1 66.10 | Val@5 96.92 || Test@1 66.02 | Test@5 96.86 || Best Val@1 66.10 *\n[lm_smoe_hash] Ep 018/30 | TrainLoss 0.8238 | Train@1 71.03 | Train@5 97.90 || Val@1 58.54 | Val@5 96.22 || Test@1 58.88 | Test@5 96.41 || Best Val@1 66.10 \n[lm_smoe_hash] Ep 019/30 | TrainLoss 0.8110 | Train@1 71.54 | Train@5 98.00 || Val@1 66.60 | Val@5 97.18 || Test@1 66.66 | Test@5 97.59 || Best Val@1 66.60 *\n[lm_smoe_hash] Ep 020/30 | TrainLoss 0.7837 | Train@1 72.71 | Train@5 98.10 || Val@1 62.96 | Val@5 96.28 || Test@1 63.26 | Test@5 96.44 || Best Val@1 66.60 \n[lm_smoe_hash] Ep 021/30 | TrainLoss 0.7749 | Train@1 72.91 | Train@5 98.16 || Val@1 63.40 | Val@5 97.20 || Test@1 64.10 | Test@5 97.01 || Best Val@1 66.60 \n[lm_smoe_hash] Ep 022/30 | TrainLoss 0.7508 | Train@1 73.69 | Train@5 98.25 || Val@1 63.94 | Val@5 96.46 || Test@1 64.05 | Test@5 96.59 || Best Val@1 66.60 \n[lm_smoe_hash] Ep 023/30 | TrainLoss 0.7403 | Train@1 74.02 | Train@5 98.28 || Val@1 67.14 | Val@5 97.14 || Test@1 67.20 | Test@5 97.22 || Best Val@1 67.14 *\n[lm_smoe_hash] Ep 024/30 | TrainLoss 0.7271 | Train@1 74.58 | Train@5 98.25 || Val@1 71.96 | Val@5 97.94 || Test@1 72.29 | Test@5 98.00 || Best Val@1 71.96 *\n[lm_smoe_hash] Ep 025/30 | TrainLoss 0.6990 | Train@1 75.42 | Train@5 98.48 || Val@1 71.72 | Val@5 97.88 || Test@1 71.65 | Test@5 97.23 || Best Val@1 71.96 \n[lm_smoe_hash] Ep 026/30 | TrainLoss 0.6811 | Train@1 76.10 | Train@5 98.51 || Val@1 73.92 | Val@5 97.78 || Test@1 73.58 | Test@5 97.95 || Best Val@1 73.92 *\n[lm_smoe_hash] Ep 027/30 | TrainLoss 0.6661 | Train@1 76.87 | Train@5 98.57 || Val@1 73.20 | Val@5 97.72 || Test@1 73.59 | Test@5 97.83 || Best Val@1 73.92 \n[lm_smoe_hash] Ep 028/30 | TrainLoss 0.6516 | Train@1 77.33 | Train@5 98.63 || Val@1 73.50 | Val@5 97.58 || Test@1 74.08 | Test@5 97.76 || Best Val@1 73.92 \n[lm_smoe_hash] Ep 029/30 | TrainLoss 0.6474 | Train@1 77.70 | Train@5 98.59 || Val@1 74.02 | Val@5 98.00 || Test@1 74.33 | Test@5 97.95 || Best Val@1 74.02 *\n[lm_smoe_hash] Ep 030/30 | TrainLoss 0.6374 | Train@1 77.86 | Train@5 98.72 || Val@1 74.20 | Val@5 97.88 || Test@1 74.34 | Test@5 97.79 || Best Val@1 74.20 *\n[Done:lm_smoe_hash] Params:40,978 | FLOPs:N/A | Lat(ms):3.23 | Best Val@1:74.20 | ckpt=checkpoints/lm_smoe_hash.pth\n","output_type":"stream"}],"execution_count":10}]}