{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7741dbb",
   "metadata": {
    "id": "e1f2a85b",
    "papermill": {
     "duration": 0.006833,
     "end_time": "2025-09-16T08:34:57.902526",
     "exception": false,
     "start_time": "2025-09-16T08:34:57.895693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Knowledge Distillation on CIFAR-10\n",
    "\n",
    "Mục tiêu: Cài đặt nhiều phương pháp Knowledge Distillation (KD) khác nhau cho bài toán phân loại CIFAR-10.\n",
    "\n",
    "Yêu cầu chính:\n",
    "- Dataset: CIFAR-10 (train/test chuẩn của torchvision)\n",
    "- Model teacher: pretrained trên CIFAR-10; model student nhỏ hơn, chưa train.\n",
    "- Một biến chung `KD_EPOCHS` xác định số epoch train cho TẤT CẢ phương pháp KD.\n",
    "- Trước mỗi phương pháp có một cell markdown ghi tên phương pháp.\n",
    "- Mỗi phương pháp in ra: tổng thời gian train + accuracy trên train và test.\n",
    "\n",
    "Các phần dưới đây cung cấp phần setup dùng chung (dataloader, model, util, teacher), sau đó là từng phương pháp KD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c6ed69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T08:34:57.915372Z",
     "iopub.status.busy": "2025-09-16T08:34:57.915062Z",
     "iopub.status.idle": "2025-09-16T09:49:46.005106Z",
     "shell.execute_reply": "2025-09-16T09:49:46.003848Z"
    },
    "id": "3d503ed3",
    "outputId": "7894f04e-bf6d-4fe1-e465-76c1ae67d25e",
    "papermill": {
     "duration": 4488.09867,
     "end_time": "2025-09-16T09:49:46.006862",
     "exception": false,
     "start_time": "2025-09-16T08:34:57.908192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:12<00:00, 13.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher checkpoint not found. Training ResNet18 teacher from scratch on CIFAR-10 (select best by val acc).\n",
      "Teacher epoch 1/200 - train_acc: 0.3017 - val_acc: 0.4076 (saved best)\n",
      "Teacher epoch 2/200 - train_acc: 0.4340 - val_acc: 0.4502 (saved best)\n",
      "Teacher epoch 3/200 - train_acc: 0.5376 - val_acc: 0.5800 (saved best)\n",
      "Teacher epoch 4/200 - train_acc: 0.6212 - val_acc: 0.6356 (saved best)\n",
      "Teacher epoch 5/200 - train_acc: 0.6813 - val_acc: 0.7140 (saved best)\n",
      "Teacher epoch 6/200 - train_acc: 0.7300 - val_acc: 0.7498 (saved best)\n",
      "Teacher epoch 7/200 - train_acc: 0.7535 - val_acc: 0.7464\n",
      "Teacher epoch 8/200 - train_acc: 0.7645 - val_acc: 0.7954 (saved best)\n",
      "Teacher epoch 9/200 - train_acc: 0.7752 - val_acc: 0.7662\n",
      "Teacher epoch 10/200 - train_acc: 0.7808 - val_acc: 0.7894\n",
      "Teacher epoch 11/200 - train_acc: 0.7891 - val_acc: 0.7934\n",
      "Teacher epoch 12/200 - train_acc: 0.7942 - val_acc: 0.7770\n",
      "Teacher epoch 13/200 - train_acc: 0.7971 - val_acc: 0.7934\n",
      "Teacher epoch 14/200 - train_acc: 0.8001 - val_acc: 0.7912\n",
      "Teacher epoch 15/200 - train_acc: 0.8087 - val_acc: 0.7752\n",
      "Teacher epoch 16/200 - train_acc: 0.8062 - val_acc: 0.7572\n",
      "Teacher epoch 17/200 - train_acc: 0.8096 - val_acc: 0.7928\n",
      "Teacher epoch 18/200 - train_acc: 0.8136 - val_acc: 0.8102 (saved best)\n",
      "Teacher epoch 19/200 - train_acc: 0.8128 - val_acc: 0.7820\n",
      "Teacher epoch 20/200 - train_acc: 0.8178 - val_acc: 0.7030\n",
      "Teacher epoch 21/200 - train_acc: 0.8168 - val_acc: 0.7250\n",
      "Teacher epoch 22/200 - train_acc: 0.8214 - val_acc: 0.7566\n",
      "Teacher epoch 23/200 - train_acc: 0.8189 - val_acc: 0.7962\n",
      "Teacher epoch 24/200 - train_acc: 0.8206 - val_acc: 0.8084\n",
      "Teacher epoch 25/200 - train_acc: 0.8270 - val_acc: 0.8002\n",
      "Teacher epoch 26/200 - train_acc: 0.8225 - val_acc: 0.6678\n",
      "Teacher epoch 27/200 - train_acc: 0.8249 - val_acc: 0.7694\n",
      "Teacher epoch 28/200 - train_acc: 0.8270 - val_acc: 0.8082\n",
      "Teacher epoch 29/200 - train_acc: 0.8270 - val_acc: 0.7774\n",
      "Teacher epoch 30/200 - train_acc: 0.8290 - val_acc: 0.8076\n",
      "Teacher epoch 31/200 - train_acc: 0.8274 - val_acc: 0.7926\n",
      "Teacher epoch 32/200 - train_acc: 0.8328 - val_acc: 0.7278\n",
      "Teacher epoch 33/200 - train_acc: 0.8307 - val_acc: 0.7224\n",
      "Teacher epoch 34/200 - train_acc: 0.8307 - val_acc: 0.7948\n",
      "Teacher epoch 35/200 - train_acc: 0.8306 - val_acc: 0.7718\n",
      "Teacher epoch 36/200 - train_acc: 0.8359 - val_acc: 0.7824\n",
      "Teacher epoch 37/200 - train_acc: 0.8361 - val_acc: 0.8444 (saved best)\n",
      "Teacher epoch 38/200 - train_acc: 0.8391 - val_acc: 0.7908\n",
      "Teacher epoch 39/200 - train_acc: 0.8363 - val_acc: 0.8140\n",
      "Teacher epoch 40/200 - train_acc: 0.8395 - val_acc: 0.8052\n",
      "Teacher epoch 41/200 - train_acc: 0.8411 - val_acc: 0.8114\n",
      "Teacher epoch 42/200 - train_acc: 0.8384 - val_acc: 0.7858\n",
      "Teacher epoch 43/200 - train_acc: 0.8385 - val_acc: 0.8144\n",
      "Teacher epoch 44/200 - train_acc: 0.8424 - val_acc: 0.8234\n",
      "Teacher epoch 45/200 - train_acc: 0.8416 - val_acc: 0.8242\n",
      "Teacher epoch 46/200 - train_acc: 0.8447 - val_acc: 0.8144\n",
      "Teacher epoch 47/200 - train_acc: 0.8440 - val_acc: 0.7718\n",
      "Teacher epoch 48/200 - train_acc: 0.8431 - val_acc: 0.8040\n",
      "Teacher epoch 49/200 - train_acc: 0.8431 - val_acc: 0.7900\n",
      "Teacher epoch 50/200 - train_acc: 0.8464 - val_acc: 0.7882\n",
      "Teacher epoch 51/200 - train_acc: 0.8476 - val_acc: 0.7838\n",
      "Teacher epoch 52/200 - train_acc: 0.8466 - val_acc: 0.8312\n",
      "Teacher epoch 53/200 - train_acc: 0.8471 - val_acc: 0.8220\n",
      "Teacher epoch 54/200 - train_acc: 0.8510 - val_acc: 0.7982\n",
      "Teacher epoch 55/200 - train_acc: 0.8470 - val_acc: 0.7932\n",
      "Teacher epoch 56/200 - train_acc: 0.8501 - val_acc: 0.8100\n",
      "Teacher epoch 57/200 - train_acc: 0.8520 - val_acc: 0.8444\n",
      "Teacher epoch 58/200 - train_acc: 0.8515 - val_acc: 0.7450\n",
      "Teacher epoch 59/200 - train_acc: 0.8540 - val_acc: 0.7930\n",
      "Teacher epoch 60/200 - train_acc: 0.8537 - val_acc: 0.7440\n",
      "Teacher epoch 61/200 - train_acc: 0.8555 - val_acc: 0.7438\n",
      "Teacher epoch 62/200 - train_acc: 0.8558 - val_acc: 0.8272\n",
      "Teacher epoch 63/200 - train_acc: 0.8554 - val_acc: 0.8206\n",
      "Teacher epoch 64/200 - train_acc: 0.8555 - val_acc: 0.7644\n",
      "Teacher epoch 65/200 - train_acc: 0.8557 - val_acc: 0.8160\n",
      "Teacher epoch 66/200 - train_acc: 0.8575 - val_acc: 0.8470 (saved best)\n",
      "Teacher epoch 67/200 - train_acc: 0.8570 - val_acc: 0.8434\n",
      "Teacher epoch 68/200 - train_acc: 0.8609 - val_acc: 0.8510 (saved best)\n",
      "Teacher epoch 69/200 - train_acc: 0.8624 - val_acc: 0.7394\n",
      "Teacher epoch 70/200 - train_acc: 0.8593 - val_acc: 0.8308\n",
      "Teacher epoch 71/200 - train_acc: 0.8636 - val_acc: 0.7948\n",
      "Teacher epoch 72/200 - train_acc: 0.8635 - val_acc: 0.8046\n",
      "Teacher epoch 73/200 - train_acc: 0.8614 - val_acc: 0.8082\n",
      "Teacher epoch 74/200 - train_acc: 0.8663 - val_acc: 0.8462\n",
      "Teacher epoch 75/200 - train_acc: 0.8630 - val_acc: 0.8162\n",
      "Teacher epoch 76/200 - train_acc: 0.8649 - val_acc: 0.8440\n",
      "Teacher epoch 77/200 - train_acc: 0.8688 - val_acc: 0.8328\n",
      "Teacher epoch 78/200 - train_acc: 0.8664 - val_acc: 0.8104\n",
      "Teacher epoch 79/200 - train_acc: 0.8706 - val_acc: 0.8472\n",
      "Teacher epoch 80/200 - train_acc: 0.8692 - val_acc: 0.7974\n",
      "Teacher epoch 81/200 - train_acc: 0.8719 - val_acc: 0.8698 (saved best)\n",
      "Teacher epoch 82/200 - train_acc: 0.8688 - val_acc: 0.8408\n",
      "Teacher epoch 83/200 - train_acc: 0.8721 - val_acc: 0.8200\n",
      "Teacher epoch 84/200 - train_acc: 0.8713 - val_acc: 0.8546\n",
      "Teacher epoch 85/200 - train_acc: 0.8748 - val_acc: 0.8326\n",
      "Teacher epoch 86/200 - train_acc: 0.8753 - val_acc: 0.8372\n",
      "Teacher epoch 87/200 - train_acc: 0.8742 - val_acc: 0.8360\n",
      "Teacher epoch 88/200 - train_acc: 0.8770 - val_acc: 0.8374\n",
      "Teacher epoch 89/200 - train_acc: 0.8791 - val_acc: 0.8394\n",
      "Teacher epoch 90/200 - train_acc: 0.8778 - val_acc: 0.8648\n",
      "Teacher epoch 91/200 - train_acc: 0.8822 - val_acc: 0.8700 (saved best)\n",
      "Teacher epoch 92/200 - train_acc: 0.8801 - val_acc: 0.8446\n",
      "Teacher epoch 93/200 - train_acc: 0.8842 - val_acc: 0.8746 (saved best)\n",
      "Teacher epoch 94/200 - train_acc: 0.8827 - val_acc: 0.8846 (saved best)\n",
      "Teacher epoch 95/200 - train_acc: 0.8818 - val_acc: 0.8474\n",
      "Teacher epoch 96/200 - train_acc: 0.8861 - val_acc: 0.8110\n",
      "Teacher epoch 97/200 - train_acc: 0.8844 - val_acc: 0.8666\n",
      "Teacher epoch 98/200 - train_acc: 0.8870 - val_acc: 0.8472\n",
      "Teacher epoch 99/200 - train_acc: 0.8882 - val_acc: 0.8514\n",
      "Teacher epoch 100/200 - train_acc: 0.8890 - val_acc: 0.8568\n",
      "Teacher epoch 101/200 - train_acc: 0.8889 - val_acc: 0.8430\n",
      "Teacher epoch 102/200 - train_acc: 0.8907 - val_acc: 0.8772\n",
      "Teacher epoch 103/200 - train_acc: 0.8956 - val_acc: 0.8336\n",
      "Teacher epoch 104/200 - train_acc: 0.8937 - val_acc: 0.8806\n",
      "Teacher epoch 105/200 - train_acc: 0.8965 - val_acc: 0.8520\n",
      "Teacher epoch 106/200 - train_acc: 0.8981 - val_acc: 0.8756\n",
      "Teacher epoch 107/200 - train_acc: 0.8978 - val_acc: 0.8384\n",
      "Teacher epoch 108/200 - train_acc: 0.8981 - val_acc: 0.8804\n",
      "Teacher epoch 109/200 - train_acc: 0.8987 - val_acc: 0.8518\n",
      "Teacher epoch 110/200 - train_acc: 0.9018 - val_acc: 0.8804\n",
      "Teacher epoch 111/200 - train_acc: 0.9047 - val_acc: 0.8770\n",
      "Teacher epoch 112/200 - train_acc: 0.9051 - val_acc: 0.8776\n",
      "Teacher epoch 113/200 - train_acc: 0.9034 - val_acc: 0.8642\n",
      "Teacher epoch 114/200 - train_acc: 0.9055 - val_acc: 0.8890 (saved best)\n",
      "Teacher epoch 115/200 - train_acc: 0.9093 - val_acc: 0.8724\n",
      "Teacher epoch 116/200 - train_acc: 0.9057 - val_acc: 0.8770\n",
      "Teacher epoch 117/200 - train_acc: 0.9110 - val_acc: 0.8792\n",
      "Teacher epoch 118/200 - train_acc: 0.9114 - val_acc: 0.8954 (saved best)\n",
      "Teacher epoch 119/200 - train_acc: 0.9133 - val_acc: 0.8884\n",
      "Teacher epoch 120/200 - train_acc: 0.9115 - val_acc: 0.8530\n",
      "Teacher epoch 121/200 - train_acc: 0.9157 - val_acc: 0.8934\n",
      "Teacher epoch 122/200 - train_acc: 0.9162 - val_acc: 0.8862\n",
      "Teacher epoch 123/200 - train_acc: 0.9167 - val_acc: 0.9050 (saved best)\n",
      "Teacher epoch 124/200 - train_acc: 0.9184 - val_acc: 0.8894\n",
      "Teacher epoch 125/200 - train_acc: 0.9218 - val_acc: 0.8792\n",
      "Teacher epoch 126/200 - train_acc: 0.9217 - val_acc: 0.8826\n",
      "Teacher epoch 127/200 - train_acc: 0.9239 - val_acc: 0.8770\n",
      "Teacher epoch 128/200 - train_acc: 0.9252 - val_acc: 0.8818\n",
      "Teacher epoch 129/200 - train_acc: 0.9220 - val_acc: 0.9008\n",
      "Teacher epoch 130/200 - train_acc: 0.9300 - val_acc: 0.8956\n",
      "Teacher epoch 131/200 - train_acc: 0.9286 - val_acc: 0.8980\n",
      "Teacher epoch 132/200 - train_acc: 0.9307 - val_acc: 0.9104 (saved best)\n",
      "Teacher epoch 133/200 - train_acc: 0.9341 - val_acc: 0.9058\n",
      "Teacher epoch 134/200 - train_acc: 0.9329 - val_acc: 0.8824\n",
      "Teacher epoch 135/200 - train_acc: 0.9357 - val_acc: 0.9044\n",
      "Teacher epoch 136/200 - train_acc: 0.9374 - val_acc: 0.8744\n",
      "Teacher epoch 137/200 - train_acc: 0.9395 - val_acc: 0.8884\n",
      "Teacher epoch 138/200 - train_acc: 0.9399 - val_acc: 0.9022\n",
      "Teacher epoch 139/200 - train_acc: 0.9432 - val_acc: 0.9092\n",
      "Teacher epoch 140/200 - train_acc: 0.9418 - val_acc: 0.9004\n",
      "Teacher epoch 141/200 - train_acc: 0.9431 - val_acc: 0.9074\n",
      "Teacher epoch 142/200 - train_acc: 0.9476 - val_acc: 0.9100\n",
      "Teacher epoch 143/200 - train_acc: 0.9474 - val_acc: 0.9170 (saved best)\n",
      "Teacher epoch 144/200 - train_acc: 0.9512 - val_acc: 0.9208 (saved best)\n",
      "Teacher epoch 145/200 - train_acc: 0.9513 - val_acc: 0.9172\n",
      "Teacher epoch 146/200 - train_acc: 0.9535 - val_acc: 0.9124\n",
      "Teacher epoch 147/200 - train_acc: 0.9538 - val_acc: 0.9190\n",
      "Teacher epoch 148/200 - train_acc: 0.9567 - val_acc: 0.9226 (saved best)\n",
      "Teacher epoch 149/200 - train_acc: 0.9572 - val_acc: 0.9232 (saved best)\n",
      "Teacher epoch 150/200 - train_acc: 0.9585 - val_acc: 0.9112\n",
      "Teacher epoch 151/200 - train_acc: 0.9585 - val_acc: 0.9212\n",
      "Teacher epoch 152/200 - train_acc: 0.9638 - val_acc: 0.9238 (saved best)\n",
      "Teacher epoch 153/200 - train_acc: 0.9661 - val_acc: 0.9182\n",
      "Teacher epoch 154/200 - train_acc: 0.9662 - val_acc: 0.9256 (saved best)\n",
      "Teacher epoch 155/200 - train_acc: 0.9690 - val_acc: 0.9226\n",
      "Teacher epoch 156/200 - train_acc: 0.9693 - val_acc: 0.9210\n",
      "Teacher epoch 157/200 - train_acc: 0.9683 - val_acc: 0.9214\n",
      "Teacher epoch 158/200 - train_acc: 0.9724 - val_acc: 0.9302 (saved best)\n",
      "Teacher epoch 159/200 - train_acc: 0.9764 - val_acc: 0.9260\n",
      "Teacher epoch 160/200 - train_acc: 0.9781 - val_acc: 0.9324 (saved best)\n",
      "Teacher epoch 161/200 - train_acc: 0.9788 - val_acc: 0.9348 (saved best)\n",
      "Teacher epoch 162/200 - train_acc: 0.9782 - val_acc: 0.9318\n",
      "Teacher epoch 163/200 - train_acc: 0.9815 - val_acc: 0.9302\n",
      "Teacher epoch 164/200 - train_acc: 0.9814 - val_acc: 0.9322\n",
      "Teacher epoch 165/200 - train_acc: 0.9848 - val_acc: 0.9380 (saved best)\n",
      "Teacher epoch 166/200 - train_acc: 0.9862 - val_acc: 0.9374\n",
      "Teacher epoch 167/200 - train_acc: 0.9880 - val_acc: 0.9416 (saved best)\n",
      "Teacher epoch 168/200 - train_acc: 0.9899 - val_acc: 0.9362\n",
      "Teacher epoch 169/200 - train_acc: 0.9906 - val_acc: 0.9324\n",
      "Teacher epoch 170/200 - train_acc: 0.9905 - val_acc: 0.9362\n",
      "Teacher epoch 171/200 - train_acc: 0.9924 - val_acc: 0.9386\n",
      "Teacher epoch 172/200 - train_acc: 0.9926 - val_acc: 0.9406\n",
      "Teacher epoch 173/200 - train_acc: 0.9944 - val_acc: 0.9404\n",
      "Teacher epoch 174/200 - train_acc: 0.9956 - val_acc: 0.9458 (saved best)\n",
      "Teacher epoch 175/200 - train_acc: 0.9962 - val_acc: 0.9448\n",
      "Teacher epoch 176/200 - train_acc: 0.9972 - val_acc: 0.9460 (saved best)\n",
      "Teacher epoch 177/200 - train_acc: 0.9979 - val_acc: 0.9464 (saved best)\n",
      "Teacher epoch 178/200 - train_acc: 0.9976 - val_acc: 0.9448\n",
      "Teacher epoch 179/200 - train_acc: 0.9980 - val_acc: 0.9472 (saved best)\n",
      "Teacher epoch 180/200 - train_acc: 0.9986 - val_acc: 0.9502 (saved best)\n",
      "Teacher epoch 181/200 - train_acc: 0.9990 - val_acc: 0.9460\n",
      "Teacher epoch 182/200 - train_acc: 0.9991 - val_acc: 0.9486\n",
      "Teacher epoch 183/200 - train_acc: 0.9995 - val_acc: 0.9510 (saved best)\n",
      "Teacher epoch 184/200 - train_acc: 0.9995 - val_acc: 0.9498\n",
      "Teacher epoch 185/200 - train_acc: 0.9996 - val_acc: 0.9516 (saved best)\n",
      "Teacher epoch 186/200 - train_acc: 0.9996 - val_acc: 0.9506\n",
      "Teacher epoch 187/200 - train_acc: 0.9997 - val_acc: 0.9514\n",
      "Teacher epoch 188/200 - train_acc: 0.9997 - val_acc: 0.9518 (saved best)\n",
      "Teacher epoch 189/200 - train_acc: 0.9998 - val_acc: 0.9518\n",
      "Teacher epoch 190/200 - train_acc: 0.9999 - val_acc: 0.9524 (saved best)\n",
      "Teacher epoch 191/200 - train_acc: 0.9998 - val_acc: 0.9524\n",
      "Teacher epoch 192/200 - train_acc: 0.9998 - val_acc: 0.9526 (saved best)\n",
      "Teacher epoch 193/200 - train_acc: 0.9998 - val_acc: 0.9524\n",
      "Teacher epoch 194/200 - train_acc: 0.9999 - val_acc: 0.9522\n",
      "Teacher epoch 195/200 - train_acc: 0.9998 - val_acc: 0.9528 (saved best)\n",
      "Teacher epoch 196/200 - train_acc: 0.9998 - val_acc: 0.9526\n",
      "Teacher epoch 197/200 - train_acc: 0.9997 - val_acc: 0.9520\n",
      "Teacher epoch 198/200 - train_acc: 0.9998 - val_acc: 0.9534 (saved best)\n",
      "Teacher epoch 199/200 - train_acc: 0.9998 - val_acc: 0.9528\n",
      "Teacher epoch 200/200 - train_acc: 0.9999 - val_acc: 0.9528\n",
      "Saved best teacher to ./checkpoints/kd_teacher.pth. Best val_acc: 0.9534. Training time: 4459.3s\n",
      "Teacher test acc: 0.9489\n",
      "KD_EPOCHS = 50\n"
     ]
    }
   ],
   "source": [
    "# %% Shared Setup: dependencies, config, data, models, utils\n",
    "import os, time, math, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import copy\n",
    "\n",
    "# Reproducibility\n",
    "SEED = int(os.environ.get(\"SEED\", 42))\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Global epochs shared by all KD methods\n",
    "KD_EPOCHS = int(os.environ.get(\"KD_EPOCHS\", 50))  # chỉnh tại đây nếu muốn\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "NUM_CLASSES = 10\n",
    "VAL_RATIO = float(os.environ.get(\"VAL_RATIO\", 0.1))  # 10% train -> val split\n",
    "\n",
    "# Teacher training epochs if training from scratch\n",
    "TEACHER_EPOCHS = int(os.environ.get(\"TEACHER_EPOCHS\", 200))\n",
    "\n",
    "# Ensure checkpoints directory exists\n",
    "CKPT_DIR = os.environ.get(\"CKPT_DIR\", \"./checkpoints\")\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# Data: CIFAR-10\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "train_tf = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# Build full train set twice to allow different transforms for train vs validation\n",
    "train_full_aug = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "train_full_plain = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=False, transform=test_tf)\n",
    "\n",
    "# Create reproducible train/val split\n",
    "N = len(train_full_aug)\n",
    "val_size = max(1, int(VAL_RATIO * N))\n",
    "train_size = N - val_size\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "perm = torch.randperm(N, generator=gen)\n",
    "val_idx = perm[:val_size].tolist()\n",
    "train_idx = perm[val_size:].tolist()\n",
    "\n",
    "train_set = Subset(train_full_aug, train_idx)\n",
    "val_set = Subset(train_full_plain, val_idx)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Student model (smaller than ResNet18)\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Teacher model: minimal ResNet18 for CIFAR-10 (3x3 stem, no initial maxpool)\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # layers: 2,2,2,2\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * BasicBlock.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_teacher(num_classes=10):\n",
    "    # Build a CIFAR-10 style ResNet18 from scratch\n",
    "    return ResNet18(num_classes)\n",
    "\n",
    "# Point teacher checkpoint to checkpoints/kd_teacher.pth\n",
    "TEACHER_CKPT = os.path.join(CKPT_DIR, \"kd_teacher.pth\")\n",
    "\n",
    "# Train/eval utilities\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        out = model(x)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "        loss = criterion(logits, y)\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    acc = correct / total\n",
    "    avg_loss = loss_sum / total\n",
    "    return acc, avg_loss\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    train_acc: float\n",
    "    test_acc: float\n",
    "    train_time_sec: float\n",
    "\n",
    "\n",
    "def top1_acc(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "\n",
    "def train_ce(model: nn.Module, loader: DataLoader, optimizer, device: torch.device, scaler: Optional[GradScaler] = None):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                out = model(x)\n",
    "                logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "                loss = criterion(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model(x)\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    return total_correct / max(1, total_samples)\n",
    "\n",
    "def flatten_features(x):\n",
    "    return torch.flatten(x, 1)\n",
    "\n",
    "# Hooks to grab intermediate features (for feature distillation methods)\n",
    "class FeatureHook:\n",
    "    def __init__(self, module: nn.Module):\n",
    "        self.feat = None\n",
    "        module.register_forward_hook(self.hook)\n",
    "    def hook(self, module, input, output):\n",
    "        self.feat = output\n",
    "\n",
    "# Teacher build/load\n",
    "teacher = build_teacher(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "TEACHER_OPT_LR = float(os.environ.get(\"TEACHER_OPT_LR\", 0.1))\n",
    "TEACHER_WD = float(os.environ.get(\"TEACHER_WD\", 5e-4))\n",
    "\n",
    "if os.path.isfile(TEACHER_CKPT):\n",
    "    teacher.load_state_dict(torch.load(TEACHER_CKPT, map_location=DEVICE))\n",
    "    print(f\"Loaded teacher weights from {TEACHER_CKPT}\")\n",
    "else:\n",
    "    print(\"Teacher checkpoint not found. Training ResNet18 teacher from scratch on CIFAR-10 (select best by val acc).\")\n",
    "    optimizer_t = optim.SGD(teacher.parameters(), lr=TEACHER_OPT_LR, momentum=0.9, weight_decay=TEACHER_WD)\n",
    "    scheduler_t = optim.lr_scheduler.CosineAnnealingLR(optimizer_t, T_max=TEACHER_EPOCHS)\n",
    "    scaler_t = GradScaler(enabled=torch.cuda.is_available())\n",
    "    best_val, best_state = 0.0, None\n",
    "    start_t = time.time()\n",
    "    for e in range(TEACHER_EPOCHS):\n",
    "        acc_train = train_ce(teacher, train_loader, optimizer_t, DEVICE, scaler=scaler_t)\n",
    "        acc_val, _ = evaluate(teacher, val_loader, DEVICE)\n",
    "        if acc_val > best_val:\n",
    "            best_val = acc_val\n",
    "            best_state = copy.deepcopy(teacher.state_dict())\n",
    "            torch.save(best_state, TEACHER_CKPT)\n",
    "            tag = \" (saved best)\"\n",
    "        else:\n",
    "            tag = \"\"\n",
    "        scheduler_t.step()\n",
    "        print(f\"Teacher epoch {e+1}/{TEACHER_EPOCHS} - train_acc: {acc_train:.4f} - val_acc: {acc_val:.4f}{tag}\")\n",
    "    elapsed_t = time.time() - start_t\n",
    "    if best_state is not None:\n",
    "        teacher.load_state_dict(best_state)\n",
    "    print(f\"Saved best teacher to {TEACHER_CKPT}. Best val_acc: {best_val:.4f}. Training time: {elapsed_t:.1f}s\")\n",
    "\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "teacher.eval()\n",
    "\n",
    "acc_test, _ = evaluate(teacher, test_loader, DEVICE)\n",
    "print(f\"Teacher test acc: {acc_test:.4f}\")\n",
    "\n",
    "print(\"KD_EPOCHS =\", KD_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de87063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T09:49:46.043159Z",
     "iopub.status.busy": "2025-09-16T09:49:46.042636Z",
     "iopub.status.idle": "2025-09-16T09:49:46.061977Z",
     "shell.execute_reply": "2025-09-16T09:49:46.061142Z"
    },
    "papermill": {
     "duration": 0.03821,
     "end_time": "2025-09-16T09:49:46.063291",
     "exception": false,
     "start_time": "2025-09-16T09:49:46.025081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Captured initial SmallNet weights with param-sum=646.114041 (seed=42)\n"
     ]
    }
   ],
   "source": [
    "# %% Initialize a single SmallNet once for fair comparisons across methods\n",
    "# We capture the randomly initialized weights here and reuse them before each training method.\n",
    "import copy\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # More deterministic behavior (may impact performance slightly)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Ensure the same seed is used for the initial student snapshot\n",
    "seed_all(SEED)\n",
    "_initial_student = SmallNet(NUM_CLASSES)\n",
    "initial_student_state = copy.deepcopy(_initial_student.state_dict())\n",
    "del _initial_student\n",
    "\n",
    "\n",
    "def make_fresh_student(model_cls=SmallNet):\n",
    "    \"\"\"Create a new student model and load the same initial weights.\n",
    "    Use this before training each method to ensure fair initialization.\n",
    "    \"\"\"\n",
    "    m = model_cls(NUM_CLASSES)\n",
    "    # strict=True since subclasses reuse the same layers/keys; adjust if you customize layers\n",
    "    m.load_state_dict(initial_student_state, strict=True)\n",
    "    return m.to(DEVICE)\n",
    "\n",
    "# Quick fingerprint for sanity (sum of parameters from the saved initial state)\n",
    "with torch.no_grad():\n",
    "    _sum = 0.0\n",
    "    for k, v in initial_student_state.items():\n",
    "        if torch.is_floating_point(v):\n",
    "            _sum += v.float().sum().item()\n",
    "print(f\"[Init] Captured initial SmallNet weights with param-sum={_sum:.6f} (seed={SEED})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f9a6b",
   "metadata": {
    "id": "795b9274",
    "papermill": {
     "duration": 0.015905,
     "end_time": "2025-09-16T09:49:46.098180",
     "exception": false,
     "start_time": "2025-09-16T09:49:46.082275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## So sánh mô hình teacher và student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7813f3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T09:49:46.132233Z",
     "iopub.status.busy": "2025-09-16T09:49:46.131962Z",
     "iopub.status.idle": "2025-09-16T09:49:47.143039Z",
     "shell.execute_reply": "2025-09-16T09:49:47.142200Z"
    },
    "id": "eefe070d",
    "outputId": "ae9ec1ef-2829-4629-b0f5-d3da32cc2207",
    "papermill": {
     "duration": 1.029844,
     "end_time": "2025-09-16T09:49:47.144511",
     "exception": false,
     "start_time": "2025-09-16T09:49:46.114667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So sánh mô hình teacher và student (đầu vào 32x32):\n",
      "\n",
      "[Teacher (ResNet18)]\n",
      "  Params: 11173962 (trainable: 0) -> 11.17M params\n",
      "  Layers (Conv+FC): 21\n",
      "  MACs (32x32): 555.42M  |  FLOPs~ 1.11B\n",
      "  Latency: ~0.161 ms / image (batch=128, avg over repeats)\n",
      "\n",
      "[Student (SmallNet)]\n",
      "  Params: 141354 (trainable: 141354) -> 141.35K params\n",
      "  Layers (Conv+FC): 6\n",
      "  MACs (32x32): 29.20M  |  FLOPs~ 58.40M\n",
      "  Latency: ~0.016 ms / image (batch=128, avg over repeats)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  # %% So sánh teacher vs student: tham số, layer, FLOPs/MACs, Latency\n",
    "from collections import defaultdict\n",
    "\n",
    "# Choose the canonical student used by most methods\n",
    "student_ref = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "def count_params(m: nn.Module):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "def count_layers(m: nn.Module):\n",
    "    # count conv + linear layers as \"layers\"\n",
    "    return sum(isinstance(mod, (nn.Conv2d, nn.Linear)) for mod in m.modules())\n",
    "\n",
    "\n",
    "# Lightweight FLOPs/MACs estimation using hooks (counts MACs ~ multiply-adds)\n",
    "def estimate_macs(model: nn.Module, input_size=(1, 3, 32, 32)):\n",
    "    macs = 0\n",
    "\n",
    "    def conv_hook(self, inp, out):\n",
    "        nonlocal macs\n",
    "        # inp[0]: N,Cin,Hin,Win ; out: N,Cout,Hout,Wout\n",
    "        x = inp[0]\n",
    "        N, Cin, Hin, Win = x.shape\n",
    "        Cout, Hout, Wout = out.shape[1:]\n",
    "        kH, kW = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
    "        # MACs per output element: Cin * kH * kW\n",
    "        macs += N * Cout * Hout * Wout * Cin * kH * kW\n",
    "\n",
    "    def linear_hook(self, inp, out):\n",
    "        nonlocal macs\n",
    "        # inp[0]: N, in_features ; out: N, out_features\n",
    "        N, in_f = inp[0].shape\n",
    "        out_f = out.shape[1]\n",
    "        macs += N * in_f * out_f\n",
    "\n",
    "    hooks = []\n",
    "    for mod in model.modules():\n",
    "        if isinstance(mod, nn.Conv2d):\n",
    "            hooks.append(mod.register_forward_hook(conv_hook))\n",
    "        elif isinstance(mod, nn.Linear):\n",
    "            hooks.append(mod.register_forward_hook(linear_hook))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(*input_size, device=DEVICE)\n",
    "        _ = model(dummy)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # FLOPs ~ 2 * MACs if counting MUL+ADD as two ops. Report both.\n",
    "    flops = 2 * macs\n",
    "    return macs, flops\n",
    "\n",
    "\n",
    "def pretty(n):\n",
    "    # format large numbers with units\n",
    "    for unit in [\"\", \"K\", \"M\", \"B\", \"T\"]:\n",
    "        if abs(n) < 1000:\n",
    "            return f\"{n:.2f}{unit}\"\n",
    "        n /= 1000\n",
    "    return f\"{n:.2f}P\"\n",
    "\n",
    "\n",
    "def measure_latency_ms_per_image(model: nn.Module, batch_size: int = 128, repeats: int = 30, warmup: int = 10):\n",
    "    model.eval()\n",
    "    x = torch.randn(batch_size, 3, 32, 32, device=DEVICE)\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    # Timed runs\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeats):\n",
    "            t0 = time.perf_counter()\n",
    "            _ = model(x)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            t1 = time.perf_counter()\n",
    "            times.append((t1 - t0))\n",
    "    avg_s = sum(times) / len(times)\n",
    "    ms_per_image = (avg_s / batch_size) * 1000.0\n",
    "    return ms_per_image\n",
    "\n",
    "\n",
    "def report_model_stats(name: str, m: nn.Module):\n",
    "    total, trainable = count_params(m)\n",
    "    layers = count_layers(m)\n",
    "    macs, flops = estimate_macs(m)\n",
    "    latency = measure_latency_ms_per_image(m)\n",
    "    print(\n",
    "        f\"[{name}]\\n\"\n",
    "        f\"  Params: {total} (trainable: {trainable}) -> {pretty(total)} params\\n\"\n",
    "        f\"  Layers (Conv+FC): {layers}\\n\"\n",
    "        f\"  MACs (32x32): {pretty(macs)}  |  FLOPs~ {pretty(flops)}\\n\"\n",
    "        f\"  Latency: ~{latency:.3f} ms / image (batch=128, avg over repeats)\\n\"\n",
    "    )\n",
    "\n",
    "print(\"So sánh mô hình teacher và student (đầu vào 32x32):\\n\")\n",
    "report_model_stats(\"Teacher (ResNet18)\", teacher)\n",
    "report_model_stats(\"Student (SmallNet)\", student_ref)\n",
    "\n",
    "# Cleanup\n",
    "del student_ref\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac44f878",
   "metadata": {
    "papermill": {
     "duration": 0.016071,
     "end_time": "2025-09-16T09:49:47.177604",
     "exception": false,
     "start_time": "2025-09-16T09:49:47.161533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Baseline: Train-from-scratch cho student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d6d300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T09:49:47.211057Z",
     "iopub.status.busy": "2025-09-16T09:49:47.210809Z",
     "iopub.status.idle": "2025-09-16T09:49:47.228792Z",
     "shell.execute_reply": "2025-09-16T09:49:47.228178Z"
    },
    "papermill": {
     "duration": 0.035995,
     "end_time": "2025-09-16T09:49:47.229973",
     "exception": false,
     "start_time": "2025-09-16T09:49:47.193978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check] SmallNet fresh param-sum: s1=646.114041, s2=646.114041, equal=True\n"
     ]
    }
   ],
   "source": [
    "# %% Integrity check: verify fresh students start identically\n",
    "with torch.no_grad():\n",
    "    s1 = make_fresh_student(SmallNet)\n",
    "    s2 = make_fresh_student(SmallNet)\n",
    "    # Sum of all FP params as a quick fingerprint\n",
    "    def param_sum(m):\n",
    "        s = 0.0\n",
    "        for p in m.state_dict().values():\n",
    "            if torch.is_floating_point(p):\n",
    "                s += p.float().sum().item()\n",
    "        return s\n",
    "    ps1, ps2 = param_sum(s1), param_sum(s2)\n",
    "    print(f\"[Check] SmallNet fresh param-sum: s1={ps1:.6f}, s2={ps2:.6f}, equal={abs(ps1-ps2)<1e-6}\")\n",
    "\n",
    "# Clean up temporary models to save memory\n",
    "del s1, s2\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff6d5ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T09:49:47.264004Z",
     "iopub.status.busy": "2025-09-16T09:49:47.263486Z",
     "iopub.status.idle": "2025-09-16T09:58:02.399149Z",
     "shell.execute_reply": "2025-09-16T09:58:02.398232Z"
    },
    "papermill": {
     "duration": 495.154255,
     "end_time": "2025-09-16T09:58:02.400524",
     "exception": false,
     "start_time": "2025-09-16T09:49:47.246269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - train_acc: 0.4394 - val_acc: 0.4862 (best)\n",
      "Epoch 2/50 - train_acc: 0.6098 - val_acc: 0.5700 (best)\n",
      "Epoch 3/50 - train_acc: 0.6664 - val_acc: 0.5820 (best)\n",
      "Epoch 4/50 - train_acc: 0.7068 - val_acc: 0.6018 (best)\n",
      "Epoch 5/50 - train_acc: 0.7244 - val_acc: 0.6732 (best)\n",
      "Epoch 6/50 - train_acc: 0.7403 - val_acc: 0.6302\n",
      "Epoch 7/50 - train_acc: 0.7450 - val_acc: 0.6380\n",
      "Epoch 8/50 - train_acc: 0.7509 - val_acc: 0.7254 (best)\n",
      "Epoch 9/50 - train_acc: 0.7560 - val_acc: 0.6414\n",
      "Epoch 10/50 - train_acc: 0.7623 - val_acc: 0.7294 (best)\n",
      "Epoch 11/50 - train_acc: 0.7630 - val_acc: 0.7556 (best)\n",
      "Epoch 12/50 - train_acc: 0.7696 - val_acc: 0.7164\n",
      "Epoch 13/50 - train_acc: 0.7718 - val_acc: 0.6658\n",
      "Epoch 14/50 - train_acc: 0.7744 - val_acc: 0.5532\n",
      "Epoch 15/50 - train_acc: 0.7773 - val_acc: 0.7294\n",
      "Epoch 16/50 - train_acc: 0.7837 - val_acc: 0.7128\n",
      "Epoch 17/50 - train_acc: 0.7862 - val_acc: 0.7236\n",
      "Epoch 18/50 - train_acc: 0.7896 - val_acc: 0.7264\n",
      "Epoch 19/50 - train_acc: 0.7920 - val_acc: 0.7574 (best)\n",
      "Epoch 20/50 - train_acc: 0.7980 - val_acc: 0.7554\n",
      "Epoch 21/50 - train_acc: 0.7972 - val_acc: 0.7600 (best)\n",
      "Epoch 22/50 - train_acc: 0.8025 - val_acc: 0.7798 (best)\n",
      "Epoch 23/50 - train_acc: 0.8055 - val_acc: 0.7546\n",
      "Epoch 24/50 - train_acc: 0.8037 - val_acc: 0.7472\n",
      "Epoch 25/50 - train_acc: 0.8102 - val_acc: 0.7896 (best)\n",
      "Epoch 26/50 - train_acc: 0.8156 - val_acc: 0.6162\n",
      "Epoch 27/50 - train_acc: 0.8167 - val_acc: 0.7672\n",
      "Epoch 28/50 - train_acc: 0.8244 - val_acc: 0.8024 (best)\n",
      "Epoch 29/50 - train_acc: 0.8280 - val_acc: 0.8186 (best)\n",
      "Epoch 30/50 - train_acc: 0.8300 - val_acc: 0.8026\n",
      "Epoch 31/50 - train_acc: 0.8347 - val_acc: 0.7810\n",
      "Epoch 32/50 - train_acc: 0.8374 - val_acc: 0.8044\n",
      "Epoch 33/50 - train_acc: 0.8440 - val_acc: 0.8130\n",
      "Epoch 34/50 - train_acc: 0.8476 - val_acc: 0.7948\n",
      "Epoch 35/50 - train_acc: 0.8533 - val_acc: 0.8104\n",
      "Epoch 36/50 - train_acc: 0.8568 - val_acc: 0.8388 (best)\n",
      "Epoch 37/50 - train_acc: 0.8643 - val_acc: 0.8432 (best)\n",
      "Epoch 38/50 - train_acc: 0.8711 - val_acc: 0.8244\n",
      "Epoch 39/50 - train_acc: 0.8731 - val_acc: 0.8464 (best)\n",
      "Epoch 40/50 - train_acc: 0.8786 - val_acc: 0.8672 (best)\n",
      "Epoch 41/50 - train_acc: 0.8826 - val_acc: 0.8720 (best)\n",
      "Epoch 42/50 - train_acc: 0.8886 - val_acc: 0.8690\n",
      "Epoch 43/50 - train_acc: 0.8949 - val_acc: 0.8708\n",
      "Epoch 44/50 - train_acc: 0.8989 - val_acc: 0.8842 (best)\n",
      "Epoch 45/50 - train_acc: 0.9044 - val_acc: 0.8810\n",
      "Epoch 46/50 - train_acc: 0.9103 - val_acc: 0.8868 (best)\n",
      "Epoch 47/50 - train_acc: 0.9128 - val_acc: 0.8844\n",
      "Epoch 48/50 - train_acc: 0.9193 - val_acc: 0.8870 (best)\n",
      "Epoch 49/50 - train_acc: 0.9195 - val_acc: 0.8906 (best)\n",
      "Epoch 50/50 - train_acc: 0.9204 - val_acc: 0.8884\n",
      "Saved student checkpoint (v0) to ./checkpoints/kd_student_v0.pth\n",
      "{'method': 'Baseline CE (no teacher)', 'train_time_sec': 485.04, 'train_acc': 0.9287, 'val_acc': 0.8906, 'test_acc': 0.8801}\n"
     ]
    }
   ],
   "source": [
    "# %% Train student WITHOUT teacher (baseline CE) and save kd_student_v0.pth\n",
    "# Hyperparams\n",
    "LR = 0.1\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "# Build student from the same initial weights for fair comparison\n",
    "student = make_fresh_student(SmallNet)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    # one epoch CE-only\n",
    "    train_acc_epoch = train_ce(student, train_loader, optimizer, DEVICE, scaler=scaler)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save checkpoint\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v0.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v0) to {ckpt_path}\")\n",
    "\n",
    "# Final report\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Baseline CE (no teacher)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea5cd5",
   "metadata": {
    "id": "81a6e5cb",
    "papermill": {
     "duration": 0.018385,
     "end_time": "2025-09-16T09:58:02.438065",
     "exception": false,
     "start_time": "2025-09-16T09:58:02.419680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 1: Vanilla KD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcb0ae2",
   "metadata": {
    "papermill": {
     "duration": 0.018267,
     "end_time": "2025-09-16T09:58:02.475296",
     "exception": false,
     "start_time": "2025-09-16T09:58:02.457029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Để student học theo \"soft targets\" của  thay vì chỉ dựa vào nhãn cứng. Soft targets giữ lại thông tin về độ tự tin giữa các lớp.\n",
    "\n",
    "Ký hiệu cho một mẫu $(x, y)$:\n",
    "- $z_t$ = logits của teacher, $z_s$ = logits của student.\n",
    "- Nhiệt độ (temperature) $\\tau>0$ làm mềm phân phối:  \n",
    "  $$\n",
    "  p_t^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_t}{\\tau}\\right), \\quad\n",
    "  p_s^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_s}{\\tau}\\right).\n",
    "  $$\n",
    "\n",
    "Hàm mất mát KD dùng KL-divergence giữa phân phối mềm của teacher và student, có hệ số hiệu chỉnh $\\tau^2$:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{KD}} = \\tau^2 \\, \\mathrm{KL}\\big( p_t^{(\\tau)} \\,\\Vert\\, p_s^{(\\tau)} \\big).\n",
    "$$\n",
    "\n",
    "Kết hợp với cross-entropy (CE) chuẩn theo nhãn thật $y$:\n",
    "$$\n",
    "\\mathcal{L} = \\alpha\\,\\mathcal{L}_{\\mathrm{KD}} + (1-\\alpha)\\, \\mathrm{CE}(z_s, y).\n",
    "$$\n",
    "- $\\alpha\\in[0,1]$ điều chỉnh tỷ trọng giữa “học theo giáo viên” và “học theo nhãn thật”.\n",
    "- $\\tau$ lớn làm phân phối mềm hơn (giảm cực đoan), giúp student học được cấu trúc liên lớp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a93ee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T09:58:02.513881Z",
     "iopub.status.busy": "2025-09-16T09:58:02.513601Z",
     "iopub.status.idle": "2025-09-16T10:09:15.628602Z",
     "shell.execute_reply": "2025-09-16T10:09:15.627639Z"
    },
    "id": "9950493c",
    "outputId": "41b0cf57-a916-48b3-fb0c-dd1a651d53e4",
    "papermill": {
     "duration": 673.222903,
     "end_time": "2025-09-16T10:09:15.717058",
     "exception": false,
     "start_time": "2025-09-16T09:58:02.494155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - train_acc: 0.4130 - val_acc: 0.4678 (best)\n",
      "Epoch 2/50 - train_acc: 0.6010 - val_acc: 0.5284 (best)\n",
      "Epoch 3/50 - train_acc: 0.6625 - val_acc: 0.5658 (best)\n",
      "Epoch 4/50 - train_acc: 0.7088 - val_acc: 0.6158 (best)\n",
      "Epoch 5/50 - train_acc: 0.7329 - val_acc: 0.7044 (best)\n",
      "Epoch 6/50 - train_acc: 0.7474 - val_acc: 0.7432 (best)\n",
      "Epoch 7/50 - train_acc: 0.7596 - val_acc: 0.6774\n",
      "Epoch 8/50 - train_acc: 0.7666 - val_acc: 0.7080\n",
      "Epoch 9/50 - train_acc: 0.7721 - val_acc: 0.7544 (best)\n",
      "Epoch 10/50 - train_acc: 0.7746 - val_acc: 0.7536\n",
      "Epoch 11/50 - train_acc: 0.7797 - val_acc: 0.7062\n",
      "Epoch 12/50 - train_acc: 0.7838 - val_acc: 0.7290\n",
      "Epoch 13/50 - train_acc: 0.7880 - val_acc: 0.7544\n",
      "Epoch 14/50 - train_acc: 0.7917 - val_acc: 0.7542\n",
      "Epoch 15/50 - train_acc: 0.7932 - val_acc: 0.7382\n",
      "Epoch 16/50 - train_acc: 0.7972 - val_acc: 0.7422\n",
      "Epoch 17/50 - train_acc: 0.7995 - val_acc: 0.6826\n",
      "Epoch 18/50 - train_acc: 0.8034 - val_acc: 0.7902 (best)\n",
      "Epoch 19/50 - train_acc: 0.8083 - val_acc: 0.6840\n",
      "Epoch 20/50 - train_acc: 0.8100 - val_acc: 0.7914 (best)\n",
      "Epoch 21/50 - train_acc: 0.8134 - val_acc: 0.7238\n",
      "Epoch 22/50 - train_acc: 0.8167 - val_acc: 0.7862\n",
      "Epoch 23/50 - train_acc: 0.8202 - val_acc: 0.7650\n",
      "Epoch 24/50 - train_acc: 0.8229 - val_acc: 0.7586\n",
      "Epoch 25/50 - train_acc: 0.8255 - val_acc: 0.7728\n",
      "Epoch 26/50 - train_acc: 0.8282 - val_acc: 0.7630\n",
      "Epoch 27/50 - train_acc: 0.8326 - val_acc: 0.7948 (best)\n",
      "Epoch 28/50 - train_acc: 0.8358 - val_acc: 0.7886\n",
      "Epoch 29/50 - train_acc: 0.8407 - val_acc: 0.7972 (best)\n",
      "Epoch 30/50 - train_acc: 0.8434 - val_acc: 0.7714\n",
      "Epoch 31/50 - train_acc: 0.8477 - val_acc: 0.7730\n",
      "Epoch 32/50 - train_acc: 0.8523 - val_acc: 0.7906\n",
      "Epoch 33/50 - train_acc: 0.8568 - val_acc: 0.8234 (best)\n",
      "Epoch 34/50 - train_acc: 0.8586 - val_acc: 0.7558\n",
      "Epoch 35/50 - train_acc: 0.8621 - val_acc: 0.7782\n",
      "Epoch 36/50 - train_acc: 0.8681 - val_acc: 0.8404 (best)\n",
      "Epoch 37/50 - train_acc: 0.8715 - val_acc: 0.8434 (best)\n",
      "Epoch 38/50 - train_acc: 0.8772 - val_acc: 0.8464 (best)\n",
      "Epoch 39/50 - train_acc: 0.8786 - val_acc: 0.8512 (best)\n",
      "Epoch 40/50 - train_acc: 0.8869 - val_acc: 0.8554 (best)\n",
      "Epoch 41/50 - train_acc: 0.8909 - val_acc: 0.8592 (best)\n",
      "Epoch 42/50 - train_acc: 0.8947 - val_acc: 0.8378\n",
      "Epoch 43/50 - train_acc: 0.8997 - val_acc: 0.8756 (best)\n",
      "Epoch 44/50 - train_acc: 0.9048 - val_acc: 0.8826 (best)\n",
      "Epoch 45/50 - train_acc: 0.9093 - val_acc: 0.8850 (best)\n",
      "Epoch 46/50 - train_acc: 0.9120 - val_acc: 0.8820\n",
      "Epoch 47/50 - train_acc: 0.9135 - val_acc: 0.8898 (best)\n",
      "Epoch 48/50 - train_acc: 0.9187 - val_acc: 0.8876\n",
      "Epoch 49/50 - train_acc: 0.9205 - val_acc: 0.8926 (best)\n",
      "Epoch 50/50 - train_acc: 0.9207 - val_acc: 0.8914\n",
      "Saved student checkpoint (v1) to ./checkpoints/kd_student_v1.pth\n",
      "{'method': 'Vanilla KD', 'train_time_sec': 663.11, 'train_acc': 0.9259, 'val_acc': 0.8926, 'test_acc': 0.8812}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Vanilla KD\n",
    "# Hyperparams for KD\n",
    "T = 4.0\n",
    "ALPHA = 0.7\n",
    "LR = 0.1\n",
    "\n",
    "# KD loss function for Vanilla KD\n",
    "def kd_loss_vanilla(logits_s, logits_t, y, T=4.0, alpha=0.5):\n",
    "    ce = F.cross_entropy(logits_s, y)\n",
    "    p_s = F.log_softmax(logits_s / T, dim=1)\n",
    "    p_t = F.softmax(logits_t / T, dim=1)\n",
    "    kd = F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "    return alpha * kd + (1 - alpha) * ce\n",
    "\n",
    "student = make_fresh_student(SmallNet)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = kd_loss_vanilla(logits_s, logits_t, y, T=T, alpha=ALPHA)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 1\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v1.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v1) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Vanilla KD\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1f6a7",
   "metadata": {
    "id": "afb52880",
    "papermill": {
     "duration": 0.020219,
     "end_time": "2025-09-16T10:09:15.757413",
     "exception": false,
     "start_time": "2025-09-16T10:09:15.737194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 2: Hard-label Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09527ed4",
   "metadata": {
    "papermill": {
     "duration": 0.020438,
     "end_time": "2025-09-16T10:09:15.797924",
     "exception": false,
     "start_time": "2025-09-16T10:09:15.777486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Biến dự đoán của teacher thành ''nhãn cứng'' giả (pseudo-label) rồi kết hợp với nhãn thật.\n",
    "\n",
    "Với một mẫu $(x, y)$:\n",
    "- $z_t$ = logits của teacher, nhãn giả của teacher là:  \n",
    "  $$\n",
    "  \\tilde{y} = \\arg\\max\\limits_{c}\\; z_{t,c}.\n",
    "  $$\n",
    "- $z_s$ = logits của student.\n",
    "\n",
    "Hàm mất mát kết hợp hai cross-entropy:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{CEE}} = \\beta\\,\\mathrm{CE}(z_s, \\tilde{y}) + (1-\\beta)\\, \\mathrm{CE}(z_s, y).\n",
    "$$\n",
    "- Thành phần $\\mathrm{CE}(z_s, y)$ giúp bám sát nhãn thật.\n",
    "- Thành phần $\\mathrm{CE}(z_s, \\tilde{y})$ ép student bắt chước dự đoán mạnh nhất của teacher (hard target).\n",
    "- $\\beta$ điều chỉnh mức tin cậy vào teacher.\n",
    "\n",
    "Khác với Vanilla KD, CEE không dùng nhiệt độ hay phân phối mềm; nó chỉ dựa vào lớp có xác suất cao nhất của teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f8ab6d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T10:09:15.840293Z",
     "iopub.status.busy": "2025-09-16T10:09:15.839724Z",
     "iopub.status.idle": "2025-09-16T10:20:30.168932Z",
     "shell.execute_reply": "2025-09-16T10:20:30.167982Z"
    },
    "id": "403fd21e",
    "outputId": "c6e0b59b-97a2-47a1-b050-bf9fd850b797",
    "papermill": {
     "duration": 674.373615,
     "end_time": "2025-09-16T10:20:30.191988",
     "exception": false,
     "start_time": "2025-09-16T10:09:15.818373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - train_acc: 0.4375 - val_acc: 0.5446 (best)\n",
      "Epoch 2/50 - train_acc: 0.6094 - val_acc: 0.5758 (best)\n",
      "Epoch 3/50 - train_acc: 0.6667 - val_acc: 0.6786 (best)\n",
      "Epoch 4/50 - train_acc: 0.7027 - val_acc: 0.6464\n",
      "Epoch 5/50 - train_acc: 0.7272 - val_acc: 0.6508\n",
      "Epoch 6/50 - train_acc: 0.7344 - val_acc: 0.6366\n",
      "Epoch 7/50 - train_acc: 0.7443 - val_acc: 0.6668\n",
      "Epoch 8/50 - train_acc: 0.7513 - val_acc: 0.7452 (best)\n",
      "Epoch 9/50 - train_acc: 0.7554 - val_acc: 0.6414\n",
      "Epoch 10/50 - train_acc: 0.7648 - val_acc: 0.5684\n",
      "Epoch 11/50 - train_acc: 0.7657 - val_acc: 0.6922\n",
      "Epoch 12/50 - train_acc: 0.7708 - val_acc: 0.7084\n",
      "Epoch 13/50 - train_acc: 0.7719 - val_acc: 0.7016\n",
      "Epoch 14/50 - train_acc: 0.7764 - val_acc: 0.7346\n",
      "Epoch 15/50 - train_acc: 0.7777 - val_acc: 0.6736\n",
      "Epoch 16/50 - train_acc: 0.7817 - val_acc: 0.7276\n",
      "Epoch 17/50 - train_acc: 0.7857 - val_acc: 0.7116\n",
      "Epoch 18/50 - train_acc: 0.7884 - val_acc: 0.6898\n",
      "Epoch 19/50 - train_acc: 0.7890 - val_acc: 0.7534 (best)\n",
      "Epoch 20/50 - train_acc: 0.7994 - val_acc: 0.7100\n",
      "Epoch 21/50 - train_acc: 0.7987 - val_acc: 0.7598 (best)\n",
      "Epoch 22/50 - train_acc: 0.8050 - val_acc: 0.7266\n",
      "Epoch 23/50 - train_acc: 0.8049 - val_acc: 0.7402\n",
      "Epoch 24/50 - train_acc: 0.8045 - val_acc: 0.7508\n",
      "Epoch 25/50 - train_acc: 0.8158 - val_acc: 0.7666 (best)\n",
      "Epoch 26/50 - train_acc: 0.8203 - val_acc: 0.7300\n",
      "Epoch 27/50 - train_acc: 0.8199 - val_acc: 0.7846 (best)\n",
      "Epoch 28/50 - train_acc: 0.8228 - val_acc: 0.7536\n",
      "Epoch 29/50 - train_acc: 0.8278 - val_acc: 0.7944 (best)\n",
      "Epoch 30/50 - train_acc: 0.8318 - val_acc: 0.8046 (best)\n",
      "Epoch 31/50 - train_acc: 0.8363 - val_acc: 0.7934\n",
      "Epoch 32/50 - train_acc: 0.8380 - val_acc: 0.7792\n",
      "Epoch 33/50 - train_acc: 0.8445 - val_acc: 0.8292 (best)\n",
      "Epoch 34/50 - train_acc: 0.8500 - val_acc: 0.7836\n",
      "Epoch 35/50 - train_acc: 0.8553 - val_acc: 0.8114\n",
      "Epoch 36/50 - train_acc: 0.8574 - val_acc: 0.8380 (best)\n",
      "Epoch 37/50 - train_acc: 0.8631 - val_acc: 0.8276\n",
      "Epoch 38/50 - train_acc: 0.8697 - val_acc: 0.8404 (best)\n",
      "Epoch 39/50 - train_acc: 0.8738 - val_acc: 0.8552 (best)\n",
      "Epoch 40/50 - train_acc: 0.8811 - val_acc: 0.8636 (best)\n",
      "Epoch 41/50 - train_acc: 0.8856 - val_acc: 0.8616\n",
      "Epoch 42/50 - train_acc: 0.8918 - val_acc: 0.8694 (best)\n",
      "Epoch 43/50 - train_acc: 0.8973 - val_acc: 0.8602\n",
      "Epoch 44/50 - train_acc: 0.9005 - val_acc: 0.8686\n",
      "Epoch 45/50 - train_acc: 0.9060 - val_acc: 0.8844 (best)\n",
      "Epoch 46/50 - train_acc: 0.9104 - val_acc: 0.8854 (best)\n",
      "Epoch 47/50 - train_acc: 0.9145 - val_acc: 0.8860 (best)\n",
      "Epoch 48/50 - train_acc: 0.9178 - val_acc: 0.8896 (best)\n",
      "Epoch 49/50 - train_acc: 0.9204 - val_acc: 0.8898 (best)\n",
      "Epoch 50/50 - train_acc: 0.9218 - val_acc: 0.8912 (best)\n",
      "Saved student checkpoint (v2) to ./checkpoints/kd_student_v2.pth\n",
      "{'method': 'Hard-label Distillation (CEE)', 'train_time_sec': 664.33, 'train_acc': 0.9287, 'val_acc': 0.8912, 'test_acc': 0.8847}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Hard-label Distillation (CEE)\n",
    "BETA = 0.7\n",
    "LR = 0.1\n",
    "\n",
    "# CEE loss function\n",
    "def cee_loss(logits_s, logits_t, y, beta=0.7):\n",
    "    # CEE: combine CE(student, y) and CE(student, teacher_label)\n",
    "    ce_y = F.cross_entropy(logits_s, y)\n",
    "    pseudo = logits_t.argmax(dim=1)\n",
    "    ce_t = F.cross_entropy(logits_s, pseudo)\n",
    "    return beta * ce_t + (1 - beta) * ce_y\n",
    "\n",
    "student = make_fresh_student(SmallNet)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = cee_loss(logits_s, logits_t, y, beta=BETA)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 2\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v2.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v2) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Hard-label Distillation (CEE)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d3c3d",
   "metadata": {
    "id": "4d6604d1",
    "papermill": {
     "duration": 0.021912,
     "end_time": "2025-09-16T10:20:30.236259",
     "exception": false,
     "start_time": "2025-09-16T10:20:30.214347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 3: Feature Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f1127",
   "metadata": {
    "papermill": {
     "duration": 0.021999,
     "end_time": "2025-09-16T10:20:30.280324",
     "exception": false,
     "start_time": "2025-09-16T10:20:30.258325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Khớp đặc trưng trung gian (feature maps) giữa teacher và student để student học biểu diễn gần giống teacher.\n",
    "\n",
    "Ký hiệu theo minibatch:\n",
    "- $F_t \\in \\mathbb{R}^{N\\times C_t\\times H_t\\times W_t}$: đặc trưng của teacher ở một tầng (ví dụ layer cuối conv).\n",
    "- $F_s \\in \\mathbb{R}^{N\\times C_s\\times H_s\\times W_s}$: đặc trưng của student ở tầng tương ứng.\n",
    "- Do số kênh/không gian khác nhau, ta dùng một “đầu chiếu” $g_s(\\cdot)$ để đưa $F_s$ về không gian của $F_t$ và/hoặc nội suy không gian về cùng kích thước.\n",
    "- Chuẩn hoá theo kênh để giảm lệch về biên độ:  \n",
    "  $$\n",
    "  \\widehat{F} = \\frac{F}{\\sqrt{\\sum\\limits_{c} F_c^2}+\\varepsilon}.\n",
    "  $$\n",
    "\n",
    "Hàm mất mát tổng hợp:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s, y) + \\lambda_{\\mathrm{feat}}\\, \\big\\|\\, \\widehat{g_s(F_s)} - \\widehat{F_t} \\,\\big\\|_2^2.\n",
    "$$\n",
    "- CE đảm bảo mục tiêu phân loại; \n",
    "- MSE giữa đặc trưng đã chuẩn hoá giúp student học cấu trúc biểu diễn của teacher.\n",
    "- Trong thực nghiệm thường tăng dần hệ số $\\lambda_{\\mathrm{feat}}$ (ramp-up) để ổn định huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39ed111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T10:20:30.325627Z",
     "iopub.status.busy": "2025-09-16T10:20:30.325299Z",
     "iopub.status.idle": "2025-09-16T10:32:08.562030Z",
     "shell.execute_reply": "2025-09-16T10:32:08.561011Z"
    },
    "id": "3e2de962",
    "outputId": "9322bf2f-8e29-4e06-f2f1-f8bb5422e915",
    "papermill": {
     "duration": 698.284808,
     "end_time": "2025-09-16T10:32:08.587012",
     "exception": false,
     "start_time": "2025-09-16T10:20:30.302204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - train_acc: 0.4398 - val_acc: 0.3680 - feat_w: 2.0 (best)\n",
      "Epoch 2/50 - train_acc: 0.6138 - val_acc: 0.6356 - feat_w: 4.0 (best)\n",
      "Epoch 3/50 - train_acc: 0.6712 - val_acc: 0.6574 - feat_w: 6.0 (best)\n",
      "Epoch 4/50 - train_acc: 0.7068 - val_acc: 0.5892 - feat_w: 8.0\n",
      "Epoch 5/50 - train_acc: 0.7290 - val_acc: 0.6930 - feat_w: 10.0 (best)\n",
      "Epoch 6/50 - train_acc: 0.7405 - val_acc: 0.7102 - feat_w: 12.0 (best)\n",
      "Epoch 7/50 - train_acc: 0.7483 - val_acc: 0.7180 - feat_w: 14.0 (best)\n",
      "Epoch 8/50 - train_acc: 0.7558 - val_acc: 0.6428 - feat_w: 16.0\n",
      "Epoch 9/50 - train_acc: 0.7586 - val_acc: 0.7246 - feat_w: 18.0 (best)\n",
      "Epoch 10/50 - train_acc: 0.7636 - val_acc: 0.6852 - feat_w: 20.0\n",
      "Epoch 11/50 - train_acc: 0.7664 - val_acc: 0.6444 - feat_w: 22.0\n",
      "Epoch 12/50 - train_acc: 0.7698 - val_acc: 0.7330 - feat_w: 24.0 (best)\n",
      "Epoch 13/50 - train_acc: 0.7744 - val_acc: 0.7654 - feat_w: 26.0 (best)\n",
      "Epoch 14/50 - train_acc: 0.7789 - val_acc: 0.6658 - feat_w: 28.0\n",
      "Epoch 15/50 - train_acc: 0.7785 - val_acc: 0.7266 - feat_w: 30.0\n",
      "Epoch 16/50 - train_acc: 0.7827 - val_acc: 0.7794 - feat_w: 32.0 (best)\n",
      "Epoch 17/50 - train_acc: 0.7872 - val_acc: 0.6334 - feat_w: 34.0\n",
      "Epoch 18/50 - train_acc: 0.7902 - val_acc: 0.7384 - feat_w: 36.0\n",
      "Epoch 19/50 - train_acc: 0.7938 - val_acc: 0.7392 - feat_w: 38.0\n",
      "Epoch 20/50 - train_acc: 0.7974 - val_acc: 0.7376 - feat_w: 40.0\n",
      "Epoch 21/50 - train_acc: 0.7972 - val_acc: 0.7492 - feat_w: 42.0\n",
      "Epoch 22/50 - train_acc: 0.8046 - val_acc: 0.7818 - feat_w: 44.0 (best)\n",
      "Epoch 23/50 - train_acc: 0.8076 - val_acc: 0.6892 - feat_w: 46.0\n",
      "Epoch 24/50 - train_acc: 0.8095 - val_acc: 0.7748 - feat_w: 48.0\n",
      "Epoch 25/50 - train_acc: 0.8155 - val_acc: 0.7094 - feat_w: 50.0\n",
      "Epoch 26/50 - train_acc: 0.8170 - val_acc: 0.7366 - feat_w: 50.0\n",
      "Epoch 27/50 - train_acc: 0.8214 - val_acc: 0.7642 - feat_w: 50.0\n",
      "Epoch 28/50 - train_acc: 0.8242 - val_acc: 0.7574 - feat_w: 50.0\n",
      "Epoch 29/50 - train_acc: 0.8278 - val_acc: 0.7992 - feat_w: 50.0 (best)\n",
      "Epoch 30/50 - train_acc: 0.8305 - val_acc: 0.7968 - feat_w: 50.0\n",
      "Epoch 31/50 - train_acc: 0.8376 - val_acc: 0.8050 - feat_w: 50.0 (best)\n",
      "Epoch 32/50 - train_acc: 0.8403 - val_acc: 0.7988 - feat_w: 50.0\n",
      "Epoch 33/50 - train_acc: 0.8464 - val_acc: 0.8180 - feat_w: 50.0 (best)\n",
      "Epoch 34/50 - train_acc: 0.8460 - val_acc: 0.8340 - feat_w: 50.0 (best)\n",
      "Epoch 35/50 - train_acc: 0.8559 - val_acc: 0.8524 - feat_w: 50.0 (best)\n",
      "Epoch 36/50 - train_acc: 0.8590 - val_acc: 0.8562 - feat_w: 50.0 (best)\n",
      "Epoch 37/50 - train_acc: 0.8635 - val_acc: 0.8544 - feat_w: 50.0\n",
      "Epoch 38/50 - train_acc: 0.8694 - val_acc: 0.8280 - feat_w: 50.0\n",
      "Epoch 39/50 - train_acc: 0.8741 - val_acc: 0.8520 - feat_w: 50.0\n",
      "Epoch 40/50 - train_acc: 0.8806 - val_acc: 0.8436 - feat_w: 50.0\n",
      "Epoch 41/50 - train_acc: 0.8840 - val_acc: 0.8704 - feat_w: 50.0 (best)\n",
      "Epoch 42/50 - train_acc: 0.8902 - val_acc: 0.8694 - feat_w: 50.0\n",
      "Epoch 43/50 - train_acc: 0.8960 - val_acc: 0.8488 - feat_w: 50.0\n",
      "Epoch 44/50 - train_acc: 0.8990 - val_acc: 0.8754 - feat_w: 50.0 (best)\n",
      "Epoch 45/50 - train_acc: 0.9064 - val_acc: 0.8790 - feat_w: 50.0 (best)\n",
      "Epoch 46/50 - train_acc: 0.9098 - val_acc: 0.8782 - feat_w: 50.0\n",
      "Epoch 47/50 - train_acc: 0.9161 - val_acc: 0.8842 - feat_w: 50.0 (best)\n",
      "Epoch 48/50 - train_acc: 0.9164 - val_acc: 0.8872 - feat_w: 50.0 (best)\n",
      "Epoch 49/50 - train_acc: 0.9204 - val_acc: 0.8860 - feat_w: 50.0\n",
      "Epoch 50/50 - train_acc: 0.9199 - val_acc: 0.8886 - feat_w: 50.0 (best)\n",
      "Saved student checkpoint (v3) to ./checkpoints/kd_student_v3.pth\n",
      "{'method': 'Feature Distillation (MSE)', 'train_time_sec': 688.28, 'train_acc': 0.9288, 'val_acc': 0.8886, 'test_acc': 0.8802}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Feature Distillation (penultimate features)\n",
    "# We'll tap student at its final conv output (before average pool) and teacher at layer4 output.\n",
    "LR = 0.1\n",
    "W_FEAT = 50.0  # lower weight; we will ramp it up during training\n",
    "\n",
    "# Keep student architecture unchanged; just expose features\n",
    "class StudentExposeFeat(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)  # N,128,4,4\n",
    "        logits = self.classifier(f)\n",
    "        return logits, f\n",
    "\n",
    "student = make_fresh_student(StudentExposeFeat)\n",
    "\n",
    "# Teacher hook at last conv block (layer4)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Separate projection head (FitNet-style regressor) outside the student\n",
    "proj_s = nn.Sequential(\n",
    "    nn.Conv2d(128, 512, kernel_size=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace=True),\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.SGD(list(student.parameters()) + list(proj_s.parameters()), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# channel-wise L2 normalization helper\n",
    "def norm_channel(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    return x / (x.pow(2).sum(dim=1, keepdim=True).sqrt().clamp_min(eps))\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train(); proj_s.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    # linear ramp-up for feature loss (first half epochs)\n",
    "    ramp = min(1.0, (epoch + 1) / max(1, KD_EPOCHS // 2))\n",
    "    feat_w = W_FEAT * ramp\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook_t.feat\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            f_s_proj = proj_s(f_s)  # project to 512 channels\n",
    "            # Align spatial dims if needed using adaptive avgpool to teacher spatial size\n",
    "            if f_t is None:\n",
    "                raise RuntimeError(\"Teacher feature hook not captured.\")\n",
    "            if f_s_proj.shape[-2:] != f_t.shape[-2:]:\n",
    "                f_s_resized = F.adaptive_avg_pool2d(f_s_proj, f_t.shape[-2:])\n",
    "            else:\n",
    "                f_s_resized = f_s_proj\n",
    "            # Normalize features along channel dimension to reduce scale mismatch\n",
    "            nf_s = norm_channel(f_s_resized)\n",
    "            nf_t = norm_channel(f_t)\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            loss_feat = F.mse_loss(nf_s, nf_t.detach())\n",
    "            loss = loss_ce + feat_w * loss_feat\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    student.eval(); proj_s.eval()\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = {\n",
    "            'student': copy.deepcopy(student.state_dict()),\n",
    "            'proj_s': copy.deepcopy(proj_s.state_dict()),\n",
    "        }\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f} - feat_w: {feat_w:.1f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state['student'])\n",
    "    proj_s.load_state_dict(best_state['proj_s'])\n",
    "\n",
    "# Save student checkpoint for method 3\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v3.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v3) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Feature Distillation (MSE)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc199f",
   "metadata": {
    "id": "bfb25fc4",
    "papermill": {
     "duration": 0.023369,
     "end_time": "2025-09-16T10:32:08.634409",
     "exception": false,
     "start_time": "2025-09-16T10:32:08.611040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 4: Attention Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3575a7f",
   "metadata": {
    "papermill": {
     "duration": 0.023923,
     "end_time": "2025-09-16T10:32:08.682379",
     "exception": false,
     "start_time": "2025-09-16T10:32:08.658456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Khớp toàn bộ feature, AT khớp \"bản đồ chú ý\" (attention map) – độ mạnh tổng hợp theo kênh ở từng vị trí không gian.\n",
    "\n",
    "Cho feature $F \\in \\mathbb{R}^{N\\times C\\times H\\times W}$, bản đồ chú ý $A \\in \\mathbb{R}^{N\\times H\\times W}$ được định nghĩa (một biến thể hay dùng):\n",
    "$$\n",
    "A = \\frac{\\tfrac{1}{C}\\sum\\limits_{c=1}^C F_c^2}{\\left\\|\\, \\tfrac{1}{C}\\sum\\limits_{c=1}^C F_c^2 \\right\\|_2 + \\varepsilon}.\n",
    "$$\n",
    "\n",
    "Với $A_s, A_t$ lần lượt của student và teacher (đã chuẩn hoá), hàm mất mát AT:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{AT}} = \\lambda_{\\mathrm{AT}}\\, \\big\\| A_s - A_t \\big\\|_2^2.\n",
    "$$\n",
    "Tổng mất mát:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s, y) + \\mathcal{L}_{\\mathrm{AT}}.\n",
    "$$\n",
    "\n",
    "AT truyền \"nơi nào quan trọng\" trong ảnh theo teacher. Student học tập trung vào vùng hữu ích thay vì khớp mọi chi tiết của feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d96325b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T10:32:08.733064Z",
     "iopub.status.busy": "2025-09-16T10:32:08.732751Z",
     "iopub.status.idle": "2025-09-16T10:43:26.614662Z",
     "shell.execute_reply": "2025-09-16T10:43:26.613690Z"
    },
    "id": "c10bc1cb",
    "outputId": "9b99cce7-8dbc-439e-d55f-319d44f92554",
    "papermill": {
     "duration": 677.936135,
     "end_time": "2025-09-16T10:43:26.642435",
     "exception": false,
     "start_time": "2025-09-16T10:32:08.706300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - train_acc: 0.3660 - val_acc: 0.4314 (best)\n",
      "Epoch 2/50 - train_acc: 0.5391 - val_acc: 0.5118 (best)\n",
      "Epoch 3/50 - train_acc: 0.6142 - val_acc: 0.5558 (best)\n",
      "Epoch 4/50 - train_acc: 0.6563 - val_acc: 0.5556\n",
      "Epoch 5/50 - train_acc: 0.6868 - val_acc: 0.5908 (best)\n",
      "Epoch 6/50 - train_acc: 0.7080 - val_acc: 0.5850\n",
      "Epoch 7/50 - train_acc: 0.7301 - val_acc: 0.6136 (best)\n",
      "Epoch 8/50 - train_acc: 0.7396 - val_acc: 0.7324 (best)\n",
      "Epoch 9/50 - train_acc: 0.7496 - val_acc: 0.7272\n",
      "Epoch 10/50 - train_acc: 0.7593 - val_acc: 0.6712\n",
      "Epoch 11/50 - train_acc: 0.7667 - val_acc: 0.7422 (best)\n",
      "Epoch 12/50 - train_acc: 0.7734 - val_acc: 0.7474 (best)\n",
      "Epoch 13/50 - train_acc: 0.7780 - val_acc: 0.7272\n",
      "Epoch 14/50 - train_acc: 0.7803 - val_acc: 0.7636 (best)\n",
      "Epoch 15/50 - train_acc: 0.7858 - val_acc: 0.6960\n",
      "Epoch 16/50 - train_acc: 0.7900 - val_acc: 0.7808 (best)\n",
      "Epoch 17/50 - train_acc: 0.7921 - val_acc: 0.7710\n",
      "Epoch 18/50 - train_acc: 0.7948 - val_acc: 0.7718\n",
      "Epoch 19/50 - train_acc: 0.8010 - val_acc: 0.7604\n",
      "Epoch 20/50 - train_acc: 0.8031 - val_acc: 0.7340\n",
      "Epoch 21/50 - train_acc: 0.8117 - val_acc: 0.7792\n",
      "Epoch 22/50 - train_acc: 0.8096 - val_acc: 0.7616\n",
      "Epoch 23/50 - train_acc: 0.8148 - val_acc: 0.7812 (best)\n",
      "Epoch 24/50 - train_acc: 0.8195 - val_acc: 0.7560\n",
      "Epoch 25/50 - train_acc: 0.8249 - val_acc: 0.8126 (best)\n",
      "Epoch 26/50 - train_acc: 0.8250 - val_acc: 0.7894\n",
      "Epoch 27/50 - train_acc: 0.8288 - val_acc: 0.8240 (best)\n",
      "Epoch 28/50 - train_acc: 0.8337 - val_acc: 0.7982\n",
      "Epoch 29/50 - train_acc: 0.8365 - val_acc: 0.8054\n",
      "Epoch 30/50 - train_acc: 0.8443 - val_acc: 0.8014\n",
      "Epoch 31/50 - train_acc: 0.8473 - val_acc: 0.8338 (best)\n",
      "Epoch 32/50 - train_acc: 0.8481 - val_acc: 0.7950\n",
      "Epoch 33/50 - train_acc: 0.8532 - val_acc: 0.8076\n",
      "Epoch 34/50 - train_acc: 0.8558 - val_acc: 0.8254\n",
      "Epoch 35/50 - train_acc: 0.8645 - val_acc: 0.8382 (best)\n",
      "Epoch 36/50 - train_acc: 0.8656 - val_acc: 0.8292\n",
      "Epoch 37/50 - train_acc: 0.8703 - val_acc: 0.8010\n",
      "Epoch 38/50 - train_acc: 0.8744 - val_acc: 0.8574 (best)\n",
      "Epoch 39/50 - train_acc: 0.8782 - val_acc: 0.8346\n",
      "Epoch 40/50 - train_acc: 0.8856 - val_acc: 0.8588 (best)\n",
      "Epoch 41/50 - train_acc: 0.8892 - val_acc: 0.8568\n",
      "Epoch 42/50 - train_acc: 0.8941 - val_acc: 0.8734 (best)\n",
      "Epoch 43/50 - train_acc: 0.8972 - val_acc: 0.8762 (best)\n",
      "Epoch 44/50 - train_acc: 0.9008 - val_acc: 0.8758\n",
      "Epoch 45/50 - train_acc: 0.9047 - val_acc: 0.8782 (best)\n",
      "Epoch 46/50 - train_acc: 0.9049 - val_acc: 0.8824 (best)\n",
      "Epoch 47/50 - train_acc: 0.9081 - val_acc: 0.8826 (best)\n",
      "Epoch 48/50 - train_acc: 0.9115 - val_acc: 0.8810\n",
      "Epoch 49/50 - train_acc: 0.9124 - val_acc: 0.8830 (best)\n",
      "Epoch 50/50 - train_acc: 0.9112 - val_acc: 0.8842 (best)\n",
      "Saved student checkpoint (v4) to ./checkpoints/kd_student_v4.pth\n",
      "{'method': 'Attention Transfer', 'train_time_sec': 668.08, 'train_acc': 0.918, 'val_acc': 0.8842, 'test_acc': 0.8734}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Attention Transfer\n",
    "LR = 0.05\n",
    "W_AT = 250.0\n",
    "\n",
    "# Attention Transfer loss function\n",
    "def attention_transfer_loss(f_s, f_t, w=1.0, eps=1e-6):\n",
    "    # Attention Transfer (Zagoruyko & Komodakis): match normalized spatial attention maps\n",
    "    def att_map(f):\n",
    "        # f: N, C, H, W -> N, H, W\n",
    "        am = f.pow(2).mean(dim=1)\n",
    "        am = am / (am.flatten(1).norm(p=2, dim=1, keepdim=True).clamp_min(eps).view(-1,1,1))\n",
    "        return am\n",
    "    a_s, a_t = att_map(f_s), att_map(f_t)\n",
    "    return w * F.mse_loss(a_s, a_t.detach())\n",
    "\n",
    "class StudentWithFeatAT(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        logits = self.classifier(f)\n",
    "        return logits, f\n",
    "\n",
    "student = make_fresh_student(StudentWithFeatAT)\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            if f_t is None:\n",
    "                raise RuntimeError(\"Teacher feature hook not captured.\")\n",
    "            if f_s.shape[-2:] != f_t.shape[-2:]:\n",
    "                f_s_resized = F.adaptive_avg_pool2d(f_s, f_t.shape[-2:])\n",
    "            else:\n",
    "                f_s_resized = f_s\n",
    "            loss = F.cross_entropy(logits_s, y) + attention_transfer_loss(f_s_resized, f_t, w=W_AT)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 4\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v4.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v4) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Attention Transfer\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb7044",
   "metadata": {
    "id": "60690a71",
    "papermill": {
     "duration": 0.02573,
     "end_time": "2025-09-16T10:43:26.694275",
     "exception": false,
     "start_time": "2025-09-16T10:43:26.668545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 5: Logit Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0788f",
   "metadata": {
    "papermill": {
     "duration": 0.025764,
     "end_time": "2025-09-16T10:43:26.745921",
     "exception": false,
     "start_time": "2025-09-16T10:43:26.720157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Ép vector logit của student gần với logit của teacher bằng tổn thất $L_2$, đồng thời giữ CE theo nhãn thật.\n",
    "\n",
    "Với một mẫu $(x, y)$:\n",
    "- $z_t, z_s \\in \\mathbb{R}^C$ là logits (trước softmax) của teacher và student.\n",
    "- Tổn thất logit matching:\n",
    "  $$\n",
    "  \\mathcal{L}_{\\mathrm{logit}} = \\big\\| z_s - z_t \\big\\|_2^2.\n",
    "  $$\n",
    "- Tổng mất mát:\n",
    "  $$\n",
    "  \\mathcal{L} = \\mathrm{CE}(z_s, y) + \\lambda\\, \\mathcal{L}_{\\mathrm{logit}}.\n",
    "  $$\n",
    "\n",
    "Khác Vanilla KD (dùng KL trên phân phối mềm), cách này làm việc trực tiếp ở không gian logit, thường đơn giản và ổn định nhưng có thể kém nhạy với cấu trúc phân phối so với KD dùng temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cef950b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T10:43:26.799580Z",
     "iopub.status.busy": "2025-09-16T10:43:26.798911Z",
     "iopub.status.idle": "2025-09-16T10:54:38.795204Z",
     "shell.execute_reply": "2025-09-16T10:54:38.794268Z"
    },
    "id": "698fb456",
    "outputId": "84ef92ac-0829-40e3-8200-4d573e330ee7",
    "papermill": {
     "duration": 672.052123,
     "end_time": "2025-09-16T10:54:38.823835",
     "exception": false,
     "start_time": "2025-09-16T10:43:26.771712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - train_acc: 0.4384 - val_acc: 0.4422 (best)\n",
      "Epoch 2/50 - train_acc: 0.6100 - val_acc: 0.5688 (best)\n",
      "Epoch 3/50 - train_acc: 0.6777 - val_acc: 0.6376 (best)\n",
      "Epoch 4/50 - train_acc: 0.7217 - val_acc: 0.7076 (best)\n",
      "Epoch 5/50 - train_acc: 0.7410 - val_acc: 0.6584\n",
      "Epoch 6/50 - train_acc: 0.7532 - val_acc: 0.7182 (best)\n",
      "Epoch 7/50 - train_acc: 0.7629 - val_acc: 0.6964\n",
      "Epoch 8/50 - train_acc: 0.7675 - val_acc: 0.7034\n",
      "Epoch 9/50 - train_acc: 0.7733 - val_acc: 0.7144\n",
      "Epoch 10/50 - train_acc: 0.7824 - val_acc: 0.7698 (best)\n",
      "Epoch 11/50 - train_acc: 0.7849 - val_acc: 0.6380\n",
      "Epoch 12/50 - train_acc: 0.7879 - val_acc: 0.7088\n",
      "Epoch 13/50 - train_acc: 0.7911 - val_acc: 0.7228\n",
      "Epoch 14/50 - train_acc: 0.7952 - val_acc: 0.7196\n",
      "Epoch 15/50 - train_acc: 0.7994 - val_acc: 0.7804 (best)\n",
      "Epoch 16/50 - train_acc: 0.8017 - val_acc: 0.7626\n",
      "Epoch 17/50 - train_acc: 0.8053 - val_acc: 0.7830 (best)\n",
      "Epoch 18/50 - train_acc: 0.8064 - val_acc: 0.7706\n",
      "Epoch 19/50 - train_acc: 0.8129 - val_acc: 0.7908 (best)\n",
      "Epoch 20/50 - train_acc: 0.8137 - val_acc: 0.7030\n",
      "Epoch 21/50 - train_acc: 0.8188 - val_acc: 0.7486\n",
      "Epoch 22/50 - train_acc: 0.8190 - val_acc: 0.7388\n",
      "Epoch 23/50 - train_acc: 0.8256 - val_acc: 0.7930 (best)\n",
      "Epoch 24/50 - train_acc: 0.8276 - val_acc: 0.7972 (best)\n",
      "Epoch 25/50 - train_acc: 0.8320 - val_acc: 0.7664\n",
      "Epoch 26/50 - train_acc: 0.8309 - val_acc: 0.8158 (best)\n",
      "Epoch 27/50 - train_acc: 0.8398 - val_acc: 0.7926\n",
      "Epoch 28/50 - train_acc: 0.8394 - val_acc: 0.8238 (best)\n",
      "Epoch 29/50 - train_acc: 0.8455 - val_acc: 0.8162\n",
      "Epoch 30/50 - train_acc: 0.8488 - val_acc: 0.7882\n",
      "Epoch 31/50 - train_acc: 0.8524 - val_acc: 0.8060\n",
      "Epoch 32/50 - train_acc: 0.8560 - val_acc: 0.8318 (best)\n",
      "Epoch 33/50 - train_acc: 0.8606 - val_acc: 0.8290\n",
      "Epoch 34/50 - train_acc: 0.8648 - val_acc: 0.8360 (best)\n",
      "Epoch 35/50 - train_acc: 0.8657 - val_acc: 0.8338\n",
      "Epoch 36/50 - train_acc: 0.8704 - val_acc: 0.8210\n",
      "Epoch 37/50 - train_acc: 0.8755 - val_acc: 0.8426 (best)\n",
      "Epoch 38/50 - train_acc: 0.8809 - val_acc: 0.8574 (best)\n",
      "Epoch 39/50 - train_acc: 0.8859 - val_acc: 0.8554\n",
      "Epoch 40/50 - train_acc: 0.8913 - val_acc: 0.8624 (best)\n",
      "Epoch 41/50 - train_acc: 0.8941 - val_acc: 0.8814 (best)\n",
      "Epoch 42/50 - train_acc: 0.8978 - val_acc: 0.8746\n",
      "Epoch 43/50 - train_acc: 0.9016 - val_acc: 0.8834 (best)\n",
      "Epoch 44/50 - train_acc: 0.9053 - val_acc: 0.8846 (best)\n",
      "Epoch 45/50 - train_acc: 0.9102 - val_acc: 0.8878 (best)\n",
      "Epoch 46/50 - train_acc: 0.9134 - val_acc: 0.8936 (best)\n",
      "Epoch 47/50 - train_acc: 0.9164 - val_acc: 0.8924\n",
      "Epoch 48/50 - train_acc: 0.9177 - val_acc: 0.8982 (best)\n",
      "Epoch 49/50 - train_acc: 0.9210 - val_acc: 0.8974\n",
      "Epoch 50/50 - train_acc: 0.9214 - val_acc: 0.8996 (best)\n",
      "Saved student checkpoint (v5) to ./checkpoints/kd_student_v5.pth\n",
      "{'method': 'Logit Matching (L2) + CE', 'train_time_sec': 662.2, 'train_acc': 0.9263, 'val_acc': 0.8996, 'test_acc': 0.8885}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Logit Matching (L2) + CE\n",
    "LR = 0.1\n",
    "W_LOGIT = 1.0\n",
    "\n",
    "student = make_fresh_student(SmallNet)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = F.cross_entropy(logits_s, y) + W_LOGIT * F.mse_loss(logits_s, logits_t)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 5\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v5.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v5) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Logit Matching (L2) + CE\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ef16e",
   "metadata": {
    "id": "ebfcc591",
    "papermill": {
     "duration": 0.027469,
     "end_time": "2025-09-16T10:54:38.879176",
     "exception": false,
     "start_time": "2025-09-16T10:54:38.851707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 6: Focal Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeba213",
   "metadata": {
    "papermill": {
     "duration": 0.027408,
     "end_time": "2025-09-16T10:54:38.935175",
     "exception": false,
     "start_time": "2025-09-16T10:54:38.907767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Nhấn mạnh các mẫu/nhãn mà student còn khó (sai hoặc xác suất thấp ở lớp đúng) khi khớp với phân phối của teacher. Thay vì đối xử đồng đều như KL thông thường, Focal KD tái trọng số từng mẫu/lớp theo độ khó, giúp giảm nhiễu ở các trường hợp teacher quá tự tin và tăng cường học ở các trường hợp student còn yếu.\n",
    "\n",
    "Ký hiệu cho một mẫu $(x, y)$:\n",
    "- $z_t, z_s \\in \\mathbb{R}^C$: logits của teacher và student.\n",
    "- Nhiệt độ $\\tau>0$ làm mềm phân phối:\n",
    "  $$\n",
    "  p_t^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_t}{\\tau}\\right),\\quad\n",
    "  p_s^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_s}{\\tau}\\right).\n",
    "  $$\n",
    "\n",
    "Trọng số tiêu điểm (focal) cho từng lớp $c$ phụ thuộc vào độ khó theo student:\n",
    "- Độ khó ở lớp $c$: $d_c = 1 - p_s^{(\\tau)}(c)$.\n",
    "- Trọng số:\n",
    "  $$\n",
    "  w_c = d_c^{\\gamma} = \\big(1 - p_s^{(\\tau)}(c)\\big)^{\\gamma},\\qquad \\gamma \\ge 0.\n",
    "  $$\n",
    "- Khi $\\gamma$ lớn, mẫu/lớp khó (student dự đoán thấp) sẽ được nhấn mạnh mạnh hơn.\n",
    "\n",
    "Hàm mất mát Focal KD dùng KL có trọng số:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{FKD}} = \\tau^2\\, \\sum_{c=1}^C w_c\\, p_t^{(\\tau)}(c)\\, \\log\\frac{p_t^{(\\tau)}(c)}{p_s^{(\\tau)}(c)}\n",
    "\\;=\\; \\tau^2\\, \\mathrm{KL}\\big( p_t^{(\\tau)} \\,\\Vert\\, p_s^{(\\tau)};\\, w\\big),\n",
    "$$\n",
    "trong đó ký hiệu $\\mathrm{KL}(\\cdot\\,\\Vert\\,\\cdot; w)$ là KL được tính với trọng số theo thành phần.\n",
    "\n",
    "Tổng mất mát huấn luyện:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s, y) + \\lambda_{\\mathrm{FKD}}\\, \\mathcal{L}_{\\mathrm{FKD}}.\n",
    "$$\n",
    "- $\\tau$ điều chỉnh độ mềm; nhân $\\tau^2$ để cân bằng thang độ như KD cổ điển.\n",
    "- $\\gamma$ điều chỉnh mức độ “tiêu điểm”; $\\lambda_{\\mathrm{FKD}}$ cân bằng giữa CE và FKD.\n",
    "- Ưu điểm: tập trung vào những phần student còn yếu, giảm tác động của các thành phần đã dễ; thường cải thiện độ chính xác khi Vanilla KD/DKD kém hiệu quả do mất cân bằng độ khó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "675c388d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T10:54:38.991908Z",
     "iopub.status.busy": "2025-09-16T10:54:38.991321Z",
     "iopub.status.idle": "2025-09-16T11:05:54.975176Z",
     "shell.execute_reply": "2025-09-16T11:05:54.974186Z"
    },
    "id": "e880452c",
    "outputId": "ee25de2a-38be-48f7-880b-b139844f4a1f",
    "papermill": {
     "duration": 676.043098,
     "end_time": "2025-09-16T11:05:55.005930",
     "exception": false,
     "start_time": "2025-09-16T10:54:38.962832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - train_acc: 0.3785 - val_acc: 0.4764 (best)\n",
      "Epoch 2/50 - train_acc: 0.5767 - val_acc: 0.4840 (best)\n",
      "Epoch 3/50 - train_acc: 0.6418 - val_acc: 0.6340 (best)\n",
      "Epoch 4/50 - train_acc: 0.6861 - val_acc: 0.6656 (best)\n",
      "Epoch 5/50 - train_acc: 0.7182 - val_acc: 0.6426\n",
      "Epoch 6/50 - train_acc: 0.7308 - val_acc: 0.7412 (best)\n",
      "Epoch 7/50 - train_acc: 0.7454 - val_acc: 0.7106\n",
      "Epoch 8/50 - train_acc: 0.7507 - val_acc: 0.6790\n",
      "Epoch 9/50 - train_acc: 0.7614 - val_acc: 0.5728\n",
      "Epoch 10/50 - train_acc: 0.7643 - val_acc: 0.6934\n",
      "Epoch 11/50 - train_acc: 0.7692 - val_acc: 0.7214\n",
      "Epoch 12/50 - train_acc: 0.7734 - val_acc: 0.6992\n",
      "Epoch 13/50 - train_acc: 0.7750 - val_acc: 0.7142\n",
      "Epoch 14/50 - train_acc: 0.7812 - val_acc: 0.6812\n",
      "Epoch 15/50 - train_acc: 0.7840 - val_acc: 0.7726 (best)\n",
      "Epoch 16/50 - train_acc: 0.7871 - val_acc: 0.7412\n",
      "Epoch 17/50 - train_acc: 0.7899 - val_acc: 0.7648\n",
      "Epoch 18/50 - train_acc: 0.7942 - val_acc: 0.7286\n",
      "Epoch 19/50 - train_acc: 0.7968 - val_acc: 0.7554\n",
      "Epoch 20/50 - train_acc: 0.8027 - val_acc: 0.7552\n",
      "Epoch 21/50 - train_acc: 0.8054 - val_acc: 0.7182\n",
      "Epoch 22/50 - train_acc: 0.8079 - val_acc: 0.7388\n",
      "Epoch 23/50 - train_acc: 0.8123 - val_acc: 0.7540\n",
      "Epoch 24/50 - train_acc: 0.8170 - val_acc: 0.7288\n",
      "Epoch 25/50 - train_acc: 0.8213 - val_acc: 0.7568\n",
      "Epoch 26/50 - train_acc: 0.8234 - val_acc: 0.7898 (best)\n",
      "Epoch 27/50 - train_acc: 0.8296 - val_acc: 0.7554\n",
      "Epoch 28/50 - train_acc: 0.8314 - val_acc: 0.7520\n",
      "Epoch 29/50 - train_acc: 0.8380 - val_acc: 0.8154 (best)\n",
      "Epoch 30/50 - train_acc: 0.8413 - val_acc: 0.8392 (best)\n",
      "Epoch 31/50 - train_acc: 0.8442 - val_acc: 0.8158\n",
      "Epoch 32/50 - train_acc: 0.8492 - val_acc: 0.8322\n",
      "Epoch 33/50 - train_acc: 0.8539 - val_acc: 0.8384\n",
      "Epoch 34/50 - train_acc: 0.8584 - val_acc: 0.8260\n",
      "Epoch 35/50 - train_acc: 0.8639 - val_acc: 0.8090\n",
      "Epoch 36/50 - train_acc: 0.8664 - val_acc: 0.8222\n",
      "Epoch 37/50 - train_acc: 0.8747 - val_acc: 0.8500 (best)\n",
      "Epoch 38/50 - train_acc: 0.8759 - val_acc: 0.8596 (best)\n",
      "Epoch 39/50 - train_acc: 0.8828 - val_acc: 0.8602 (best)\n",
      "Epoch 40/50 - train_acc: 0.8891 - val_acc: 0.8610 (best)\n",
      "Epoch 41/50 - train_acc: 0.8926 - val_acc: 0.8652 (best)\n",
      "Epoch 42/50 - train_acc: 0.8970 - val_acc: 0.8644\n",
      "Epoch 43/50 - train_acc: 0.9032 - val_acc: 0.8766 (best)\n",
      "Epoch 44/50 - train_acc: 0.9065 - val_acc: 0.8812 (best)\n",
      "Epoch 45/50 - train_acc: 0.9124 - val_acc: 0.8844 (best)\n",
      "Epoch 46/50 - train_acc: 0.9143 - val_acc: 0.8890 (best)\n",
      "Epoch 47/50 - train_acc: 0.9199 - val_acc: 0.8922 (best)\n",
      "Epoch 48/50 - train_acc: 0.9192 - val_acc: 0.8956 (best)\n",
      "Epoch 49/50 - train_acc: 0.9233 - val_acc: 0.8918\n",
      "Epoch 50/50 - train_acc: 0.9239 - val_acc: 0.8940\n",
      "Saved student checkpoint (v6 FKD) to ./checkpoints/kd_student_v6.pth\n",
      "{'method': 'Focal Knowledge Distillation (FKD)', 'train_time_sec': 666.25, 'train_acc': 0.9295, 'val_acc': 0.8956, 'test_acc': 0.8822}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Focal Knowledge Distillation (FKD)\n",
    "T = 4.0\n",
    "GAMMA_FKD = 2.0      # focal gamma\n",
    "LAMBDA_FKD = 1.0     # weight for FKD loss\n",
    "LR = 0.1\n",
    "\n",
    "\n",
    "def fkd_loss(logits_s, logits_t, T=4.0, gamma=2.0, eps=1e-12):\n",
    "    # Probabilities with temperature\n",
    "    ps = F.softmax(logits_s / T, dim=1)   # N,C\n",
    "    pt = F.softmax(logits_t / T, dim=1)   # N,C\n",
    "\n",
    "    # Focal weights: higher weight where student is uncertain (harder)\n",
    "    w = (1.0 - ps).clamp(min=0.0, max=1.0) ** gamma   # N,C\n",
    "\n",
    "    # Weighted KL(pt || ps)\n",
    "    # Compute element-wise: pt * (log pt - log ps) * w, average over batch\n",
    "    log_ps = (ps + eps).log()\n",
    "    log_pt = (pt + eps).log()\n",
    "    kl_elem = w * pt * (log_pt - log_ps)\n",
    "    loss = kl_elem.sum(dim=1).mean()  # batch mean\n",
    "    return (T * T) * loss\n",
    "\n",
    "\n",
    "student = make_fresh_student(SmallNet)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss_ce  = F.cross_entropy(logits_s, y)\n",
    "            loss_kd  = fkd_loss(logits_s, logits_t, T=T, gamma=GAMMA_FKD)\n",
    "            loss     = loss_ce + LAMBDA_FKD * loss_kd\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 6 (FKD) - giữ nguyên tên file để phần đánh giá không cần sửa\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v6.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v6 FKD) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Focal Knowledge Distillation (FKD)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8b056",
   "metadata": {
    "id": "9552435b",
    "papermill": {
     "duration": 0.028942,
     "end_time": "2025-09-16T11:05:55.064361",
     "exception": false,
     "start_time": "2025-09-16T11:05:55.035419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 7: Relational Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a7ece",
   "metadata": {
    "papermill": {
     "duration": 0.029345,
     "end_time": "2025-09-16T11:05:55.123030",
     "exception": false,
     "start_time": "2025-09-16T11:05:55.093685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** RKD không khớp trực tiếp đặc trưng/logit mà khớp các “quan hệ” giữa các mẫu trong batch.\n",
    "\n",
    "Biểu diễn vector hoá (sau pool) và chuẩn hoá $L_2$:  \n",
    "$$\n",
    "z_s = \\mathrm{norm}(\\mathrm{GAP}(F_s)), \\quad z_t = \\mathrm{norm}(\\mathrm{GAP}(F_t)).\n",
    "$$\n",
    "với $\\mathrm{GAP}$ là global average pooling, $\\mathrm{norm}$ là chuẩn hoá $L_2$.\n",
    "\n",
    "1) Khoảng cách cặp (pairwise distance):\n",
    "- Ma trận khoảng cách Euclid:  \n",
    "  $$\n",
    "  D(z)_{ij} = \\| z_i - z_j \\|_2.\n",
    "  $$\n",
    "- Chuẩn hoá theo trung bình phần tử dương để bất biến tỉ lệ:  \n",
    "  $$\n",
    "  \\tilde{D}_t = \\frac{D(z_t)}{\\mathrm{mean}\\big( D(z_t)_{ij} : D(z_t)_{ij}>0 \\big)}, \\quad\n",
    "  \\tilde{D}_s = \\frac{D(z_s)}{\\mathrm{mean}\\big( D(z_s)_{ij} : D(z_s)_{ij}>0 \\big)}.\n",
    "  $$\n",
    "- Mất mát khoảng cách:  \n",
    "  $$\n",
    "  \\mathcal{L}_{\\mathrm{dist}} = \\mathrm{SmoothL1}(\\tilde{D}_s, \\tilde{D}_t).\n",
    "  $$\n",
    "\n",
    "2) Góc bộ ba (triplet angle):\n",
    "- Với mọi $i, j, k$:  \n",
    "  $$\n",
    "  v_{ij} = z_j - z_i,\\; v_{ik} = z_k - z_i,\\; \\cos\\angle(jik) = \\frac{v_{ij}^\\top v_{ik}}{\\|v_{ij}\\|\\,\\|v_{ik}\\|}.\n",
    "  $$\n",
    "- Tập hợp vào tensor $A(z)$ với $A_{i,j,k} = \\cos(\\angle jik)$.  \n",
    "  $$\n",
    "  \\mathcal{L}_{\\mathrm{angle}} = \\mathrm{SmoothL1}\\big( A(z_s), A(z_t) \\big).\n",
    "  $$\n",
    "\n",
    "Tổng mất mát:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s^{\\text{logit}}, y) + \\lambda_d\\, \\mathcal{L}_{\\mathrm{dist}} + \\lambda_a\\, \\mathcal{L}_{\\mathrm{angle}}.\n",
    "$$\n",
    "Trong đó $z_s^{\\text{logit}}$ là logits cho CE; $z_s, z_t$ cho phần RKD được lấy từ feature đã pool và chuẩn hoá. Cách này truyền cấu trúc hình học của không gian biểu diễn từ teacher sang student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e708e53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:05:55.184952Z",
     "iopub.status.busy": "2025-09-16T11:05:55.184163Z",
     "iopub.status.idle": "2025-09-16T11:19:26.065924Z",
     "shell.execute_reply": "2025-09-16T11:19:26.064806Z"
    },
    "id": "55cb670f",
    "outputId": "04ce6059-0c1c-4436-fe81-35e7c6353938",
    "papermill": {
     "duration": 810.913854,
     "end_time": "2025-09-16T11:19:26.067485",
     "exception": false,
     "start_time": "2025-09-16T11:05:55.153631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - train_acc: 0.4540 - val_acc: 0.4882 (best)\n",
      "Epoch 2/50 - train_acc: 0.6223 - val_acc: 0.6394 (best)\n",
      "Epoch 3/50 - train_acc: 0.6777 - val_acc: 0.5842\n",
      "Epoch 4/50 - train_acc: 0.7166 - val_acc: 0.7166 (best)\n",
      "Epoch 5/50 - train_acc: 0.7465 - val_acc: 0.5680\n",
      "Epoch 6/50 - train_acc: 0.7652 - val_acc: 0.7648 (best)\n",
      "Epoch 7/50 - train_acc: 0.7752 - val_acc: 0.7416\n",
      "Epoch 8/50 - train_acc: 0.7831 - val_acc: 0.7414\n",
      "Epoch 9/50 - train_acc: 0.7870 - val_acc: 0.7734 (best)\n",
      "Epoch 10/50 - train_acc: 0.7935 - val_acc: 0.6740\n",
      "Epoch 11/50 - train_acc: 0.7988 - val_acc: 0.7804 (best)\n",
      "Epoch 12/50 - train_acc: 0.8031 - val_acc: 0.7708\n",
      "Epoch 13/50 - train_acc: 0.8085 - val_acc: 0.7766\n",
      "Epoch 14/50 - train_acc: 0.8090 - val_acc: 0.7762\n",
      "Epoch 15/50 - train_acc: 0.8120 - val_acc: 0.7840 (best)\n",
      "Epoch 16/50 - train_acc: 0.8172 - val_acc: 0.7904 (best)\n",
      "Epoch 17/50 - train_acc: 0.8199 - val_acc: 0.8234 (best)\n",
      "Epoch 18/50 - train_acc: 0.8248 - val_acc: 0.7786\n",
      "Epoch 19/50 - train_acc: 0.8244 - val_acc: 0.7994\n",
      "Epoch 20/50 - train_acc: 0.8278 - val_acc: 0.7958\n",
      "Epoch 21/50 - train_acc: 0.8304 - val_acc: 0.8078\n",
      "Epoch 22/50 - train_acc: 0.8317 - val_acc: 0.7926\n",
      "Epoch 23/50 - train_acc: 0.8326 - val_acc: 0.7978\n",
      "Epoch 24/50 - train_acc: 0.8386 - val_acc: 0.7926\n",
      "Epoch 25/50 - train_acc: 0.8404 - val_acc: 0.8000\n",
      "Epoch 26/50 - train_acc: 0.8427 - val_acc: 0.8386 (best)\n",
      "Epoch 27/50 - train_acc: 0.8469 - val_acc: 0.8090\n",
      "Epoch 28/50 - train_acc: 0.8477 - val_acc: 0.8390 (best)\n",
      "Epoch 29/50 - train_acc: 0.8521 - val_acc: 0.8284\n",
      "Epoch 30/50 - train_acc: 0.8551 - val_acc: 0.8428 (best)\n",
      "Epoch 31/50 - train_acc: 0.8571 - val_acc: 0.8470 (best)\n",
      "Epoch 32/50 - train_acc: 0.8598 - val_acc: 0.8482 (best)\n",
      "Epoch 33/50 - train_acc: 0.8636 - val_acc: 0.8304\n",
      "Epoch 34/50 - train_acc: 0.8669 - val_acc: 0.8410\n",
      "Epoch 35/50 - train_acc: 0.8698 - val_acc: 0.8458\n",
      "Epoch 36/50 - train_acc: 0.8753 - val_acc: 0.8426\n",
      "Epoch 37/50 - train_acc: 0.8776 - val_acc: 0.8618 (best)\n",
      "Epoch 38/50 - train_acc: 0.8790 - val_acc: 0.8696 (best)\n",
      "Epoch 39/50 - train_acc: 0.8840 - val_acc: 0.8678\n",
      "Epoch 40/50 - train_acc: 0.8891 - val_acc: 0.8666\n",
      "Epoch 41/50 - train_acc: 0.8924 - val_acc: 0.8636\n",
      "Epoch 42/50 - train_acc: 0.8942 - val_acc: 0.8770 (best)\n",
      "Epoch 43/50 - train_acc: 0.8985 - val_acc: 0.8724\n",
      "Epoch 44/50 - train_acc: 0.9012 - val_acc: 0.8852 (best)\n",
      "Epoch 45/50 - train_acc: 0.9034 - val_acc: 0.8828\n",
      "Epoch 46/50 - train_acc: 0.9064 - val_acc: 0.8830\n",
      "Epoch 47/50 - train_acc: 0.9101 - val_acc: 0.8886 (best)\n",
      "Epoch 48/50 - train_acc: 0.9126 - val_acc: 0.8872\n",
      "Epoch 49/50 - train_acc: 0.9118 - val_acc: 0.8890 (best)\n",
      "Epoch 50/50 - train_acc: 0.9124 - val_acc: 0.8898 (best)\n",
      "Saved student checkpoint (v7 RKD) to ./checkpoints/kd_student_v7.pth\n",
      "{'method': 'Relational Knowledge Distillation (RKD)', 'train_time_sec': 801.05, 'train_acc': 0.9225, 'val_acc': 0.8898, 'test_acc': 0.8816}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Relational Knowledge Distillation (RKD)\n",
    "# Reference: Park et al., CVPR 2019. We implement RKD with both distance and angle losses.\n",
    "# Idea: match relational structures between samples (pairwise distances and triplet angles) in feature space.\n",
    "\n",
    "LR = 0.05\n",
    "W_RKD_DIST = 25.0\n",
    "W_RKD_ANGLE = 50.0\n",
    "\n",
    "# Student wrapper to expose features before global pooling\n",
    "class StudentWithFeatRKD(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)         # N, 128, H, W\n",
    "        logits = self.classifier(f)  # N, C\n",
    "        return logits, f\n",
    "\n",
    "student = make_fresh_student(StudentWithFeatRKD)\n",
    "\n",
    "# Teacher feature hook (layer4 output)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Helper: global-average pool to vectors and L2-normalize\n",
    "def to_vec_norm(fm: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # fm: N, C, H, W -> N, C\n",
    "    v = F.adaptive_avg_pool2d(fm, 1).flatten(1)\n",
    "    v = F.normalize(v, dim=1, eps=eps)\n",
    "    return v\n",
    "\n",
    "# RKD: Distance loss (pairwise)\n",
    "def rkd_distance(z_s: torch.Tensor, z_t: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # pairwise Euclidean distances\n",
    "    with torch.no_grad():\n",
    "        # teacher pairwise distances normalized by mean\n",
    "        d_t = torch.cdist(z_t, z_t, p=2)\n",
    "        mean_t = d_t[d_t>0].mean().clamp_min(eps)\n",
    "        d_t = d_t / mean_t\n",
    "    d_s = torch.cdist(z_s, z_s, p=2)\n",
    "    mean_s = d_s[d_s>0].mean().clamp_min(eps)\n",
    "    d_s = d_s / mean_s\n",
    "    return F.smooth_l1_loss(d_s, d_t)\n",
    "\n",
    "# RKD: Angle loss (triplet angles)\n",
    "def rkd_angle(z_s: torch.Tensor, z_t: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # For each anchor i, compute vectors to j and k: v_ij, v_ik, then their angle via cosine\n",
    "    def angle_matrix(z: torch.Tensor) -> torch.Tensor:\n",
    "        # z: N, D\n",
    "        n = z.size(0)\n",
    "        # compute pairwise differences: v_ij = z_j - z_i -> shape (N,N,D)\n",
    "        diff = z.unsqueeze(1) - z.unsqueeze(0)\n",
    "        # normalize along D\n",
    "        diff = F.normalize(diff, dim=2, eps=eps)\n",
    "        # cosine between v_ij and v_ik for all (j,k): cos = v_ij · v_ik\n",
    "        # angle tensor A where A[i,j,k] = cos(angle_jik)\n",
    "        A = torch.einsum('ijd,ikd->ijk', diff, diff)\n",
    "        return A\n",
    "    with torch.no_grad():\n",
    "        A_t = angle_matrix(z_t)\n",
    "    A_s = angle_matrix(z_s)\n",
    "    return F.smooth_l1_loss(A_s, A_t)\n",
    "\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            # CE term for classification\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            # RKD on pooled features\n",
    "            z_s = to_vec_norm(f_s)\n",
    "            z_t = to_vec_norm(f_t)\n",
    "            loss_dist = rkd_distance(z_s, z_t)\n",
    "            loss_ang = rkd_angle(z_s, z_t)\n",
    "            loss = loss_ce + W_RKD_DIST * loss_dist + W_RKD_ANGLE * loss_ang\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 7 (RKD)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v7.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v7 RKD) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Relational Knowledge Distillation (RKD)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a7745",
   "metadata": {
    "id": "Dx7AI67eYJwI",
    "papermill": {
     "duration": 0.031411,
     "end_time": "2025-09-16T11:19:26.131851",
     "exception": false,
     "start_time": "2025-09-16T11:19:26.100440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 8: Contrastive Representation Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d647ef",
   "metadata": {
    "papermill": {
     "duration": 0.031598,
     "end_time": "2025-09-16T11:19:26.195515",
     "exception": false,
     "start_time": "2025-09-16T11:19:26.163917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** CRD khớp biểu diễn thông qua mục tiêu tương phản (contrastive) kiểu InfoNCE giữa nhúng (embedding) của student và teacher.\n",
    "\n",
    "Giả sử có $N$ mẫu trong batch. Sau khi chiếu về cùng không gian bởi hai đầu chiếu học được $h_s(\\cdot), h_t(\\cdot)$:\n",
    "- $e_s = h_s(F_s) \\in \\mathbb{R}^{N\\times d}$, $e_t = h_t(F_t) \\in \\mathbb{R}^{N\\times d}$.\n",
    "- Chuẩn hoá $L_2$: $z_s = e_s/\\|e_s\\|$, $z_t = e_t/\\|e_t\\|$ (theo từng mẫu).\n",
    "\n",
    "Với mỗi $i$, ta coi $z_s[i]$ là query và $z_t[i]$ là positive; các $z_t[j]$ ($j\\neq i$) là negatives. Logits tương phản:\n",
    "$$\n",
    "\\ell_{i,j} = \\frac{ z_s[i]^\\top z_t[j] }{\\tau}, \\quad j=1,\\dots,N.\n",
    "$$\n",
    "Tổn thất InfoNCE trong-batch:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{CRD}} = \\frac{1}{N} \\sum_{i=1}^N \\mathrm{CE}\\big( \\ell_{i,:}, \\, j^*=i \\big).\n",
    "$$\n",
    "\n",
    "Tổng mất mát:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s^{\\text{logit}}, y) + \\lambda_{\\mathrm{CRD}}\\, \\mathcal{L}_{\\mathrm{CRD}}.\n",
    "$$\n",
    "- $\\tau$ là nhiệt độ điều chỉnh độ sắc của phân bố tương phản.\n",
    "- Mục tiêu: kéo cặp (student, teacher) của cùng mẫu lại gần, đẩy xa cặp khác mẫu, giúp student học không gian biểu diễn tương đồng teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2343b6c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:19:26.259943Z",
     "iopub.status.busy": "2025-09-16T11:19:26.259184Z",
     "iopub.status.idle": "2025-09-16T11:31:10.632830Z",
     "shell.execute_reply": "2025-09-16T11:31:10.631828Z"
    },
    "id": "zzaGwMl_UrqN",
    "outputId": "c046f154-df8a-4f47-fd2f-f2c633309eaa",
    "papermill": {
     "duration": 704.442402,
     "end_time": "2025-09-16T11:31:10.669338",
     "exception": false,
     "start_time": "2025-09-16T11:19:26.226936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - train_acc: 0.4335 - val_acc: 0.4696 (best)\n",
      "Epoch 2/50 - train_acc: 0.6041 - val_acc: 0.5204 (best)\n",
      "Epoch 3/50 - train_acc: 0.6725 - val_acc: 0.6746 (best)\n",
      "Epoch 4/50 - train_acc: 0.7165 - val_acc: 0.6906 (best)\n",
      "Epoch 5/50 - train_acc: 0.7394 - val_acc: 0.7102 (best)\n",
      "Epoch 6/50 - train_acc: 0.7550 - val_acc: 0.7272 (best)\n",
      "Epoch 7/50 - train_acc: 0.7657 - val_acc: 0.7092\n",
      "Epoch 8/50 - train_acc: 0.7744 - val_acc: 0.7614 (best)\n",
      "Epoch 9/50 - train_acc: 0.7823 - val_acc: 0.7364\n",
      "Epoch 10/50 - train_acc: 0.7856 - val_acc: 0.7334\n",
      "Epoch 11/50 - train_acc: 0.7937 - val_acc: 0.7640 (best)\n",
      "Epoch 12/50 - train_acc: 0.7948 - val_acc: 0.7584\n",
      "Epoch 13/50 - train_acc: 0.8011 - val_acc: 0.7192\n",
      "Epoch 14/50 - train_acc: 0.8022 - val_acc: 0.7960 (best)\n",
      "Epoch 15/50 - train_acc: 0.8094 - val_acc: 0.7216\n",
      "Epoch 16/50 - train_acc: 0.8080 - val_acc: 0.7408\n",
      "Epoch 17/50 - train_acc: 0.8124 - val_acc: 0.8044 (best)\n",
      "Epoch 18/50 - train_acc: 0.8145 - val_acc: 0.7346\n",
      "Epoch 19/50 - train_acc: 0.8192 - val_acc: 0.6902\n",
      "Epoch 20/50 - train_acc: 0.8210 - val_acc: 0.7602\n",
      "Epoch 21/50 - train_acc: 0.8253 - val_acc: 0.7768\n",
      "Epoch 22/50 - train_acc: 0.8256 - val_acc: 0.7958\n",
      "Epoch 23/50 - train_acc: 0.8276 - val_acc: 0.8080 (best)\n",
      "Epoch 24/50 - train_acc: 0.8316 - val_acc: 0.7768\n",
      "Epoch 25/50 - train_acc: 0.8352 - val_acc: 0.7802\n",
      "Epoch 26/50 - train_acc: 0.8407 - val_acc: 0.8040\n",
      "Epoch 27/50 - train_acc: 0.8398 - val_acc: 0.8158 (best)\n",
      "Epoch 28/50 - train_acc: 0.8475 - val_acc: 0.8182 (best)\n",
      "Epoch 29/50 - train_acc: 0.8492 - val_acc: 0.8278 (best)\n",
      "Epoch 30/50 - train_acc: 0.8530 - val_acc: 0.8176\n",
      "Epoch 31/50 - train_acc: 0.8545 - val_acc: 0.8442 (best)\n",
      "Epoch 32/50 - train_acc: 0.8602 - val_acc: 0.8342\n",
      "Epoch 33/50 - train_acc: 0.8632 - val_acc: 0.8406\n",
      "Epoch 34/50 - train_acc: 0.8669 - val_acc: 0.8462 (best)\n",
      "Epoch 35/50 - train_acc: 0.8724 - val_acc: 0.8416\n",
      "Epoch 36/50 - train_acc: 0.8739 - val_acc: 0.8376\n",
      "Epoch 37/50 - train_acc: 0.8804 - val_acc: 0.8740 (best)\n",
      "Epoch 38/50 - train_acc: 0.8806 - val_acc: 0.8536\n",
      "Epoch 39/50 - train_acc: 0.8858 - val_acc: 0.8698\n",
      "Epoch 40/50 - train_acc: 0.8899 - val_acc: 0.8794 (best)\n",
      "Epoch 41/50 - train_acc: 0.8953 - val_acc: 0.8692\n",
      "Epoch 42/50 - train_acc: 0.8972 - val_acc: 0.8826 (best)\n",
      "Epoch 43/50 - train_acc: 0.9026 - val_acc: 0.8910 (best)\n",
      "Epoch 44/50 - train_acc: 0.9066 - val_acc: 0.8882\n",
      "Epoch 45/50 - train_acc: 0.9103 - val_acc: 0.8948 (best)\n",
      "Epoch 46/50 - train_acc: 0.9124 - val_acc: 0.8958 (best)\n",
      "Epoch 47/50 - train_acc: 0.9130 - val_acc: 0.8946\n",
      "Epoch 48/50 - train_acc: 0.9170 - val_acc: 0.8978 (best)\n",
      "Epoch 49/50 - train_acc: 0.9183 - val_acc: 0.8952\n",
      "Epoch 50/50 - train_acc: 0.9191 - val_acc: 0.8946\n",
      "Saved student checkpoint (v8) to ./checkpoints/kd_student_v8.pth\n",
      "{'method': 'Contrastive Representation Distillation (CRD)', 'train_time_sec': 694.68, 'train_acc': 0.9256, 'val_acc': 0.8978, 'test_acc': 0.8866}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Contrastive Representation Distillation (CRD, in-batch)\n",
    "# Ref: Tian et al., ICLR 2020. We implement a lightweight in-batch CRD:\n",
    "#   - Take penultimate conv features from student and teacher\n",
    "#   - Project to a shared embedding space with small MLP heads\n",
    "#   - Use InfoNCE with in-batch negatives (z_s vs z_t of all samples)\n",
    "#   - Optimize CE + W_CRD * CRD\n",
    "\n",
    "LR = 0.1\n",
    "W_CRD = 1.0\n",
    "TAU = 0.07   # temperature for contrastive logits\n",
    "EMB_DIM = 128\n",
    "\n",
    "# Student wrapper to expose features\n",
    "class StudentWithFeatCRD(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)         # N, 128, H, W\n",
    "        logits = self.classifier(f)  # N, C\n",
    "        return logits, f\n",
    "\n",
    "student = make_fresh_student(StudentWithFeatCRD)\n",
    "\n",
    "# Teacher feature hook (layer4)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Projection heads (trainable). Teacher backbone is frozen, but this head is trainable.\n",
    "proj_s = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128, EMB_DIM),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(EMB_DIM, EMB_DIM),\n",
    ").to(DEVICE)\n",
    "\n",
    "proj_t = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, EMB_DIM),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(EMB_DIM, EMB_DIM),\n",
    ").to(DEVICE)\n",
    "\n",
    "# Optimizer includes student + projection heads\n",
    "optimizer = optim.SGD(list(student.parameters()) + list(proj_s.parameters()) + list(proj_t.parameters()),\n",
    "                      lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# Contrastive loss (InfoNCE) using in-batch negatives\n",
    "def crd_loss(emb_s: torch.Tensor, emb_t: torch.Tensor, tau: float = 0.07) -> torch.Tensor:\n",
    "    # Normalize\n",
    "    zs = F.normalize(emb_s, dim=1)\n",
    "    zt = F.normalize(emb_t, dim=1)\n",
    "    # Similarity logits: N x N\n",
    "    logits = (zs @ zt.t()) / tau\n",
    "    targets = torch.arange(logits.size(0), device=logits.device)\n",
    "    return F.cross_entropy(logits, targets)\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train(); proj_s.train(); proj_t.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook\n",
    "            f_t = hook_t.feat  # N, 512, Ht, Wt\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)  # f_s: N, 128, Hs, Ws\n",
    "            # Project to embeddings (vector)\n",
    "            z_s = proj_s(f_s)\n",
    "            z_t = proj_t(f_t.detach())\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            loss_con = crd_loss(z_s, z_t, tau=TAU)\n",
    "            loss = loss_ce + W_CRD * loss_con\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    # Eval on validation using classifier head only\n",
    "    student.eval(); proj_s.eval(); proj_t.eval()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = {\n",
    "            'student': copy.deepcopy(student.state_dict()),\n",
    "            'proj_s': copy.deepcopy(proj_s.state_dict()),\n",
    "            'proj_t': copy.deepcopy(proj_t.state_dict()),\n",
    "        }\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state['student'])\n",
    "    proj_s.load_state_dict(best_state['proj_s'])\n",
    "    proj_t.load_state_dict(best_state['proj_t'])\n",
    "\n",
    "# Save student checkpoint for method 8\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v8.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v8) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Contrastive Representation Distillation (CRD)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d06209",
   "metadata": {
    "papermill": {
     "duration": 0.033136,
     "end_time": "2025-09-16T11:31:10.736944",
     "exception": false,
     "start_time": "2025-09-16T11:31:10.703808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 9: Probabilistic Knowledge Transfer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1917e9",
   "metadata": {
    "papermill": {
     "duration": 0.033625,
     "end_time": "2025-09-16T11:31:10.803767",
     "exception": false,
     "start_time": "2025-09-16T11:31:10.770142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** PKT không khớp trực tiếp giá trị đặc trưng/logit mà khớp “quan hệ xác suất” giữa các mẫu trong cùng batch. Cụ thể, với biểu diễn (embedding) của mỗi mẫu, ta xây dựng phân phối xác suất tương tự (similarity distribution) từ mỗi mẫu đến các mẫu còn lại và buộc student mô phỏng phân phối của teacher. Cách này bảo tồn cấu trúc cục bộ của không gian biểu diễn và ít nhạy cảm với biến đổi biên độ/chuẩn hoá.\n",
    "\n",
    "Ký hiệu theo minibatch gồm $N$ mẫu, với embedding đã chuẩn hoá $L_2$:\n",
    "- $z_t = \\mathrm{norm}(e_t) \\in \\mathbb{R}^{N\\times d}$, $z_s = \\mathrm{norm}(e_s) \\in \\mathbb{R}^{N\\times d}$.\n",
    "- Hàm tương tự (similarity) dùng cosine: $\\mathrm{sim}(u, v) = u^\\top v$.\n",
    "- Với mỗi mẫu $i$, định nghĩa phân phối xác suất trên các mẫu khác bằng softmax nhiệt độ $\\sigma$ (khác với nhiệt độ KD trên logits):\n",
    "  $$\n",
    "  p_t(j\\,|\\,i) \\,=\\, \\frac{\\exp\\big(\\mathrm{sim}(z_t[i], z_t[j]) / \\sigma\\big)}{\\sum\\limits_{k\\ne i} \\exp\\big(\\mathrm{sim}(z_t[i], z_t[k]) / \\sigma\\big)},\\quad j\\ne i,\n",
    "  $$\n",
    "  và tương tự $p_s(j\\,|\\,i)$ từ $z_s$.\n",
    "\n",
    "Mất mát PKT là KL trung bình qua tất cả điều kiện $i$ (bỏ qua phần tử tự so sánh $j=i$):\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{PKT}} \\,=\\, \\frac{1}{N}\\sum_{i=1}^N \\mathrm{KL}\\big( p_t(\\cdot\\,|\\,i) \\,\\Vert\\, p_s(\\cdot\\,|\\,i) \\big).\n",
    "$$\n",
    "\n",
    "Tổng mất mát huấn luyện:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s^{\\text{logit}}, y) + \\lambda_{\\mathrm{PKT}}\\, \\mathcal{L}_{\\mathrm{PKT}}.\n",
    "$$\n",
    "- $\\sigma$ điều chỉnh độ sắc của phân phối tương tự; $\\lambda_{\\mathrm{PKT}}$ cân bằng với CE.\n",
    "- Khác RKD (dựa khoảng cách/góc), PKT dùng xác suất tương tự có chuẩn hoá theo từng gốc $i$, nên bền vững hơn với thay đổi tỉ lệ, và nhấn mạnh quan hệ cục bộ trong batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f984279",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:31:10.871765Z",
     "iopub.status.busy": "2025-09-16T11:31:10.871483Z",
     "iopub.status.idle": "2025-09-16T11:42:37.324484Z",
     "shell.execute_reply": "2025-09-16T11:42:37.323491Z"
    },
    "papermill": {
     "duration": 686.525649,
     "end_time": "2025-09-16T11:42:37.362752",
     "exception": false,
     "start_time": "2025-09-16T11:31:10.837103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - train_acc: 0.4134 - val_acc: 0.5552 (best)\n",
      "Epoch 2/50 - train_acc: 0.5892 - val_acc: 0.6514 (best)\n",
      "Epoch 3/50 - train_acc: 0.6606 - val_acc: 0.6096\n",
      "Epoch 4/50 - train_acc: 0.6981 - val_acc: 0.5078\n",
      "Epoch 5/50 - train_acc: 0.7222 - val_acc: 0.6506\n",
      "Epoch 6/50 - train_acc: 0.7336 - val_acc: 0.7398 (best)\n",
      "Epoch 7/50 - train_acc: 0.7481 - val_acc: 0.7098\n",
      "Epoch 8/50 - train_acc: 0.7529 - val_acc: 0.7120\n",
      "Epoch 9/50 - train_acc: 0.7552 - val_acc: 0.6394\n",
      "Epoch 10/50 - train_acc: 0.7627 - val_acc: 0.6544\n",
      "Epoch 11/50 - train_acc: 0.7690 - val_acc: 0.7050\n",
      "Epoch 12/50 - train_acc: 0.7714 - val_acc: 0.7440 (best)\n",
      "Epoch 13/50 - train_acc: 0.7740 - val_acc: 0.7450 (best)\n",
      "Epoch 14/50 - train_acc: 0.7800 - val_acc: 0.7690 (best)\n",
      "Epoch 15/50 - train_acc: 0.7778 - val_acc: 0.7254\n",
      "Epoch 16/50 - train_acc: 0.7852 - val_acc: 0.6504\n",
      "Epoch 17/50 - train_acc: 0.7907 - val_acc: 0.7764 (best)\n",
      "Epoch 18/50 - train_acc: 0.7893 - val_acc: 0.7812 (best)\n",
      "Epoch 19/50 - train_acc: 0.7927 - val_acc: 0.7406\n",
      "Epoch 20/50 - train_acc: 0.7942 - val_acc: 0.7866 (best)\n",
      "Epoch 21/50 - train_acc: 0.7996 - val_acc: 0.7798\n",
      "Epoch 22/50 - train_acc: 0.8029 - val_acc: 0.7342\n",
      "Epoch 23/50 - train_acc: 0.8069 - val_acc: 0.7998 (best)\n",
      "Epoch 24/50 - train_acc: 0.8096 - val_acc: 0.7782\n",
      "Epoch 25/50 - train_acc: 0.8139 - val_acc: 0.8012 (best)\n",
      "Epoch 26/50 - train_acc: 0.8160 - val_acc: 0.7950\n",
      "Epoch 27/50 - train_acc: 0.8181 - val_acc: 0.7518\n",
      "Epoch 28/50 - train_acc: 0.8212 - val_acc: 0.8052 (best)\n",
      "Epoch 29/50 - train_acc: 0.8251 - val_acc: 0.7452\n",
      "Epoch 30/50 - train_acc: 0.8303 - val_acc: 0.7846\n",
      "Epoch 31/50 - train_acc: 0.8360 - val_acc: 0.8418 (best)\n",
      "Epoch 32/50 - train_acc: 0.8400 - val_acc: 0.7276\n",
      "Epoch 33/50 - train_acc: 0.8444 - val_acc: 0.7948\n",
      "Epoch 34/50 - train_acc: 0.8487 - val_acc: 0.8406\n",
      "Epoch 35/50 - train_acc: 0.8529 - val_acc: 0.8212\n",
      "Epoch 36/50 - train_acc: 0.8574 - val_acc: 0.8282\n",
      "Epoch 37/50 - train_acc: 0.8619 - val_acc: 0.8380\n",
      "Epoch 38/50 - train_acc: 0.8648 - val_acc: 0.8366\n",
      "Epoch 39/50 - train_acc: 0.8727 - val_acc: 0.8560 (best)\n",
      "Epoch 40/50 - train_acc: 0.8768 - val_acc: 0.8630 (best)\n",
      "Epoch 41/50 - train_acc: 0.8814 - val_acc: 0.8598\n",
      "Epoch 42/50 - train_acc: 0.8873 - val_acc: 0.8786 (best)\n",
      "Epoch 43/50 - train_acc: 0.8908 - val_acc: 0.8712\n",
      "Epoch 44/50 - train_acc: 0.8970 - val_acc: 0.8826 (best)\n",
      "Epoch 45/50 - train_acc: 0.9015 - val_acc: 0.8842 (best)\n",
      "Epoch 46/50 - train_acc: 0.9062 - val_acc: 0.8864 (best)\n",
      "Epoch 47/50 - train_acc: 0.9099 - val_acc: 0.8864\n",
      "Epoch 48/50 - train_acc: 0.9114 - val_acc: 0.8888 (best)\n",
      "Epoch 49/50 - train_acc: 0.9146 - val_acc: 0.8912 (best)\n",
      "Epoch 50/50 - train_acc: 0.9163 - val_acc: 0.8918 (best)\n",
      "Saved student checkpoint (v9 PKT) to ./checkpoints/kd_student_v9.pth\n",
      "{'method': 'Probabilistic Knowledge Transfer (PKT)', 'train_time_sec': 676.48, 'train_acc': 0.922, 'val_acc': 0.8918, 'test_acc': 0.8832}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Probabilistic Knowledge Transfer (PKT)\n",
    "LR = 0.1\n",
    "SIGMA_PKT = 0.1      # temperature for similarity softmax\n",
    "LAMBDA_PKT = 2.0     # weight for PKT loss\n",
    "\n",
    "# We need embeddings for teacher and student. We'll reuse the penultimate conv feature then GAP + linear head to produce logits.\n",
    "# Wrap SmallNet to expose an embedding vector per sample (after GAP, before final FC) and logits.\n",
    "class SmallNetWithEmbed(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)              # N,C,H,W (e.g., 128x4x4)\n",
    "        gap = F.adaptive_avg_pool2d(f, 1).flatten(1)  # N,C\n",
    "        logits = self.classifier(f)       # original classifier may use conv+pool inside\n",
    "        return logits, gap                # logits (for CE), gap as embedding e_s\n",
    "\n",
    "# Teacher embed: build a small head to map teacher features to a vector; fallback to logits space if needed.\n",
    "# Try to tap a common block (layer4) if exists; otherwise use logits as embedding.\n",
    "\n",
    "def build_teacher_embedder(teacher_model: nn.Module, out_dim: int = None):\n",
    "    # If teacher has layer4, hook it; else return a function that uses logits as embedding\n",
    "    layer = getattr(teacher_model, 'layer4', None)\n",
    "    if layer is None:\n",
    "        # Fallback: use logits as embedding\n",
    "        def teacher_embed(x):\n",
    "            logits = teacher_model(x)\n",
    "            e = logits if out_dim is None else F.linear(logits, torch.eye(logits.shape[1], device=logits.device))\n",
    "            return logits, e\n",
    "        return teacher_embed\n",
    "\n",
    "    class Hook:\n",
    "        def __init__(self, module):\n",
    "            self.feat = None\n",
    "            module.register_forward_hook(self._hook)\n",
    "        def _hook(self, m, inp, out):\n",
    "            self.feat = out\n",
    "    h = Hook(layer)\n",
    "\n",
    "    # simple projector after hook: GAP only to get vector\n",
    "    def teacher_embed(x):\n",
    "        logits = teacher_model(x)  # populates hook\n",
    "        if h.feat is None:\n",
    "            raise RuntimeError('Teacher feature hook not captured')\n",
    "        e = F.adaptive_avg_pool2d(h.feat, 1).flatten(1)\n",
    "        return logits, e\n",
    "\n",
    "    return teacher_embed\n",
    "\n",
    "\n",
    "def pkt_loss(e_s: torch.Tensor, e_t: torch.Tensor, sigma: float = 0.1, eps: float = 1e-12) -> torch.Tensor:\n",
    "    # L2-normalize embeddings\n",
    "    z_s = F.normalize(e_s, p=2, dim=1)  # N,d\n",
    "    z_t = F.normalize(e_t, p=2, dim=1)  # N,d\n",
    "\n",
    "    # Cosine similarity matrices (exclude self with mask later)\n",
    "    S_s = z_s @ z_s.t()   # N,N\n",
    "    S_t = z_t @ z_t.t()   # N,N\n",
    "\n",
    "    # Mask out diagonal (self-similarity) by setting to -inf before softmax\n",
    "    N = S_s.size(0)\n",
    "    mask = torch.eye(N, device=S_s.device).bool()\n",
    "    S_s = S_s.masked_fill(mask, float('-inf'))\n",
    "    S_t = S_t.masked_fill(mask, float('-inf'))\n",
    "\n",
    "    # Row-wise softmax with temperature sigma\n",
    "    P_s = F.softmax(S_s / sigma, dim=1).clamp_min(eps)  # N,N\n",
    "    P_t = F.softmax(S_t / sigma, dim=1).clamp_min(eps)  # N,N\n",
    "\n",
    "    # Row-wise KL: average over rows\n",
    "    loss = F.kl_div(P_s.log(), P_t, reduction='batchmean')\n",
    "    return loss\n",
    "\n",
    "\n",
    "student = make_fresh_student(SmallNetWithEmbed)\n",
    "teacher_embed = build_teacher_embedder(teacher)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t, e_t = teacher_embed(x)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, e_s = student(x)\n",
    "            loss_ce  = F.cross_entropy(logits_s, y)\n",
    "            loss_pkt = pkt_loss(e_s, e_t, sigma=SIGMA_PKT)\n",
    "            loss = loss_ce + LAMBDA_PKT * loss_pkt\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 9 (PKT)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v9.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v9 PKT) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Probabilistic Knowledge Transfer (PKT)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8737696",
   "metadata": {
    "papermill": {
     "duration": 0.035124,
     "end_time": "2025-09-16T11:42:37.433525",
     "exception": false,
     "start_time": "2025-09-16T11:42:37.398401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Đánh giá toàn diện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ddf9dec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:42:37.507549Z",
     "iopub.status.busy": "2025-09-16T11:42:37.507269Z",
     "iopub.status.idle": "2025-09-16T11:42:55.873138Z",
     "shell.execute_reply": "2025-09-16T11:42:55.872300Z"
    },
    "papermill": {
     "duration": 18.405494,
     "end_time": "2025-09-16T11:42:55.874703",
     "exception": false,
     "start_time": "2025-09-16T11:42:37.469209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Bảng kết quả (metrics) =====\n",
      "     model    acc   loss    ece  agree_t  kl_to_t  cos_logits  entropy\n",
      "   teacher 0.9489 0.1897 0.0278      NaN      NaN         NaN   0.0719\n",
      "v1_vanilla 0.8812 0.3722 0.0393   0.8904   0.2894      0.8679   0.2312\n",
      "    v2_cee 0.8847 0.3544 0.0128   0.8908   0.2841      0.7918   0.2946\n",
      "   v3_feat 0.8802 0.3506 0.0167   0.8881   0.2774      0.7974   0.2952\n",
      "     v4_at 0.8734 0.3731 0.0113   0.8782   0.3011      0.7827   0.3377\n",
      "  v5_logit 0.8885 0.3402 0.0244   0.8969   0.2614      0.9030   0.2735\n",
      "    v6_fkd 0.8822 0.3532 0.0085   0.8903   0.2754      0.8541   0.3351\n",
      "    v7_rkd 0.8816 0.3734 0.0187   0.8868   0.2947      0.8804   0.3576\n",
      "    v8_crd 0.8866 0.3365 0.0059   0.8940   0.2587      0.7980   0.3197\n",
      "    v9_pkt 0.8832 0.3631 0.0150   0.8900   0.2865      0.8591   0.3723\n",
      "\n",
      "===== Bảng xếp hạng (rank) =====\n",
      "     model  acc_rank  loss_rank  ece_rank  agree_t_rank  kl_to_t_rank  cos_logits_rank  entropy_rank  avg_rank\n",
      "   teacher       1.0        1.0       9.0          10.0          10.0             10.0           1.0  6.000000\n",
      "v1_vanilla       8.0        8.0      10.0           4.0           7.0              3.0           2.0  6.000000\n",
      "    v2_cee       4.0        6.0       4.0           3.0           5.0              8.0           4.0  4.857143\n",
      "   v3_feat       9.0        4.0       6.0           7.0           4.0              7.0           5.0  6.000000\n",
      "     v4_at      10.0        9.0       3.0           9.0           9.0              9.0           8.0  8.142857\n",
      "  v5_logit       2.0        3.0       8.0           1.0           2.0              1.0           3.0  2.857143\n",
      "    v6_fkd       6.0        5.0       2.0           5.0           3.0              5.0           7.0  4.714286\n",
      "    v7_rkd       7.0       10.0       7.0           8.0           8.0              2.0           9.0  7.285714\n",
      "    v8_crd       3.0        2.0       1.0           2.0           1.0              6.0           6.0  3.000000\n",
      "    v9_pkt       5.0        7.0       5.0           6.0           6.0              4.0          10.0  6.142857\n"
     ]
    }
   ],
   "source": [
    "# %% Comprehensive Evaluation: Load checkpoints and compare models\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Expect checkpoints in CKPT_DIR\n",
    "ckpt_dir = CKPT_DIR if 'CKPT_DIR' in globals() else './checkpoints'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# Build teacher & student architectures (same as training)\n",
    "teacher_eval = build_teacher(NUM_CLASSES).to(DEVICE)\n",
    "student_eval = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Utility: evaluate metrics over a dataloader\n",
    "@torch.no_grad()\n",
    "def collect_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    logits_list, labels_list = [], []\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        out = model(x)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "        loss_sum += criterion(logits, y).item()\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        logits_list.append(logits.detach().cpu())\n",
    "        labels_list.append(y.detach().cpu())\n",
    "    logits_all = torch.cat(logits_list, dim=0)\n",
    "    labels_all = torch.cat(labels_list, dim=0)\n",
    "    acc = correct / max(1, total)\n",
    "    avg_loss = loss_sum / max(1, total)\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'loss': avg_loss,\n",
    "        'logits': logits_all,\n",
    "        'labels': labels_all,\n",
    "    }\n",
    "\n",
    "# Expected checkpoints\n",
    "models = OrderedDict([\n",
    "    (\"teacher\", os.path.join(ckpt_dir, \"kd_teacher.pth\")),\n",
    "    (\"v1_vanilla\", os.path.join(ckpt_dir, \"kd_student_v1.pth\")),\n",
    "    (\"v2_cee\", os.path.join(ckpt_dir, \"kd_student_v2.pth\")),\n",
    "    (\"v3_feat\", os.path.join(ckpt_dir, \"kd_student_v3.pth\")),\n",
    "    (\"v4_at\", os.path.join(ckpt_dir, \"kd_student_v4.pth\")),\n",
    "    (\"v5_logit\", os.path.join(ckpt_dir, \"kd_student_v5.pth\")),\n",
    "    (\"v6_fkd\", os.path.join(ckpt_dir, \"kd_student_v6.pth\")),\n",
    "    (\"v7_rkd\", os.path.join(ckpt_dir, \"kd_student_v7.pth\")),\n",
    "    (\"v8_crd\", os.path.join(ckpt_dir, \"kd_student_v8.pth\")),\n",
    "    (\"v9_pkt\", os.path.join(ckpt_dir, \"kd_student_v9.pth\")),\n",
    "])\n",
    "\n",
    "# Load teacher\n",
    "loaded = {}\n",
    "if os.path.isfile(models['teacher']):\n",
    "    teacher_eval.load_state_dict(torch.load(models['teacher'], map_location=DEVICE))\n",
    "    loaded['teacher'] = teacher_eval\n",
    "else:\n",
    "    print(f\"[WARN] Teacher checkpoint not found: {models['teacher']}\")\n",
    "\n",
    "# Load students into dict (same arch SmallNet)\n",
    "for name, path in models.items():\n",
    "    if name == 'teacher':\n",
    "        continue\n",
    "    if os.path.isfile(path):\n",
    "        m = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "        m.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "        loaded[name] = m\n",
    "    else:\n",
    "        print(f\"[WARN] Student checkpoint not found: {path}\")\n",
    "\n",
    "# Metrics to compute\n",
    "# - acc_test: accuracy on test set\n",
    "# - loss_test: CE loss on test set\n",
    "# - ece: Expected Calibration Error (10-bin)\n",
    "# - agree_t: agreement rate between student and teacher predictions\n",
    "# - kl_to_t: KL(student || teacher) on test logits (softmax distributions)\n",
    "# - cos_logits: cosine similarity between student and teacher logits\n",
    "# - entropy: average predictive entropy (uncertainty)\n",
    "\n",
    "\n",
    "def expected_calibration_error(probs: torch.Tensor, labels: torch.Tensor, n_bins: int = 10) -> float:\n",
    "    # probs: N,C ; labels: N\n",
    "    confidences, predictions = probs.max(dim=1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "    bins = torch.linspace(0, 1, steps=n_bins + 1)\n",
    "    ece = torch.zeros(1)\n",
    "    for i in range(n_bins):\n",
    "        in_bin = (confidences > bins[i]) & (confidences <= bins[i + 1]) if i < n_bins - 1 else (confidences > bins[i]) & (confidences <= bins[i + 1])\n",
    "        prop = in_bin.float().mean()\n",
    "        if prop.item() > 0:\n",
    "            acc_bin = accuracies[in_bin].float().mean()\n",
    "            conf_bin = confidences[in_bin].float().mean()\n",
    "            ece += torch.abs(conf_bin - acc_bin) * prop\n",
    "    return ece.item()\n",
    "\n",
    "\n",
    "def kl_divergence(p_logits: torch.Tensor, q_logits: torch.Tensor, T: float = 1.0) -> float:\n",
    "    # KL(P||Q) with temperature T\n",
    "    p = F.log_softmax(p_logits / T, dim=1)\n",
    "    q = F.softmax(q_logits / T, dim=1)\n",
    "    return F.kl_div(p, q, reduction='batchmean').item()\n",
    "\n",
    "\n",
    "def cosine_similarity_logits(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    a_flat = a.flatten(1)\n",
    "    b_flat = b.flatten(1)\n",
    "    a_n = F.normalize(a_flat, p=2, dim=1)\n",
    "    b_n = F.normalize(b_flat, p=2, dim=1)\n",
    "    return (a_n * b_n).sum(dim=1).mean().item()\n",
    "\n",
    "\n",
    "def avg_entropy_from_logits(logits: torch.Tensor) -> float:\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    entropy = -(probs * (probs.clamp_min(1e-12).log())).sum(dim=1)\n",
    "    return entropy.mean().item()\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "metrics = [\n",
    "    'acc', 'loss', 'ece', 'agree_t', 'kl_to_t', 'cos_logits', 'entropy'\n",
    "]\n",
    "\n",
    "# First collect teacher outputs\n",
    "teacher_out = None\n",
    "if 'teacher' in loaded:\n",
    "    teacher_out = collect_metrics(loaded['teacher'], test_loader, DEVICE)\n",
    "\n",
    "for name, model in loaded.items():\n",
    "    out = collect_metrics(model, test_loader, DEVICE)\n",
    "    probs = F.softmax(out['logits'], dim=1)\n",
    "    ece = expected_calibration_error(probs, out['labels'])\n",
    "\n",
    "    # Comparisons to teacher (only if teacher available and current is not teacher)\n",
    "    if teacher_out is not None and name != 'teacher':\n",
    "        agree = (out['logits'].argmax(1) == teacher_out['logits'].argmax(1)).float().mean().item()\n",
    "        kl = kl_divergence(out['logits'], teacher_out['logits'])\n",
    "        cos = cosine_similarity_logits(out['logits'], teacher_out['logits'])\n",
    "    else:\n",
    "        agree, kl, cos = np.nan, np.nan, np.nan\n",
    "\n",
    "    ent = avg_entropy_from_logits(out['logits'])\n",
    "\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'acc': round(out['acc'], 4),\n",
    "        'loss': round(out['loss'], 4),\n",
    "        'ece': round(ece, 4),\n",
    "        'agree_t': round(agree, 4) if not np.isnan(agree) else np.nan,\n",
    "        'kl_to_t': round(kl, 4) if not np.isnan(kl) else np.nan,\n",
    "        'cos_logits': round(cos, 4) if not np.isnan(cos) else np.nan,\n",
    "        'entropy': round(ent, 4),\n",
    "    })\n",
    "\n",
    "# Build results DataFrame (sorted by model order above)\n",
    "df = pd.DataFrame(results)\n",
    "# Optional: reorder rows to keep teacher first\n",
    "order = [k for k in models.keys() if k in df['model'].values]\n",
    "df['order_idx'] = df['model'].apply(lambda m: order.index(m) if m in order else 999)\n",
    "df = df.sort_values('order_idx').drop(columns=['order_idx']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n===== Bảng kết quả (metrics) =====\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Ranking table: for each metric, compute rank (best rank = 1).\n",
    "# For metrics where lower is better (loss, ece, kl, entropy), rank ascending. For higher-better (acc, agree_t, cos), rank descending.\n",
    "rank_prefs = {\n",
    "    'acc': 'desc',\n",
    "    'loss': 'asc',\n",
    "    'ece': 'asc',\n",
    "    'agree_t': 'desc',\n",
    "    'kl_to_t': 'asc',\n",
    "    'cos_logits': 'desc',\n",
    "    'entropy': 'asc',\n",
    "}\n",
    "\n",
    "rank_df = df.copy()\n",
    "for col, pref in rank_prefs.items():\n",
    "    series = rank_df[col]\n",
    "    if series.isna().all():\n",
    "        rank_df[col + '_rank'] = np.nan\n",
    "        continue\n",
    "    # For NaN values (e.g., teacher comparisons), assign worst rank\n",
    "    fill_val = series.max() + 1 if pref == 'asc' else series.min() - 1\n",
    "    series_filled = series.fillna(fill_val)\n",
    "    ascending = (pref == 'asc')\n",
    "    rank_df[col + '_rank'] = series_filled.rank(method='min', ascending=ascending)\n",
    "\n",
    "# Keep only rank columns and model name\n",
    "rank_cols = ['model'] + [c + '_rank' for c in rank_prefs.keys()]\n",
    "rank_table = rank_df[rank_cols]\n",
    "rank_table['avg_rank'] = rank_table[[c for c in rank_cols if c != 'model']].mean(axis=1)\n",
    "\n",
    "print(\"\\n===== Bảng xếp hạng (rank) =====\")\n",
    "print(rank_table.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11284.109043,
   "end_time": "2025-09-16T11:42:57.847602",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-16T08:34:53.738559",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
