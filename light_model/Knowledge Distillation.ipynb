{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219adfb",
   "metadata": {
    "id": "e1f2a85b",
    "papermill": {
     "duration": 0.007381,
     "end_time": "2025-09-16T01:59:08.626379",
     "exception": false,
     "start_time": "2025-09-16T01:59:08.618998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Knowledge Distillation on CIFAR-10\n",
    "\n",
    "Mục tiêu: Cài đặt nhiều phương pháp Knowledge Distillation (KD) khác nhau cho bài toán phân loại CIFAR-10.\n",
    "\n",
    "Yêu cầu chính:\n",
    "- Dataset: CIFAR-10 (train/test chuẩn của torchvision)\n",
    "- Model teacher: pretrained trên CIFAR-10; model student nhỏ hơn, chưa train.\n",
    "- Một biến chung `KD_EPOCHS` xác định số epoch train cho TẤT CẢ phương pháp KD.\n",
    "- Trước mỗi phương pháp có một cell markdown ghi tên phương pháp.\n",
    "- Mỗi phương pháp in ra: tổng thời gian train + accuracy trên train và test.\n",
    "\n",
    "Các phần dưới đây cung cấp phần setup dùng chung (dataloader, model, util, teacher), sau đó là từng phương pháp KD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0d0716",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T01:59:08.640712Z",
     "iopub.status.busy": "2025-09-16T01:59:08.640440Z",
     "iopub.status.idle": "2025-09-16T02:37:32.652806Z",
     "shell.execute_reply": "2025-09-16T02:37:32.651637Z"
    },
    "id": "3d503ed3",
    "outputId": "7894f04e-bf6d-4fe1-e465-76c1ae67d25e",
    "papermill": {
     "duration": 2304.022019,
     "end_time": "2025-09-16T02:37:32.654485",
     "exception": false,
     "start_time": "2025-09-16T01:59:08.632466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:02<00:00, 66.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher checkpoint not found. Training ResNet18 teacher from scratch on CIFAR-10 (select best by val acc).\n",
      "Teacher epoch 1/100 - train_acc: 0.2973 - val_acc: 0.4008 (saved best)\n",
      "Teacher epoch 2/100 - train_acc: 0.4454 - val_acc: 0.4654 (saved best)\n",
      "Teacher epoch 3/100 - train_acc: 0.5633 - val_acc: 0.5748 (saved best)\n",
      "Teacher epoch 4/100 - train_acc: 0.6345 - val_acc: 0.5894 (saved best)\n",
      "Teacher epoch 5/100 - train_acc: 0.6826 - val_acc: 0.6918 (saved best)\n",
      "Teacher epoch 6/100 - train_acc: 0.7226 - val_acc: 0.7206 (saved best)\n",
      "Teacher epoch 7/100 - train_acc: 0.7502 - val_acc: 0.7362 (saved best)\n",
      "Teacher epoch 8/100 - train_acc: 0.7654 - val_acc: 0.6730\n",
      "Teacher epoch 9/100 - train_acc: 0.7767 - val_acc: 0.7410 (saved best)\n",
      "Teacher epoch 10/100 - train_acc: 0.7874 - val_acc: 0.7772 (saved best)\n",
      "Teacher epoch 11/100 - train_acc: 0.7947 - val_acc: 0.7330\n",
      "Teacher epoch 12/100 - train_acc: 0.7983 - val_acc: 0.7568\n",
      "Teacher epoch 13/100 - train_acc: 0.8023 - val_acc: 0.7812 (saved best)\n",
      "Teacher epoch 14/100 - train_acc: 0.8055 - val_acc: 0.7822 (saved best)\n",
      "Teacher epoch 15/100 - train_acc: 0.8136 - val_acc: 0.8066 (saved best)\n",
      "Teacher epoch 16/100 - train_acc: 0.8155 - val_acc: 0.7790\n",
      "Teacher epoch 17/100 - train_acc: 0.8187 - val_acc: 0.7884\n",
      "Teacher epoch 18/100 - train_acc: 0.8195 - val_acc: 0.7780\n",
      "Teacher epoch 19/100 - train_acc: 0.8211 - val_acc: 0.7136\n",
      "Teacher epoch 20/100 - train_acc: 0.8244 - val_acc: 0.7366\n",
      "Teacher epoch 21/100 - train_acc: 0.8253 - val_acc: 0.7670\n",
      "Teacher epoch 22/100 - train_acc: 0.8289 - val_acc: 0.8050\n",
      "Teacher epoch 23/100 - train_acc: 0.8299 - val_acc: 0.8134 (saved best)\n",
      "Teacher epoch 24/100 - train_acc: 0.8330 - val_acc: 0.8130\n",
      "Teacher epoch 25/100 - train_acc: 0.8360 - val_acc: 0.7942\n",
      "Teacher epoch 26/100 - train_acc: 0.8354 - val_acc: 0.7512\n",
      "Teacher epoch 27/100 - train_acc: 0.8371 - val_acc: 0.8322 (saved best)\n",
      "Teacher epoch 28/100 - train_acc: 0.8407 - val_acc: 0.7768\n",
      "Teacher epoch 29/100 - train_acc: 0.8426 - val_acc: 0.8180\n",
      "Teacher epoch 30/100 - train_acc: 0.8421 - val_acc: 0.8166\n",
      "Teacher epoch 31/100 - train_acc: 0.8463 - val_acc: 0.7616\n",
      "Teacher epoch 32/100 - train_acc: 0.8478 - val_acc: 0.7758\n",
      "Teacher epoch 33/100 - train_acc: 0.8501 - val_acc: 0.8296\n",
      "Teacher epoch 34/100 - train_acc: 0.8524 - val_acc: 0.8140\n",
      "Teacher epoch 35/100 - train_acc: 0.8499 - val_acc: 0.8150\n",
      "Teacher epoch 36/100 - train_acc: 0.8569 - val_acc: 0.8236\n",
      "Teacher epoch 37/100 - train_acc: 0.8563 - val_acc: 0.8500 (saved best)\n",
      "Teacher epoch 38/100 - train_acc: 0.8592 - val_acc: 0.8458\n",
      "Teacher epoch 39/100 - train_acc: 0.8602 - val_acc: 0.8322\n",
      "Teacher epoch 40/100 - train_acc: 0.8610 - val_acc: 0.8154\n",
      "Teacher epoch 41/100 - train_acc: 0.8625 - val_acc: 0.8096\n",
      "Teacher epoch 42/100 - train_acc: 0.8662 - val_acc: 0.7756\n",
      "Teacher epoch 43/100 - train_acc: 0.8650 - val_acc: 0.8474\n",
      "Teacher epoch 44/100 - train_acc: 0.8685 - val_acc: 0.8762 (saved best)\n",
      "Teacher epoch 45/100 - train_acc: 0.8720 - val_acc: 0.8368\n",
      "Teacher epoch 46/100 - train_acc: 0.8739 - val_acc: 0.8476\n",
      "Teacher epoch 47/100 - train_acc: 0.8783 - val_acc: 0.8342\n",
      "Teacher epoch 48/100 - train_acc: 0.8815 - val_acc: 0.8554\n",
      "Teacher epoch 49/100 - train_acc: 0.8810 - val_acc: 0.8672\n",
      "Teacher epoch 50/100 - train_acc: 0.8852 - val_acc: 0.8300\n",
      "Teacher epoch 51/100 - train_acc: 0.8875 - val_acc: 0.8480\n",
      "Teacher epoch 52/100 - train_acc: 0.8861 - val_acc: 0.8444\n",
      "Teacher epoch 53/100 - train_acc: 0.8882 - val_acc: 0.8790 (saved best)\n",
      "Teacher epoch 54/100 - train_acc: 0.8952 - val_acc: 0.8502\n",
      "Teacher epoch 55/100 - train_acc: 0.8952 - val_acc: 0.8492\n",
      "Teacher epoch 56/100 - train_acc: 0.8952 - val_acc: 0.8780\n",
      "Teacher epoch 57/100 - train_acc: 0.9028 - val_acc: 0.8762\n",
      "Teacher epoch 58/100 - train_acc: 0.9046 - val_acc: 0.8670\n",
      "Teacher epoch 59/100 - train_acc: 0.9068 - val_acc: 0.8702\n",
      "Teacher epoch 60/100 - train_acc: 0.9083 - val_acc: 0.8654\n",
      "Teacher epoch 61/100 - train_acc: 0.9109 - val_acc: 0.8842 (saved best)\n",
      "Teacher epoch 62/100 - train_acc: 0.9122 - val_acc: 0.8882 (saved best)\n",
      "Teacher epoch 63/100 - train_acc: 0.9155 - val_acc: 0.8774\n",
      "Teacher epoch 64/100 - train_acc: 0.9211 - val_acc: 0.8882\n",
      "Teacher epoch 65/100 - train_acc: 0.9221 - val_acc: 0.8942 (saved best)\n",
      "Teacher epoch 66/100 - train_acc: 0.9238 - val_acc: 0.8966 (saved best)\n",
      "Teacher epoch 67/100 - train_acc: 0.9272 - val_acc: 0.8988 (saved best)\n",
      "Teacher epoch 68/100 - train_acc: 0.9314 - val_acc: 0.9134 (saved best)\n",
      "Teacher epoch 69/100 - train_acc: 0.9339 - val_acc: 0.8870\n",
      "Teacher epoch 70/100 - train_acc: 0.9360 - val_acc: 0.9000\n",
      "Teacher epoch 71/100 - train_acc: 0.9390 - val_acc: 0.9052\n",
      "Teacher epoch 72/100 - train_acc: 0.9447 - val_acc: 0.9034\n",
      "Teacher epoch 73/100 - train_acc: 0.9477 - val_acc: 0.8998\n",
      "Teacher epoch 74/100 - train_acc: 0.9538 - val_acc: 0.9168 (saved best)\n",
      "Teacher epoch 75/100 - train_acc: 0.9556 - val_acc: 0.9218 (saved best)\n",
      "Teacher epoch 76/100 - train_acc: 0.9594 - val_acc: 0.9210\n",
      "Teacher epoch 77/100 - train_acc: 0.9629 - val_acc: 0.9156\n",
      "Teacher epoch 78/100 - train_acc: 0.9623 - val_acc: 0.9140\n",
      "Teacher epoch 79/100 - train_acc: 0.9705 - val_acc: 0.9308 (saved best)\n",
      "Teacher epoch 80/100 - train_acc: 0.9723 - val_acc: 0.9288\n",
      "Teacher epoch 81/100 - train_acc: 0.9734 - val_acc: 0.9230\n",
      "Teacher epoch 82/100 - train_acc: 0.9798 - val_acc: 0.9360 (saved best)\n",
      "Teacher epoch 83/100 - train_acc: 0.9824 - val_acc: 0.9324\n",
      "Teacher epoch 84/100 - train_acc: 0.9855 - val_acc: 0.9338\n",
      "Teacher epoch 85/100 - train_acc: 0.9879 - val_acc: 0.9374 (saved best)\n",
      "Teacher epoch 86/100 - train_acc: 0.9919 - val_acc: 0.9420 (saved best)\n",
      "Teacher epoch 87/100 - train_acc: 0.9932 - val_acc: 0.9426 (saved best)\n",
      "Teacher epoch 88/100 - train_acc: 0.9945 - val_acc: 0.9474 (saved best)\n",
      "Teacher epoch 89/100 - train_acc: 0.9966 - val_acc: 0.9452\n",
      "Teacher epoch 90/100 - train_acc: 0.9973 - val_acc: 0.9482 (saved best)\n",
      "Teacher epoch 91/100 - train_acc: 0.9980 - val_acc: 0.9500 (saved best)\n",
      "Teacher epoch 92/100 - train_acc: 0.9985 - val_acc: 0.9516 (saved best)\n",
      "Teacher epoch 93/100 - train_acc: 0.9986 - val_acc: 0.9518 (saved best)\n",
      "Teacher epoch 94/100 - train_acc: 0.9993 - val_acc: 0.9506\n",
      "Teacher epoch 95/100 - train_acc: 0.9989 - val_acc: 0.9516\n",
      "Teacher epoch 96/100 - train_acc: 0.9992 - val_acc: 0.9490\n",
      "Teacher epoch 97/100 - train_acc: 0.9992 - val_acc: 0.9512\n",
      "Teacher epoch 98/100 - train_acc: 0.9992 - val_acc: 0.9498\n",
      "Teacher epoch 99/100 - train_acc: 0.9995 - val_acc: 0.9508\n",
      "Teacher epoch 100/100 - train_acc: 0.9994 - val_acc: 0.9508\n",
      "Saved best teacher to ./checkpoints/kd_teacher.pth. Best val_acc: 0.9518. Training time: 2285.5s\n",
      "Teacher test acc: 0.9444\n",
      "KD_EPOCHS = 10\n"
     ]
    }
   ],
   "source": [
    "# %% Shared Setup: dependencies, config, data, models, utils\n",
    "import os, time, math, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import copy\n",
    "\n",
    "# Reproducibility\n",
    "SEED = int(os.environ.get(\"SEED\", 42))\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Global epochs shared by all KD methods\n",
    "KD_EPOCHS = int(os.environ.get(\"KD_EPOCHS\", 10))  # chỉnh tại đây nếu muốn\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "NUM_CLASSES = 10\n",
    "VAL_RATIO = float(os.environ.get(\"VAL_RATIO\", 0.1))  # 10% train -> val split\n",
    "\n",
    "# Teacher training epochs if training from scratch\n",
    "TEACHER_EPOCHS = int(os.environ.get(\"TEACHER_EPOCHS\", 100))\n",
    "\n",
    "# Ensure checkpoints directory exists\n",
    "CKPT_DIR = os.environ.get(\"CKPT_DIR\", \"./checkpoints\")\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# Data: CIFAR-10\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "train_tf = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# Build full train set twice to allow different transforms for train vs validation\n",
    "train_full_aug = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "train_full_plain = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=False, transform=test_tf)\n",
    "\n",
    "# Create reproducible train/val split\n",
    "N = len(train_full_aug)\n",
    "val_size = max(1, int(VAL_RATIO * N))\n",
    "train_size = N - val_size\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "perm = torch.randperm(N, generator=gen)\n",
    "val_idx = perm[:val_size].tolist()\n",
    "train_idx = perm[val_size:].tolist()\n",
    "\n",
    "train_set = Subset(train_full_aug, train_idx)\n",
    "val_set = Subset(train_full_plain, val_idx)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Student model (smaller than ResNet18)\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Teacher model: minimal ResNet18 for CIFAR-10 (3x3 stem, no initial maxpool)\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # layers: 2,2,2,2\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * BasicBlock.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_teacher(num_classes=10):\n",
    "    # Build a CIFAR-10 style ResNet18 from scratch\n",
    "    return ResNet18(num_classes)\n",
    "\n",
    "# Point teacher checkpoint to checkpoints/kd_teacher.pth\n",
    "TEACHER_CKPT = os.path.join(CKPT_DIR, \"kd_teacher.pth\")\n",
    "\n",
    "# Train/eval utilities\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        out = model(x)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "        loss = criterion(logits, y)\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    acc = correct / total\n",
    "    avg_loss = loss_sum / total\n",
    "    return acc, avg_loss\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    train_acc: float\n",
    "    test_acc: float\n",
    "    train_time_sec: float\n",
    "\n",
    "\n",
    "def top1_acc(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "\n",
    "def train_ce(model: nn.Module, loader: DataLoader, optimizer, device: torch.device, scaler: Optional[GradScaler] = None):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                out = model(x)\n",
    "                logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "                loss = criterion(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model(x)\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    return total_correct / max(1, total_samples)\n",
    "\n",
    "def flatten_features(x):\n",
    "    return torch.flatten(x, 1)\n",
    "\n",
    "# Hooks to grab intermediate features (for feature distillation methods)\n",
    "class FeatureHook:\n",
    "    def __init__(self, module: nn.Module):\n",
    "        self.feat = None\n",
    "        module.register_forward_hook(self.hook)\n",
    "    def hook(self, module, input, output):\n",
    "        self.feat = output\n",
    "\n",
    "# Teacher build/load\n",
    "teacher = build_teacher(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "TEACHER_OPT_LR = float(os.environ.get(\"TEACHER_OPT_LR\", 0.1))\n",
    "TEACHER_WD = float(os.environ.get(\"TEACHER_WD\", 5e-4))\n",
    "\n",
    "if os.path.isfile(TEACHER_CKPT):\n",
    "    teacher.load_state_dict(torch.load(TEACHER_CKPT, map_location=DEVICE))\n",
    "    print(f\"Loaded teacher weights from {TEACHER_CKPT}\")\n",
    "else:\n",
    "    print(\"Teacher checkpoint not found. Training ResNet18 teacher from scratch on CIFAR-10 (select best by val acc).\")\n",
    "    optimizer_t = optim.SGD(teacher.parameters(), lr=TEACHER_OPT_LR, momentum=0.9, weight_decay=TEACHER_WD)\n",
    "    scheduler_t = optim.lr_scheduler.CosineAnnealingLR(optimizer_t, T_max=TEACHER_EPOCHS)\n",
    "    scaler_t = GradScaler(enabled=torch.cuda.is_available())\n",
    "    best_val, best_state = 0.0, None\n",
    "    start_t = time.time()\n",
    "    for e in range(TEACHER_EPOCHS):\n",
    "        acc_train = train_ce(teacher, train_loader, optimizer_t, DEVICE, scaler=scaler_t)\n",
    "        acc_val, _ = evaluate(teacher, val_loader, DEVICE)\n",
    "        if acc_val > best_val:\n",
    "            best_val = acc_val\n",
    "            best_state = copy.deepcopy(teacher.state_dict())\n",
    "            torch.save(best_state, TEACHER_CKPT)\n",
    "            tag = \" (saved best)\"\n",
    "        else:\n",
    "            tag = \"\"\n",
    "        scheduler_t.step()\n",
    "        print(f\"Teacher epoch {e+1}/{TEACHER_EPOCHS} - train_acc: {acc_train:.4f} - val_acc: {acc_val:.4f}{tag}\")\n",
    "    elapsed_t = time.time() - start_t\n",
    "    if best_state is not None:\n",
    "        teacher.load_state_dict(best_state)\n",
    "    print(f\"Saved best teacher to {TEACHER_CKPT}. Best val_acc: {best_val:.4f}. Training time: {elapsed_t:.1f}s\")\n",
    "\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "teacher.eval()\n",
    "\n",
    "acc_test, _ = evaluate(teacher, test_loader, DEVICE)\n",
    "print(f\"Teacher test acc: {acc_test:.4f}\")\n",
    "\n",
    "print(\"KD_EPOCHS =\", KD_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08300b",
   "metadata": {
    "id": "795b9274",
    "papermill": {
     "duration": 0.011127,
     "end_time": "2025-09-16T02:37:32.677964",
     "exception": false,
     "start_time": "2025-09-16T02:37:32.666837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## So sánh mô hình teacher và student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b697f4d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T02:37:32.703988Z",
     "iopub.status.busy": "2025-09-16T02:37:32.703198Z",
     "iopub.status.idle": "2025-09-16T02:37:33.722605Z",
     "shell.execute_reply": "2025-09-16T02:37:33.721610Z"
    },
    "id": "eefe070d",
    "outputId": "ae9ec1ef-2829-4629-b0f5-d3da32cc2207",
    "papermill": {
     "duration": 1.033979,
     "end_time": "2025-09-16T02:37:33.723880",
     "exception": false,
     "start_time": "2025-09-16T02:37:32.689901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So sánh mô hình teacher và student (đầu vào 32x32):\n",
      "\n",
      "[Teacher (ResNet18)]\n",
      "  Params: 11173962 (trainable: 0) -> 11.17M params\n",
      "  Layers (Conv+FC): 21\n",
      "  MACs (32x32): 555.42M  |  FLOPs~ 1.11B\n",
      "  Latency: ~0.162 ms / image (batch=128, avg over repeats)\n",
      "\n",
      "[Student (SmallNet)]\n",
      "  Params: 141354 (trainable: 141354) -> 141.35K params\n",
      "  Layers (Conv+FC): 6\n",
      "  MACs (32x32): 29.20M  |  FLOPs~ 58.40M\n",
      "  Latency: ~0.016 ms / image (batch=128, avg over repeats)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  # %% So sánh teacher vs student: tham số, layer, FLOPs/MACs, Latency\n",
    "from collections import defaultdict\n",
    "\n",
    "# Choose the canonical student used by most methods\n",
    "student_ref = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "def count_params(m: nn.Module):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "def count_layers(m: nn.Module):\n",
    "    # count conv + linear layers as \"layers\"\n",
    "    return sum(isinstance(mod, (nn.Conv2d, nn.Linear)) for mod in m.modules())\n",
    "\n",
    "\n",
    "# Lightweight FLOPs/MACs estimation using hooks (counts MACs ~ multiply-adds)\n",
    "def estimate_macs(model: nn.Module, input_size=(1, 3, 32, 32)):\n",
    "    macs = 0\n",
    "\n",
    "    def conv_hook(self, inp, out):\n",
    "        nonlocal macs\n",
    "        # inp[0]: N,Cin,Hin,Win ; out: N,Cout,Hout,Wout\n",
    "        x = inp[0]\n",
    "        N, Cin, Hin, Win = x.shape\n",
    "        Cout, Hout, Wout = out.shape[1:]\n",
    "        kH, kW = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
    "        # MACs per output element: Cin * kH * kW\n",
    "        macs += N * Cout * Hout * Wout * Cin * kH * kW\n",
    "\n",
    "    def linear_hook(self, inp, out):\n",
    "        nonlocal macs\n",
    "        # inp[0]: N, in_features ; out: N, out_features\n",
    "        N, in_f = inp[0].shape\n",
    "        out_f = out.shape[1]\n",
    "        macs += N * in_f * out_f\n",
    "\n",
    "    hooks = []\n",
    "    for mod in model.modules():\n",
    "        if isinstance(mod, nn.Conv2d):\n",
    "            hooks.append(mod.register_forward_hook(conv_hook))\n",
    "        elif isinstance(mod, nn.Linear):\n",
    "            hooks.append(mod.register_forward_hook(linear_hook))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(*input_size, device=DEVICE)\n",
    "        _ = model(dummy)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # FLOPs ~ 2 * MACs if counting MUL+ADD as two ops. Report both.\n",
    "    flops = 2 * macs\n",
    "    return macs, flops\n",
    "\n",
    "\n",
    "def pretty(n):\n",
    "    # format large numbers with units\n",
    "    for unit in [\"\", \"K\", \"M\", \"B\", \"T\"]:\n",
    "        if abs(n) < 1000:\n",
    "            return f\"{n:.2f}{unit}\"\n",
    "        n /= 1000\n",
    "    return f\"{n:.2f}P\"\n",
    "\n",
    "\n",
    "def measure_latency_ms_per_image(model: nn.Module, batch_size: int = 128, repeats: int = 30, warmup: int = 10):\n",
    "    model.eval()\n",
    "    x = torch.randn(batch_size, 3, 32, 32, device=DEVICE)\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    # Timed runs\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeats):\n",
    "            t0 = time.perf_counter()\n",
    "            _ = model(x)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            t1 = time.perf_counter()\n",
    "            times.append((t1 - t0))\n",
    "    avg_s = sum(times) / len(times)\n",
    "    ms_per_image = (avg_s / batch_size) * 1000.0\n",
    "    return ms_per_image\n",
    "\n",
    "\n",
    "def report_model_stats(name: str, m: nn.Module):\n",
    "    total, trainable = count_params(m)\n",
    "    layers = count_layers(m)\n",
    "    macs, flops = estimate_macs(m)\n",
    "    latency = measure_latency_ms_per_image(m)\n",
    "    print(\n",
    "        f\"[{name}]\\n\"\n",
    "        f\"  Params: {total} (trainable: {trainable}) -> {pretty(total)} params\\n\"\n",
    "        f\"  Layers (Conv+FC): {layers}\\n\"\n",
    "        f\"  MACs (32x32): {pretty(macs)}  |  FLOPs~ {pretty(flops)}\\n\"\n",
    "        f\"  Latency: ~{latency:.3f} ms / image (batch=128, avg over repeats)\\n\"\n",
    "    )\n",
    "\n",
    "print(\"So sánh mô hình teacher và student (đầu vào 32x32):\\n\")\n",
    "report_model_stats(\"Teacher (ResNet18)\", teacher)\n",
    "report_model_stats(\"Student (SmallNet)\", student_ref)\n",
    "\n",
    "# Cleanup\n",
    "del student_ref\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00455a",
   "metadata": {
    "papermill": {
     "duration": 0.011356,
     "end_time": "2025-09-16T02:37:33.747717",
     "exception": false,
     "start_time": "2025-09-16T02:37:33.736361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Baseline: Train-from-scratch cho student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68378be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T02:37:33.772506Z",
     "iopub.status.busy": "2025-09-16T02:37:33.772236Z",
     "iopub.status.idle": "2025-09-16T02:39:34.107244Z",
     "shell.execute_reply": "2025-09-16T02:39:34.106041Z"
    },
    "papermill": {
     "duration": 120.349442,
     "end_time": "2025-09-16T02:39:34.108847",
     "exception": false,
     "start_time": "2025-09-16T02:37:33.759405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_acc: 0.4287 - val_acc: 0.4850 (best)\n",
      "Epoch 2/10 - train_acc: 0.6060 - val_acc: 0.5872 (best)\n",
      "Epoch 3/10 - train_acc: 0.6714 - val_acc: 0.5096\n",
      "Epoch 4/10 - train_acc: 0.7094 - val_acc: 0.6482 (best)\n",
      "Epoch 5/10 - train_acc: 0.7416 - val_acc: 0.6676 (best)\n",
      "Epoch 6/10 - train_acc: 0.7661 - val_acc: 0.7648 (best)\n",
      "Epoch 7/10 - train_acc: 0.7890 - val_acc: 0.7742 (best)\n",
      "Epoch 8/10 - train_acc: 0.8105 - val_acc: 0.7892 (best)\n",
      "Epoch 9/10 - train_acc: 0.8344 - val_acc: 0.8280 (best)\n",
      "Epoch 10/10 - train_acc: 0.8469 - val_acc: 0.8378 (best)\n",
      "Saved student checkpoint (v0) to ./checkpoints/kd_student_v0.pth\n",
      "{'method': 'Baseline CE (no teacher)', 'train_time_sec': 109.38, 'train_acc': 0.8603, 'val_acc': 0.8378, 'test_acc': 0.8277}\n"
     ]
    }
   ],
   "source": [
    "# %% Train student WITHOUT teacher (baseline CE) and save kd_student_v0.pth\n",
    "# Hyperparams\n",
    "LR = 0.1\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "# Build student\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    # one epoch CE-only\n",
    "    train_acc_epoch = train_ce(student, train_loader, optimizer, DEVICE, scaler=scaler)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save checkpoint\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v0.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v0) to {ckpt_path}\")\n",
    "\n",
    "# Final report\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Baseline CE (no teacher)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2043040",
   "metadata": {
    "id": "81a6e5cb",
    "papermill": {
     "duration": 0.01248,
     "end_time": "2025-09-16T02:39:34.134484",
     "exception": false,
     "start_time": "2025-09-16T02:39:34.122004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 1: Vanilla KD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d69e5",
   "metadata": {
    "papermill": {
     "duration": 0.012024,
     "end_time": "2025-09-16T02:39:34.158861",
     "exception": false,
     "start_time": "2025-09-16T02:39:34.146837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Để student học theo \"soft targets\" của  thay vì chỉ dựa vào nhãn cứng. Soft targets giữ lại thông tin về độ tự tin giữa các lớp.\n",
    "\n",
    "Ký hiệu cho một mẫu $(x, y)$:\n",
    "- $z_t$ = logits của teacher, $z_s$ = logits của student.\n",
    "- Nhiệt độ (temperature) $\\tau>0$ làm mềm phân phối:  \n",
    "  $$\n",
    "  p_t^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_t}{\\tau}\\right), \\quad\n",
    "  p_s^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_s}{\\tau}\\right).\n",
    "  $$\n",
    "\n",
    "Hàm mất mát KD dùng KL-divergence giữa phân phối mềm của teacher và student, có hệ số hiệu chỉnh $\\tau^2$:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{KD}} = \\tau^2 \\, \\mathrm{KL}\\big( p_t^{(\\tau)} \\,\\Vert\\, p_s^{(\\tau)} \\big).\n",
    "$$\n",
    "\n",
    "Kết hợp với cross-entropy (CE) chuẩn theo nhãn thật $y$:\n",
    "$$\n",
    "\\mathcal{L} = \\alpha\\,\\mathcal{L}_{\\mathrm{KD}} + (1-\\alpha)\\, \\mathrm{CE}(z_s, y).\n",
    "$$\n",
    "- $\\alpha\\in[0,1]$ điều chỉnh tỷ trọng giữa “học theo giáo viên” và “học theo nhãn thật”.\n",
    "- $\\tau$ lớn làm phân phối mềm hơn (giảm cực đoan), giúp student học được cấu trúc liên lớp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b7e591",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T02:39:34.184925Z",
     "iopub.status.busy": "2025-09-16T02:39:34.184040Z",
     "iopub.status.idle": "2025-09-16T02:42:02.368906Z",
     "shell.execute_reply": "2025-09-16T02:42:02.367901Z"
    },
    "id": "9950493c",
    "outputId": "41b0cf57-a916-48b3-fb0c-dd1a651d53e4",
    "papermill": {
     "duration": 148.199893,
     "end_time": "2025-09-16T02:42:02.370815",
     "exception": false,
     "start_time": "2025-09-16T02:39:34.170922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_acc: 0.4361 - val_acc: 0.5826 (best)\n",
      "Epoch 2/10 - train_acc: 0.6066 - val_acc: 0.6080 (best)\n",
      "Epoch 3/10 - train_acc: 0.6764 - val_acc: 0.6750 (best)\n",
      "Epoch 4/10 - train_acc: 0.7204 - val_acc: 0.7090 (best)\n",
      "Epoch 5/10 - train_acc: 0.7541 - val_acc: 0.6478\n",
      "Epoch 6/10 - train_acc: 0.7768 - val_acc: 0.7130 (best)\n",
      "Epoch 7/10 - train_acc: 0.7994 - val_acc: 0.7596 (best)\n",
      "Epoch 8/10 - train_acc: 0.8232 - val_acc: 0.8162 (best)\n",
      "Epoch 9/10 - train_acc: 0.8387 - val_acc: 0.8324 (best)\n",
      "Epoch 10/10 - train_acc: 0.8541 - val_acc: 0.8432 (best)\n",
      "Saved student checkpoint (v1) to ./checkpoints/kd_student_v1.pth\n",
      "{'method': 'Vanilla KD', 'train_time_sec': 137.3, 'train_acc': 0.8638, 'val_acc': 0.8432, 'test_acc': 0.8413}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Vanilla KD\n",
    "# Hyperparams for KD\n",
    "T = 4.0\n",
    "ALPHA = 0.7\n",
    "LR = 0.1\n",
    "\n",
    "# KD loss function for Vanilla KD\n",
    "def kd_loss_vanilla(logits_s, logits_t, y, T=4.0, alpha=0.5):\n",
    "    ce = F.cross_entropy(logits_s, y)\n",
    "    p_s = F.log_softmax(logits_s / T, dim=1)\n",
    "    p_t = F.softmax(logits_t / T, dim=1)\n",
    "    kd = F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "    return alpha * kd + (1 - alpha) * ce\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = kd_loss_vanilla(logits_s, logits_t, y, T=T, alpha=ALPHA)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 1\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v1.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v1) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Vanilla KD\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c76b69",
   "metadata": {
    "id": "afb52880",
    "papermill": {
     "duration": 0.012596,
     "end_time": "2025-09-16T02:42:02.396920",
     "exception": false,
     "start_time": "2025-09-16T02:42:02.384324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 2: Hard-label Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae8690",
   "metadata": {
    "papermill": {
     "duration": 0.012289,
     "end_time": "2025-09-16T02:42:02.422015",
     "exception": false,
     "start_time": "2025-09-16T02:42:02.409726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Biến dự đoán của teacher thành ''nhãn cứng'' giả (pseudo-label) rồi kết hợp với nhãn thật.\n",
    "\n",
    "Với một mẫu $(x, y)$:\n",
    "- $z_t$ = logits của teacher, nhãn giả của teacher là:  \n",
    "  $$\n",
    "  \\tilde{y} = \\arg\\max\\limits_{c}\\; z_{t,c}.\n",
    "  $$\n",
    "- $z_s$ = logits của student.\n",
    "\n",
    "Hàm mất mát kết hợp hai cross-entropy:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{CEE}} = \\beta\\,\\mathrm{CE}(z_s, \\tilde{y}) + (1-\\beta)\\, \\mathrm{CE}(z_s, y).\n",
    "$$\n",
    "- Thành phần $\\mathrm{CE}(z_s, y)$ giúp bám sát nhãn thật.\n",
    "- Thành phần $\\mathrm{CE}(z_s, \\tilde{y})$ ép student bắt chước dự đoán mạnh nhất của teacher (hard target).\n",
    "- $\\beta$ điều chỉnh mức tin cậy vào teacher.\n",
    "\n",
    "Khác với Vanilla KD, CEE không dùng nhiệt độ hay phân phối mềm; nó chỉ dựa vào lớp có xác suất cao nhất của teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b39f0d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T02:42:02.449548Z",
     "iopub.status.busy": "2025-09-16T02:42:02.448748Z",
     "iopub.status.idle": "2025-09-16T02:44:29.293506Z",
     "shell.execute_reply": "2025-09-16T02:44:29.292302Z"
    },
    "id": "403fd21e",
    "outputId": "c6e0b59b-97a2-47a1-b050-bf9fd850b797",
    "papermill": {
     "duration": 146.860374,
     "end_time": "2025-09-16T02:44:29.294946",
     "exception": false,
     "start_time": "2025-09-16T02:42:02.434572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_acc: 0.4412 - val_acc: 0.5120 (best)\n",
      "Epoch 2/10 - train_acc: 0.6141 - val_acc: 0.5824 (best)\n",
      "Epoch 3/10 - train_acc: 0.6813 - val_acc: 0.6470 (best)\n",
      "Epoch 4/10 - train_acc: 0.7173 - val_acc: 0.6382\n",
      "Epoch 5/10 - train_acc: 0.7433 - val_acc: 0.6976 (best)\n",
      "Epoch 6/10 - train_acc: 0.7716 - val_acc: 0.6708\n",
      "Epoch 7/10 - train_acc: 0.7925 - val_acc: 0.7250 (best)\n",
      "Epoch 8/10 - train_acc: 0.8150 - val_acc: 0.7822 (best)\n",
      "Epoch 9/10 - train_acc: 0.8374 - val_acc: 0.8288 (best)\n",
      "Epoch 10/10 - train_acc: 0.8497 - val_acc: 0.8496 (best)\n",
      "Saved student checkpoint (v2) to ./checkpoints/kd_student_v2.pth\n",
      "{'method': 'Hard-label Distillation (CEE)', 'train_time_sec': 136.03, 'train_acc': 0.8627, 'val_acc': 0.8496, 'test_acc': 0.8363}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Hard-label Distillation (CEE)\n",
    "BETA = 0.7\n",
    "LR = 0.1\n",
    "\n",
    "# CEE loss function\n",
    "def cee_loss(logits_s, logits_t, y, beta=0.7):\n",
    "    # CEE: combine CE(student, y) and CE(student, teacher_label)\n",
    "    ce_y = F.cross_entropy(logits_s, y)\n",
    "    pseudo = logits_t.argmax(dim=1)\n",
    "    ce_t = F.cross_entropy(logits_s, pseudo)\n",
    "    return beta * ce_t + (1 - beta) * ce_y\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = cee_loss(logits_s, logits_t, y, beta=BETA)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 2\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v2.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v2) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Hard-label Distillation (CEE)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53943c9b",
   "metadata": {
    "id": "4d6604d1",
    "papermill": {
     "duration": 0.013612,
     "end_time": "2025-09-16T02:44:29.322206",
     "exception": false,
     "start_time": "2025-09-16T02:44:29.308594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 3: Feature Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096eaefe",
   "metadata": {
    "papermill": {
     "duration": 0.011885,
     "end_time": "2025-09-16T02:44:29.346440",
     "exception": false,
     "start_time": "2025-09-16T02:44:29.334555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Khớp đặc trưng trung gian (feature maps) giữa teacher và student để student học biểu diễn gần giống teacher.\n",
    "\n",
    "Ký hiệu theo minibatch:\n",
    "- $F_t \\in \\mathbb{R}^{N\\times C_t\\times H_t\\times W_t}$: đặc trưng của teacher ở một tầng (ví dụ layer cuối conv).\n",
    "- $F_s \\in \\mathbb{R}^{N\\times C_s\\times H_s\\times W_s}$: đặc trưng của student ở tầng tương ứng.\n",
    "- Do số kênh/không gian khác nhau, ta dùng một “đầu chiếu” $g_s(\\cdot)$ để đưa $F_s$ về không gian của $F_t$ và/hoặc nội suy không gian về cùng kích thước.\n",
    "- Chuẩn hoá theo kênh để giảm lệch về biên độ:  \n",
    "  $$\n",
    "  \\widehat{F} = \\frac{F}{\\sqrt{\\sum\\limits_{c} F_c^2}+\\varepsilon}.\n",
    "  $$\n",
    "\n",
    "Hàm mất mát tổng hợp:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s, y) + \\lambda_{\\mathrm{feat}}\\, \\big\\|\\, \\widehat{g_s(F_s)} - \\widehat{F_t} \\,\\big\\|_2^2.\n",
    "$$\n",
    "- CE đảm bảo mục tiêu phân loại; \n",
    "- MSE giữa đặc trưng đã chuẩn hoá giúp student học cấu trúc biểu diễn của teacher.\n",
    "- Trong thực nghiệm thường tăng dần hệ số $\\lambda_{\\mathrm{feat}}$ (ramp-up) để ổn định huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac47169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T02:44:29.374229Z",
     "iopub.status.busy": "2025-09-16T02:44:29.373946Z",
     "iopub.status.idle": "2025-09-16T02:47:04.645327Z",
     "shell.execute_reply": "2025-09-16T02:47:04.644230Z"
    },
    "id": "3e2de962",
    "outputId": "9322bf2f-8e29-4e06-f2f1-f8bb5422e915",
    "papermill": {
     "duration": 155.287223,
     "end_time": "2025-09-16T02:47:04.646691",
     "exception": false,
     "start_time": "2025-09-16T02:44:29.359468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_acc: 0.4300 - val_acc: 0.5252 - feat_w: 10.0 (best)\n",
      "Epoch 2/10 - train_acc: 0.6136 - val_acc: 0.5818 - feat_w: 20.0 (best)\n",
      "Epoch 3/10 - train_acc: 0.6775 - val_acc: 0.5326 - feat_w: 30.0\n",
      "Epoch 4/10 - train_acc: 0.7166 - val_acc: 0.6638 - feat_w: 40.0 (best)\n",
      "Epoch 5/10 - train_acc: 0.7431 - val_acc: 0.7366 - feat_w: 50.0 (best)\n",
      "Epoch 6/10 - train_acc: 0.7678 - val_acc: 0.7694 - feat_w: 50.0 (best)\n",
      "Epoch 7/10 - train_acc: 0.7936 - val_acc: 0.7200 - feat_w: 50.0\n",
      "Epoch 8/10 - train_acc: 0.8144 - val_acc: 0.8056 - feat_w: 50.0 (best)\n",
      "Epoch 9/10 - train_acc: 0.8353 - val_acc: 0.8348 - feat_w: 50.0 (best)\n",
      "Epoch 10/10 - train_acc: 0.8498 - val_acc: 0.8494 - feat_w: 50.0 (best)\n",
      "Saved student checkpoint (v3) to ./checkpoints/kd_student_v3.pth\n",
      "{'method': 'Feature Distillation (MSE)', 'train_time_sec': 143.98, 'train_acc': 0.8634, 'val_acc': 0.8494, 'test_acc': 0.8349}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Feature Distillation (penultimate features)\n",
    "# We'll tap student at its final conv output (before average pool) and teacher at layer4 output.\n",
    "LR = 0.1\n",
    "W_FEAT = 50.0  # lower weight; we will ramp it up during training\n",
    "\n",
    "# Keep student architecture unchanged; just expose features\n",
    "class StudentExposeFeat(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)  # N,128,4,4\n",
    "        logits = self.classifier(f)\n",
    "        return logits, f\n",
    "\n",
    "student = StudentExposeFeat(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Teacher hook at last conv block (layer4)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Separate projection head (FitNet-style regressor) outside the student\n",
    "proj_s = nn.Sequential(\n",
    "    nn.Conv2d(128, 512, kernel_size=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace=True),\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.SGD(list(student.parameters()) + list(proj_s.parameters()), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# channel-wise L2 normalization helper\n",
    "def norm_channel(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    return x / (x.pow(2).sum(dim=1, keepdim=True).sqrt().clamp_min(eps))\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train(); proj_s.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    # linear ramp-up for feature loss (first half epochs)\n",
    "    ramp = min(1.0, (epoch + 1) / max(1, KD_EPOCHS // 2))\n",
    "    feat_w = W_FEAT * ramp\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook_t.feat\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            f_s_proj = proj_s(f_s)  # project to 512 channels\n",
    "            # Align spatial dims if needed using adaptive avgpool to teacher spatial size\n",
    "            if f_t is None:\n",
    "                raise RuntimeError(\"Teacher feature hook not captured.\")\n",
    "            if f_s_proj.shape[-2:] != f_t.shape[-2:]:\n",
    "                f_s_resized = F.adaptive_avg_pool2d(f_s_proj, f_t.shape[-2:])\n",
    "            else:\n",
    "                f_s_resized = f_s_proj\n",
    "            # Normalize features along channel dimension to reduce scale mismatch\n",
    "            nf_s = norm_channel(f_s_resized)\n",
    "            nf_t = norm_channel(f_t)\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            loss_feat = F.mse_loss(nf_s, nf_t.detach())\n",
    "            loss = loss_ce + feat_w * loss_feat\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    student.eval(); proj_s.eval()\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = {\n",
    "            'student': copy.deepcopy(student.state_dict()),\n",
    "            'proj_s': copy.deepcopy(proj_s.state_dict()),\n",
    "        }\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f} - feat_w: {feat_w:.1f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state['student'])\n",
    "    proj_s.load_state_dict(best_state['proj_s'])\n",
    "\n",
    "# Save student checkpoint for method 3\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v3.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v3) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Feature Distillation (MSE)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307be2d",
   "metadata": {
    "id": "bfb25fc4",
    "papermill": {
     "duration": 0.013442,
     "end_time": "2025-09-16T02:47:04.674755",
     "exception": false,
     "start_time": "2025-09-16T02:47:04.661313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 4: Attention Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d64d11",
   "metadata": {
    "papermill": {
     "duration": 0.013719,
     "end_time": "2025-09-16T02:47:04.702098",
     "exception": false,
     "start_time": "2025-09-16T02:47:04.688379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Khớp toàn bộ feature, AT khớp \"bản đồ chú ý\" (attention map) – độ mạnh tổng hợp theo kênh ở từng vị trí không gian.\n",
    "\n",
    "Cho feature $F \\in \\mathbb{R}^{N\\times C\\times H\\times W}$, bản đồ chú ý $A \\in \\mathbb{R}^{N\\times H\\times W}$ được định nghĩa (một biến thể hay dùng):\n",
    "$$\n",
    "A = \\frac{\\tfrac{1}{C}\\sum\\limits_{c=1}^C F_c^2}{\\left\\|\\, \\tfrac{1}{C}\\sum\\limits_{c=1}^C F_c^2 \\right\\|_2 + \\varepsilon}.\n",
    "$$\n",
    "\n",
    "Với $A_s, A_t$ lần lượt của student và teacher (đã chuẩn hoá), hàm mất mát AT:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{AT}} = \\lambda_{\\mathrm{AT}}\\, \\big\\| A_s - A_t \\big\\|_2^2.\n",
    "$$\n",
    "Tổng mất mát:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s, y) + \\mathcal{L}_{\\mathrm{AT}}.\n",
    "$$\n",
    "\n",
    "AT truyền \"nơi nào quan trọng\" trong ảnh theo teacher. Student học tập trung vào vùng hữu ích thay vì khớp mọi chi tiết của feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65492d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T02:47:04.731896Z",
     "iopub.status.busy": "2025-09-16T02:47:04.731602Z",
     "iopub.status.idle": "2025-09-16T02:49:38.667794Z",
     "shell.execute_reply": "2025-09-16T02:49:38.666724Z"
    },
    "id": "c10bc1cb",
    "outputId": "9b99cce7-8dbc-439e-d55f-319d44f92554",
    "papermill": {
     "duration": 153.953004,
     "end_time": "2025-09-16T02:49:38.669312",
     "exception": false,
     "start_time": "2025-09-16T02:47:04.716308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_acc: 0.3885 - val_acc: 0.4950 (best)\n",
      "Epoch 2/10 - train_acc: 0.5659 - val_acc: 0.5994 (best)\n",
      "Epoch 3/10 - train_acc: 0.6412 - val_acc: 0.5738\n",
      "Epoch 4/10 - train_acc: 0.6855 - val_acc: 0.6546 (best)\n",
      "Epoch 5/10 - train_acc: 0.7167 - val_acc: 0.6630 (best)\n",
      "Epoch 6/10 - train_acc: 0.7469 - val_acc: 0.7638 (best)\n",
      "Epoch 7/10 - train_acc: 0.7724 - val_acc: 0.7658 (best)\n",
      "Epoch 8/10 - train_acc: 0.7917 - val_acc: 0.7810 (best)\n",
      "Epoch 9/10 - train_acc: 0.8083 - val_acc: 0.8066 (best)\n",
      "Epoch 10/10 - train_acc: 0.8202 - val_acc: 0.8186 (best)\n",
      "Saved student checkpoint (v4) to ./checkpoints/kd_student_v4.pth\n",
      "{'method': 'Attention Transfer', 'train_time_sec': 142.78, 'train_acc': 0.833, 'val_acc': 0.8186, 'test_acc': 0.8073}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Attention Transfer\n",
    "LR = 0.05\n",
    "W_AT = 250.0\n",
    "\n",
    "# Attention Transfer loss function\n",
    "def attention_transfer_loss(f_s, f_t, w=1.0, eps=1e-6):\n",
    "    # Attention Transfer (Zagoruyko & Komodakis): match normalized spatial attention maps\n",
    "    def att_map(f):\n",
    "        # f: N, C, H, W -> N, H, W\n",
    "        am = f.pow(2).mean(dim=1)\n",
    "        am = am / (am.flatten(1).norm(p=2, dim=1, keepdim=True).clamp_min(eps).view(-1,1,1))\n",
    "        return am\n",
    "    a_s, a_t = att_map(f_s), att_map(f_t)\n",
    "    return w * F.mse_loss(a_s, a_t.detach())\n",
    "\n",
    "class StudentWithFeatAT(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        logits = self.classifier(f)\n",
    "        return logits, f\n",
    "\n",
    "student = StudentWithFeatAT(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            if f_t is None:\n",
    "                raise RuntimeError(\"Teacher feature hook not captured.\")\n",
    "            if f_s.shape[-2:] != f_t.shape[-2:]:\n",
    "                f_s_resized = F.adaptive_avg_pool2d(f_s, f_t.shape[-2:])\n",
    "            else:\n",
    "                f_s_resized = f_s\n",
    "            loss = F.cross_entropy(logits_s, y) + attention_transfer_loss(f_s_resized, f_t, w=W_AT)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 4\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v4.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v4) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Attention Transfer\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c73524",
   "metadata": {
    "id": "60690a71",
    "papermill": {
     "duration": 0.013746,
     "end_time": "2025-09-16T02:49:38.696973",
     "exception": false,
     "start_time": "2025-09-16T02:49:38.683227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 5: Logit Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6364c",
   "metadata": {
    "papermill": {
     "duration": 0.01334,
     "end_time": "2025-09-16T02:49:38.723499",
     "exception": false,
     "start_time": "2025-09-16T02:49:38.710159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Ép vector logit của student gần với logit của teacher bằng tổn thất $L_2$, đồng thời giữ CE theo nhãn thật.\n",
    "\n",
    "Với một mẫu $(x, y)$:\n",
    "- $z_t, z_s \\in \\mathbb{R}^C$ là logits (trước softmax) của teacher và student.\n",
    "- Tổn thất logit matching:\n",
    "  $$\n",
    "  \\mathcal{L}_{\\mathrm{logit}} = \\big\\| z_s - z_t \\big\\|_2^2.\n",
    "  $$\n",
    "- Tổng mất mát:\n",
    "  $$\n",
    "  \\mathcal{L} = \\mathrm{CE}(z_s, y) + \\lambda\\, \\mathcal{L}_{\\mathrm{logit}}.\n",
    "  $$\n",
    "\n",
    "Khác Vanilla KD (dùng KL trên phân phối mềm), cách này làm việc trực tiếp ở không gian logit, thường đơn giản và ổn định nhưng có thể kém nhạy với cấu trúc phân phối so với KD dùng temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d18b9e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T02:49:38.752115Z",
     "iopub.status.busy": "2025-09-16T02:49:38.751771Z",
     "iopub.status.idle": "2025-09-16T02:52:05.796323Z",
     "shell.execute_reply": "2025-09-16T02:52:05.795310Z"
    },
    "id": "698fb456",
    "outputId": "84ef92ac-0829-40e3-8200-4d573e330ee7",
    "papermill": {
     "duration": 147.060761,
     "end_time": "2025-09-16T02:52:05.797640",
     "exception": false,
     "start_time": "2025-09-16T02:49:38.736879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_acc: 0.3546 - val_acc: 0.4810 (best)\n",
      "Epoch 2/10 - train_acc: 0.5628 - val_acc: 0.6238 (best)\n",
      "Epoch 3/10 - train_acc: 0.6456 - val_acc: 0.6496 (best)\n",
      "Epoch 4/10 - train_acc: 0.7034 - val_acc: 0.5976\n",
      "Epoch 5/10 - train_acc: 0.7427 - val_acc: 0.7186 (best)\n",
      "Epoch 6/10 - train_acc: 0.7744 - val_acc: 0.7298 (best)\n",
      "Epoch 7/10 - train_acc: 0.7980 - val_acc: 0.7794 (best)\n",
      "Epoch 8/10 - train_acc: 0.8202 - val_acc: 0.7968 (best)\n",
      "Epoch 9/10 - train_acc: 0.8380 - val_acc: 0.8304 (best)\n",
      "Epoch 10/10 - train_acc: 0.8542 - val_acc: 0.8510 (best)\n",
      "Saved student checkpoint (v5) to ./checkpoints/kd_student_v5.pth\n",
      "{'method': 'Logit Matching (L2) + CE', 'train_time_sec': 135.97, 'train_acc': 0.8618, 'val_acc': 0.851, 'test_acc': 0.8385}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Logit Matching (L2) + CE\n",
    "LR = 0.1\n",
    "W_LOGIT = 1.0\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = F.cross_entropy(logits_s, y) + W_LOGIT * F.mse_loss(logits_s, logits_t)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 5\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v5.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v5) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Logit Matching (L2) + CE\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e311a",
   "metadata": {
    "id": "ebfcc591",
    "papermill": {
     "duration": 0.014023,
     "end_time": "2025-09-16T02:52:05.826059",
     "exception": false,
     "start_time": "2025-09-16T02:52:05.812036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 6: Focal Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e4e3b",
   "metadata": {
    "papermill": {
     "duration": 0.014182,
     "end_time": "2025-09-16T02:52:05.854455",
     "exception": false,
     "start_time": "2025-09-16T02:52:05.840273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** Nhấn mạnh các mẫu/nhãn mà student còn khó (sai hoặc xác suất thấp ở lớp đúng) khi khớp với phân phối của teacher. Thay vì đối xử đồng đều như KL thông thường, Focal KD tái trọng số từng mẫu/lớp theo độ khó, giúp giảm nhiễu ở các trường hợp teacher quá tự tin và tăng cường học ở các trường hợp student còn yếu.\n",
    "\n",
    "Ký hiệu cho một mẫu $(x, y)$:\n",
    "- $z_t, z_s \\in \\mathbb{R}^C$: logits của teacher và student.\n",
    "- Nhiệt độ $\\tau>0$ làm mềm phân phối:\n",
    "  $$\n",
    "  p_t^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_t}{\\tau}\\right),\\quad\n",
    "  p_s^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_s}{\\tau}\\right).\n",
    "  $$\n",
    "\n",
    "Trọng số tiêu điểm (focal) cho từng lớp $c$ phụ thuộc vào độ khó theo student:\n",
    "- Độ khó ở lớp $c$: $d_c = 1 - p_s^{(\\tau)}(c)$.\n",
    "- Trọng số:\n",
    "  $$\n",
    "  w_c = d_c^{\\gamma} = \\big(1 - p_s^{(\\tau)}(c)\\big)^{\\gamma},\\qquad \\gamma \\ge 0.\n",
    "  $$\n",
    "- Khi $\\gamma$ lớn, mẫu/lớp khó (student dự đoán thấp) sẽ được nhấn mạnh mạnh hơn.\n",
    "\n",
    "Hàm mất mát Focal KD dùng KL có trọng số:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{FKD}} = \\tau^2\\, \\sum_{c=1}^C w_c\\, p_t^{(\\tau)}(c)\\, \\log\\frac{p_t^{(\\tau)}(c)}{p_s^{(\\tau)}(c)}\n",
    "\\;=\\; \\tau^2\\, \\mathrm{KL}\\big( p_t^{(\\tau)} \\,\\Vert\\, p_s^{(\\tau)};\\, w\\big),\n",
    "$$\n",
    "trong đó ký hiệu $\\mathrm{KL}(\\cdot\\,\\Vert\\,\\cdot; w)$ là KL được tính với trọng số theo thành phần.\n",
    "\n",
    "Tổng mất mát huấn luyện:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s, y) + \\lambda_{\\mathrm{FKD}}\\, \\mathcal{L}_{\\mathrm{FKD}}.\n",
    "$$\n",
    "- $\\tau$ điều chỉnh độ mềm; nhân $\\tau^2$ để cân bằng thang độ như KD cổ điển.\n",
    "- $\\gamma$ điều chỉnh mức độ “tiêu điểm”; $\\lambda_{\\mathrm{FKD}}$ cân bằng giữa CE và FKD.\n",
    "- Ưu điểm: tập trung vào những phần student còn yếu, giảm tác động của các thành phần đã dễ; thường cải thiện độ chính xác khi Vanilla KD/DKD kém hiệu quả do mất cân bằng độ khó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "864311f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T02:52:05.884492Z",
     "iopub.status.busy": "2025-09-16T02:52:05.884170Z",
     "iopub.status.idle": "2025-09-16T02:54:37.855410Z",
     "shell.execute_reply": "2025-09-16T02:54:37.854358Z"
    },
    "id": "e880452c",
    "outputId": "ee25de2a-38be-48f7-880b-b139844f4a1f",
    "papermill": {
     "duration": 151.988231,
     "end_time": "2025-09-16T02:54:37.856690",
     "exception": false,
     "start_time": "2025-09-16T02:52:05.868459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_acc: 0.3813 - val_acc: 0.4538 (best)\n",
      "Epoch 2/10 - train_acc: 0.5643 - val_acc: 0.5450 (best)\n",
      "Epoch 3/10 - train_acc: 0.6486 - val_acc: 0.5306\n",
      "Epoch 4/10 - train_acc: 0.6991 - val_acc: 0.6566 (best)\n",
      "Epoch 5/10 - train_acc: 0.7353 - val_acc: 0.6620 (best)\n",
      "Epoch 6/10 - train_acc: 0.7606 - val_acc: 0.7622 (best)\n",
      "Epoch 7/10 - train_acc: 0.7887 - val_acc: 0.7702 (best)\n",
      "Epoch 8/10 - train_acc: 0.8140 - val_acc: 0.7650\n",
      "Epoch 9/10 - train_acc: 0.8316 - val_acc: 0.8098 (best)\n",
      "Epoch 10/10 - train_acc: 0.8501 - val_acc: 0.8370 (best)\n",
      "Saved student checkpoint (v6 FKD) to ./checkpoints/kd_student_v6.pth\n",
      "{'method': 'Focal Knowledge Distillation (FKD)', 'train_time_sec': 140.84, 'train_acc': 0.8564, 'val_acc': 0.837, 'test_acc': 0.8275}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Focal Knowledge Distillation (FKD)\n",
    "T = 4.0\n",
    "GAMMA_FKD = 2.0      # focal gamma\n",
    "LAMBDA_FKD = 1.0     # weight for FKD loss\n",
    "LR = 0.1\n",
    "\n",
    "\n",
    "def fkd_loss(logits_s, logits_t, T=4.0, gamma=2.0, eps=1e-12):\n",
    "    # Probabilities with temperature\n",
    "    ps = F.softmax(logits_s / T, dim=1)   # N,C\n",
    "    pt = F.softmax(logits_t / T, dim=1)   # N,C\n",
    "\n",
    "    # Focal weights: higher weight where student is uncertain (harder)\n",
    "    w = (1.0 - ps).clamp(min=0.0, max=1.0) ** gamma   # N,C\n",
    "\n",
    "    # Weighted KL(pt || ps)\n",
    "    # Compute element-wise: pt * (log pt - log ps) * w, average over batch\n",
    "    log_ps = (ps + eps).log()\n",
    "    log_pt = (pt + eps).log()\n",
    "    kl_elem = w * pt * (log_pt - log_ps)\n",
    "    loss = kl_elem.sum(dim=1).mean()  # batch mean\n",
    "    return (T * T) * loss\n",
    "\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss_ce  = F.cross_entropy(logits_s, y)\n",
    "            loss_kd  = fkd_loss(logits_s, logits_t, T=T, gamma=GAMMA_FKD)\n",
    "            loss     = loss_ce + LAMBDA_FKD * loss_kd\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 6 (FKD) - giữ nguyên tên file để phần đánh giá không cần sửa\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v6.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v6 FKD) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Focal Knowledge Distillation (FKD)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd33a2b",
   "metadata": {
    "id": "9552435b",
    "papermill": {
     "duration": 0.013877,
     "end_time": "2025-09-16T02:54:37.885800",
     "exception": false,
     "start_time": "2025-09-16T02:54:37.871923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 7: Relational Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4090c8",
   "metadata": {
    "papermill": {
     "duration": 0.014916,
     "end_time": "2025-09-16T02:54:37.914955",
     "exception": false,
     "start_time": "2025-09-16T02:54:37.900039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** RKD không khớp trực tiếp đặc trưng/logit mà khớp các “quan hệ” giữa các mẫu trong batch.\n",
    "\n",
    "Biểu diễn vector hoá (sau pool) và chuẩn hoá $L_2$:  \n",
    "$$\n",
    "z_s = \\mathrm{norm}(\\mathrm{GAP}(F_s)), \\quad z_t = \\mathrm{norm}(\\mathrm{GAP}(F_t)).\n",
    "$$\n",
    "với $\\mathrm{GAP}$ là global average pooling, $\\mathrm{norm}$ là chuẩn hoá $L_2$.\n",
    "\n",
    "1) Khoảng cách cặp (pairwise distance):\n",
    "- Ma trận khoảng cách Euclid:  \n",
    "  $$\n",
    "  D(z)_{ij} = \\| z_i - z_j \\|_2.\n",
    "  $$\n",
    "- Chuẩn hoá theo trung bình phần tử dương để bất biến tỉ lệ:  \n",
    "  $$\n",
    "  \\tilde{D}_t = \\frac{D(z_t)}{\\mathrm{mean}\\big( D(z_t)_{ij} : D(z_t)_{ij}>0 \\big)}, \\quad\n",
    "  \\tilde{D}_s = \\frac{D(z_s)}{\\mathrm{mean}\\big( D(z_s)_{ij} : D(z_s)_{ij}>0 \\big)}.\n",
    "  $$\n",
    "- Mất mát khoảng cách:  \n",
    "  $$\n",
    "  \\mathcal{L}_{\\mathrm{dist}} = \\mathrm{SmoothL1}(\\tilde{D}_s, \\tilde{D}_t).\n",
    "  $$\n",
    "\n",
    "2) Góc bộ ba (triplet angle):\n",
    "- Với mọi $i, j, k$:  \n",
    "  $$\n",
    "  v_{ij} = z_j - z_i,\\; v_{ik} = z_k - z_i,\\; \\cos\\angle(jik) = \\frac{v_{ij}^\\top v_{ik}}{\\|v_{ij}\\|\\,\\|v_{ik}\\|}.\n",
    "  $$\n",
    "- Tập hợp vào tensor $A(z)$ với $A_{i,j,k} = \\cos(\\angle jik)$.  \n",
    "  $$\n",
    "  \\mathcal{L}_{\\mathrm{angle}} = \\mathrm{SmoothL1}\\big( A(z_s), A(z_t) \\big).\n",
    "  $$\n",
    "\n",
    "Tổng mất mát:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s^{\\text{logit}}, y) + \\lambda_d\\, \\mathcal{L}_{\\mathrm{dist}} + \\lambda_a\\, \\mathcal{L}_{\\mathrm{angle}}.\n",
    "$$\n",
    "Trong đó $z_s^{\\text{logit}}$ là logits cho CE; $z_s, z_t$ cho phần RKD được lấy từ feature đã pool và chuẩn hoá. Cách này truyền cấu trúc hình học của không gian biểu diễn từ teacher sang student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba932168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T02:54:37.945810Z",
     "iopub.status.busy": "2025-09-16T02:54:37.945525Z",
     "iopub.status.idle": "2025-09-16T02:57:40.683675Z",
     "shell.execute_reply": "2025-09-16T02:57:40.682579Z"
    },
    "id": "55cb670f",
    "outputId": "04ce6059-0c1c-4436-fe81-35e7c6353938",
    "papermill": {
     "duration": 182.755493,
     "end_time": "2025-09-16T02:57:40.685116",
     "exception": false,
     "start_time": "2025-09-16T02:54:37.929623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_acc: 0.4824 - val_acc: 0.5318 (best)\n",
      "Epoch 2/10 - train_acc: 0.6400 - val_acc: 0.6478 (best)\n",
      "Epoch 3/10 - train_acc: 0.6944 - val_acc: 0.6522 (best)\n",
      "Epoch 4/10 - train_acc: 0.7362 - val_acc: 0.7058 (best)\n",
      "Epoch 5/10 - train_acc: 0.7631 - val_acc: 0.7214 (best)\n",
      "Epoch 6/10 - train_acc: 0.7845 - val_acc: 0.7882 (best)\n",
      "Epoch 7/10 - train_acc: 0.8052 - val_acc: 0.7894 (best)\n",
      "Epoch 8/10 - train_acc: 0.8208 - val_acc: 0.7898 (best)\n",
      "Epoch 9/10 - train_acc: 0.8358 - val_acc: 0.8302 (best)\n",
      "Epoch 10/10 - train_acc: 0.8471 - val_acc: 0.8410 (best)\n",
      "Saved student checkpoint (v7 RKD) to ./checkpoints/kd_student_v7.pth\n",
      "{'method': 'Relational Knowledge Distillation (RKD)', 'train_time_sec': 172.05, 'train_acc': 0.8577, 'val_acc': 0.841, 'test_acc': 0.8392}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Relational Knowledge Distillation (RKD)\n",
    "# Reference: Park et al., CVPR 2019. We implement RKD with both distance and angle losses.\n",
    "# Idea: match relational structures between samples (pairwise distances and triplet angles) in feature space.\n",
    "\n",
    "LR = 0.05\n",
    "W_RKD_DIST = 25.0\n",
    "W_RKD_ANGLE = 50.0\n",
    "\n",
    "# Student wrapper to expose features before global pooling\n",
    "class StudentWithFeatRKD(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)         # N, 128, H, W\n",
    "        logits = self.classifier(f)  # N, C\n",
    "        return logits, f\n",
    "\n",
    "student = StudentWithFeatRKD(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Teacher feature hook (layer4 output)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Helper: global-average pool to vectors and L2-normalize\n",
    "def to_vec_norm(fm: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # fm: N, C, H, W -> N, C\n",
    "    v = F.adaptive_avg_pool2d(fm, 1).flatten(1)\n",
    "    v = F.normalize(v, dim=1, eps=eps)\n",
    "    return v\n",
    "\n",
    "# RKD: Distance loss (pairwise)\n",
    "def rkd_distance(z_s: torch.Tensor, z_t: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # pairwise Euclidean distances\n",
    "    with torch.no_grad():\n",
    "        # teacher pairwise distances normalized by mean\n",
    "        d_t = torch.cdist(z_t, z_t, p=2)\n",
    "        mean_t = d_t[d_t>0].mean().clamp_min(eps)\n",
    "        d_t = d_t / mean_t\n",
    "    d_s = torch.cdist(z_s, z_s, p=2)\n",
    "    mean_s = d_s[d_s>0].mean().clamp_min(eps)\n",
    "    d_s = d_s / mean_s\n",
    "    return F.smooth_l1_loss(d_s, d_t)\n",
    "\n",
    "# RKD: Angle loss (triplet angles)\n",
    "def rkd_angle(z_s: torch.Tensor, z_t: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # For each anchor i, compute vectors to j and k: v_ij, v_ik, then their angle via cosine\n",
    "    def angle_matrix(z: torch.Tensor) -> torch.Tensor:\n",
    "        # z: N, D\n",
    "        n = z.size(0)\n",
    "        # compute pairwise differences: v_ij = z_j - z_i -> shape (N,N,D)\n",
    "        diff = z.unsqueeze(1) - z.unsqueeze(0)\n",
    "        # normalize along D\n",
    "        diff = F.normalize(diff, dim=2, eps=eps)\n",
    "        # cosine between v_ij and v_ik for all (j,k): cos = v_ij · v_ik\n",
    "        # angle tensor A where A[i,j,k] = cos(angle_jik)\n",
    "        A = torch.einsum('ijd,ikd->ijk', diff, diff)\n",
    "        return A\n",
    "    with torch.no_grad():\n",
    "        A_t = angle_matrix(z_t)\n",
    "    A_s = angle_matrix(z_s)\n",
    "    return F.smooth_l1_loss(A_s, A_t)\n",
    "\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            # CE term for classification\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            # RKD on pooled features\n",
    "            z_s = to_vec_norm(f_s)\n",
    "            z_t = to_vec_norm(f_t)\n",
    "            loss_dist = rkd_distance(z_s, z_t)\n",
    "            loss_ang = rkd_angle(z_s, z_t)\n",
    "            loss = loss_ce + W_RKD_DIST * loss_dist + W_RKD_ANGLE * loss_ang\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 7 (RKD)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v7.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v7 RKD) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Relational Knowledge Distillation (RKD)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5208952",
   "metadata": {
    "id": "Dx7AI67eYJwI",
    "papermill": {
     "duration": 0.014225,
     "end_time": "2025-09-16T02:57:40.714015",
     "exception": false,
     "start_time": "2025-09-16T02:57:40.699790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 8: Contrastive Representation Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2fad0",
   "metadata": {
    "papermill": {
     "duration": 0.013933,
     "end_time": "2025-09-16T02:57:40.741731",
     "exception": false,
     "start_time": "2025-09-16T02:57:40.727798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** CRD khớp biểu diễn thông qua mục tiêu tương phản (contrastive) kiểu InfoNCE giữa nhúng (embedding) của student và teacher.\n",
    "\n",
    "Giả sử có $N$ mẫu trong batch. Sau khi chiếu về cùng không gian bởi hai đầu chiếu học được $h_s(\\cdot), h_t(\\cdot)$:\n",
    "- $e_s = h_s(F_s) \\in \\mathbb{R}^{N\\times d}$, $e_t = h_t(F_t) \\in \\mathbb{R}^{N\\times d}$.\n",
    "- Chuẩn hoá $L_2$: $z_s = e_s/\\|e_s\\|$, $z_t = e_t/\\|e_t\\|$ (theo từng mẫu).\n",
    "\n",
    "Với mỗi $i$, ta coi $z_s[i]$ là query và $z_t[i]$ là positive; các $z_t[j]$ ($j\\neq i$) là negatives. Logits tương phản:\n",
    "$$\n",
    "\\ell_{i,j} = \\frac{ z_s[i]^\\top z_t[j] }{\\tau}, \\quad j=1,\\dots,N.\n",
    "$$\n",
    "Tổn thất InfoNCE trong-batch:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{CRD}} = \\frac{1}{N} \\sum_{i=1}^N \\mathrm{CE}\\big( \\ell_{i,:}, \\, j^*=i \\big).\n",
    "$$\n",
    "\n",
    "Tổng mất mát:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s^{\\text{logit}}, y) + \\lambda_{\\mathrm{CRD}}\\, \\mathcal{L}_{\\mathrm{CRD}}.\n",
    "$$\n",
    "- $\\tau$ là nhiệt độ điều chỉnh độ sắc của phân bố tương phản.\n",
    "- Mục tiêu: kéo cặp (student, teacher) của cùng mẫu lại gần, đẩy xa cặp khác mẫu, giúp student học không gian biểu diễn tương đồng teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72cb6949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T02:57:40.772258Z",
     "iopub.status.busy": "2025-09-16T02:57:40.771954Z",
     "iopub.status.idle": "2025-09-16T03:00:23.437789Z",
     "shell.execute_reply": "2025-09-16T03:00:23.436440Z"
    },
    "id": "zzaGwMl_UrqN",
    "outputId": "c046f154-df8a-4f47-fd2f-f2c633309eaa",
    "papermill": {
     "duration": 162.683498,
     "end_time": "2025-09-16T03:00:23.439433",
     "exception": false,
     "start_time": "2025-09-16T02:57:40.755935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_acc: 0.4197 - val_acc: 0.4888 (best)\n",
      "Epoch 2/10 - train_acc: 0.6045 - val_acc: 0.5758 (best)\n",
      "Epoch 3/10 - train_acc: 0.6753 - val_acc: 0.6390 (best)\n",
      "Epoch 4/10 - train_acc: 0.7229 - val_acc: 0.6842 (best)\n",
      "Epoch 5/10 - train_acc: 0.7567 - val_acc: 0.7490 (best)\n",
      "Epoch 6/10 - train_acc: 0.7822 - val_acc: 0.7540 (best)\n",
      "Epoch 7/10 - train_acc: 0.8062 - val_acc: 0.8048 (best)\n",
      "Epoch 8/10 - train_acc: 0.8258 - val_acc: 0.8236 (best)\n",
      "Epoch 9/10 - train_acc: 0.8440 - val_acc: 0.8430 (best)\n",
      "Epoch 10/10 - train_acc: 0.8518 - val_acc: 0.8522 (best)\n",
      "Saved student checkpoint (v8) to ./checkpoints/kd_student_v8.pth\n",
      "{'method': 'Contrastive Representation Distillation (CRD)', 'train_time_sec': 151.58, 'train_acc': 0.8645, 'val_acc': 0.8522, 'test_acc': 0.843}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Contrastive Representation Distillation (CRD, in-batch)\n",
    "# Ref: Tian et al., ICLR 2020. We implement a lightweight in-batch CRD:\n",
    "#   - Take penultimate conv features from student and teacher\n",
    "#   - Project to a shared embedding space with small MLP heads\n",
    "#   - Use InfoNCE with in-batch negatives (z_s vs z_t of all samples)\n",
    "#   - Optimize CE + W_CRD * CRD\n",
    "\n",
    "LR = 0.1\n",
    "W_CRD = 1.0\n",
    "TAU = 0.07   # temperature for contrastive logits\n",
    "EMB_DIM = 128\n",
    "\n",
    "# Student wrapper to expose features\n",
    "class StudentWithFeatCRD(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)         # N, 128, H, W\n",
    "        logits = self.classifier(f)  # N, C\n",
    "        return logits, f\n",
    "\n",
    "student = StudentWithFeatCRD(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Teacher feature hook (layer4)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Projection heads (trainable). Teacher backbone is frozen, but this head is trainable.\n",
    "proj_s = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128, EMB_DIM),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(EMB_DIM, EMB_DIM),\n",
    ").to(DEVICE)\n",
    "\n",
    "proj_t = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, EMB_DIM),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(EMB_DIM, EMB_DIM),\n",
    ").to(DEVICE)\n",
    "\n",
    "# Optimizer includes student + projection heads\n",
    "optimizer = optim.SGD(list(student.parameters()) + list(proj_s.parameters()) + list(proj_t.parameters()),\n",
    "                      lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# Contrastive loss (InfoNCE) using in-batch negatives\n",
    "def crd_loss(emb_s: torch.Tensor, emb_t: torch.Tensor, tau: float = 0.07) -> torch.Tensor:\n",
    "    # Normalize\n",
    "    zs = F.normalize(emb_s, dim=1)\n",
    "    zt = F.normalize(emb_t, dim=1)\n",
    "    # Similarity logits: N x N\n",
    "    logits = (zs @ zt.t()) / tau\n",
    "    targets = torch.arange(logits.size(0), device=logits.device)\n",
    "    return F.cross_entropy(logits, targets)\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train(); proj_s.train(); proj_t.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook\n",
    "            f_t = hook_t.feat  # N, 512, Ht, Wt\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)  # f_s: N, 128, Hs, Ws\n",
    "            # Project to embeddings (vector)\n",
    "            z_s = proj_s(f_s)\n",
    "            z_t = proj_t(f_t.detach())\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            loss_con = crd_loss(z_s, z_t, tau=TAU)\n",
    "            loss = loss_ce + W_CRD * loss_con\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    # Eval on validation using classifier head only\n",
    "    student.eval(); proj_s.eval(); proj_t.eval()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = {\n",
    "            'student': copy.deepcopy(student.state_dict()),\n",
    "            'proj_s': copy.deepcopy(proj_s.state_dict()),\n",
    "            'proj_t': copy.deepcopy(proj_t.state_dict()),\n",
    "        }\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state['student'])\n",
    "    proj_s.load_state_dict(best_state['proj_s'])\n",
    "    proj_t.load_state_dict(best_state['proj_t'])\n",
    "\n",
    "# Save student checkpoint for method 8\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v8.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v8) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Contrastive Representation Distillation (CRD)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc571785",
   "metadata": {
    "papermill": {
     "duration": 0.014346,
     "end_time": "2025-09-16T03:00:23.469744",
     "exception": false,
     "start_time": "2025-09-16T03:00:23.455398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 9: Probabilistic Knowledge Transfer (PKT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f903c7",
   "metadata": {
    "papermill": {
     "duration": 0.015795,
     "end_time": "2025-09-16T03:00:23.500314",
     "exception": false,
     "start_time": "2025-09-16T03:00:23.484519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ý tưởng:** PKT không khớp trực tiếp giá trị đặc trưng/logit mà khớp “quan hệ xác suất” giữa các mẫu trong cùng batch. Cụ thể, với biểu diễn (embedding) của mỗi mẫu, ta xây dựng phân phối xác suất tương tự (similarity distribution) từ mỗi mẫu đến các mẫu còn lại và buộc student mô phỏng phân phối của teacher. Cách này bảo tồn cấu trúc cục bộ của không gian biểu diễn và ít nhạy cảm với biến đổi biên độ/chuẩn hoá.\n",
    "\n",
    "Ký hiệu theo minibatch gồm $N$ mẫu, với embedding đã chuẩn hoá $L_2$:\n",
    "- $z_t = \\mathrm{norm}(e_t) \\in \\mathbb{R}^{N\\times d}$, $z_s = \\mathrm{norm}(e_s) \\in \\mathbb{R}^{N\\times d}$.\n",
    "- Hàm tương tự (similarity) dùng cosine: $\\mathrm{sim}(u, v) = u^\\top v$.\n",
    "- Với mỗi mẫu $i$, định nghĩa phân phối xác suất trên các mẫu khác bằng softmax nhiệt độ $\\sigma$ (khác với nhiệt độ KD trên logits):\n",
    "  $$\n",
    "  p_t(j\\,|\\,i) \\,=\\, \\frac{\\exp\\big(\\mathrm{sim}(z_t[i], z_t[j]) / \\sigma\\big)}{\\sum\\limits_{k\\ne i} \\exp\\big(\\mathrm{sim}(z_t[i], z_t[k]) / \\sigma\\big)},\\quad j\\ne i,\n",
    "  $$\n",
    "  và tương tự $p_s(j\\,|\\,i)$ từ $z_s$.\n",
    "\n",
    "Mất mát PKT là KL trung bình qua tất cả điều kiện $i$ (bỏ qua phần tử tự so sánh $j=i$):\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{PKT}} \\,=\\, \\frac{1}{N}\\sum_{i=1}^N \\mathrm{KL}\\big( p_t(\\cdot\\,|\\,i) \\,\\Vert\\, p_s(\\cdot\\,|\\,i) \\big).\n",
    "$$\n",
    "\n",
    "Tổng mất mát huấn luyện:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s^{\\text{logit}}, y) + \\lambda_{\\mathrm{PKT}}\\, \\mathcal{L}_{\\mathrm{PKT}}.\n",
    "$$\n",
    "- $\\sigma$ điều chỉnh độ sắc của phân phối tương tự; $\\lambda_{\\mathrm{PKT}}$ cân bằng với CE.\n",
    "- Khác RKD (dựa khoảng cách/góc), PKT dùng xác suất tương tự có chuẩn hoá theo từng gốc $i$, nên bền vững hơn với thay đổi tỉ lệ, và nhấn mạnh quan hệ cục bộ trong batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "264ab79c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T03:00:23.532374Z",
     "iopub.status.busy": "2025-09-16T03:00:23.532034Z",
     "iopub.status.idle": "2025-09-16T03:03:01.459266Z",
     "shell.execute_reply": "2025-09-16T03:03:01.458213Z"
    },
    "papermill": {
     "duration": 157.945269,
     "end_time": "2025-09-16T03:03:01.460599",
     "exception": false,
     "start_time": "2025-09-16T03:00:23.515330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_acc: 0.4301 - val_acc: 0.5292 (best)\n",
      "Epoch 2/10 - train_acc: 0.6020 - val_acc: 0.5908 (best)\n",
      "Epoch 3/10 - train_acc: 0.6672 - val_acc: 0.6258 (best)\n",
      "Epoch 4/10 - train_acc: 0.7121 - val_acc: 0.7066 (best)\n",
      "Epoch 5/10 - train_acc: 0.7436 - val_acc: 0.6634\n",
      "Epoch 6/10 - train_acc: 0.7680 - val_acc: 0.6818\n",
      "Epoch 7/10 - train_acc: 0.7897 - val_acc: 0.7814 (best)\n",
      "Epoch 8/10 - train_acc: 0.8155 - val_acc: 0.8016 (best)\n",
      "Epoch 9/10 - train_acc: 0.8326 - val_acc: 0.8162 (best)\n",
      "Epoch 10/10 - train_acc: 0.8486 - val_acc: 0.8538 (best)\n",
      "Saved student checkpoint (v9 PKT) to ./checkpoints/kd_student_v9.pth\n",
      "{'method': 'Probabilistic Knowledge Transfer (PKT)', 'train_time_sec': 146.62, 'train_acc': 0.8611, 'val_acc': 0.8538, 'test_acc': 0.8376}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Probabilistic Knowledge Transfer (PKT)\n",
    "LR = 0.1\n",
    "SIGMA_PKT = 0.1      # temperature for similarity softmax\n",
    "LAMBDA_PKT = 2.0     # weight for PKT loss\n",
    "\n",
    "# We need embeddings for teacher and student. We'll reuse the penultimate conv feature then GAP + linear head to produce logits.\n",
    "# Wrap SmallNet to expose an embedding vector per sample (after GAP, before final FC) and logits.\n",
    "class SmallNetWithEmbed(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)              # N,C,H,W (e.g., 128x4x4)\n",
    "        gap = F.adaptive_avg_pool2d(f, 1).flatten(1)  # N,C\n",
    "        logits = self.classifier(f)       # original classifier may use conv+pool inside\n",
    "        return logits, gap                # logits (for CE), gap as embedding e_s\n",
    "\n",
    "# Teacher embed: build a small head to map teacher features to a vector; fallback to logits space if needed.\n",
    "# Try to tap a common block (layer4) if exists; otherwise use logits as embedding.\n",
    "\n",
    "def build_teacher_embedder(teacher_model: nn.Module, out_dim: int = None):\n",
    "    # If teacher has layer4, hook it; else return a function that uses logits as embedding\n",
    "    layer = getattr(teacher_model, 'layer4', None)\n",
    "    if layer is None:\n",
    "        # Fallback: use logits as embedding\n",
    "        def teacher_embed(x):\n",
    "            logits = teacher_model(x)\n",
    "            e = logits if out_dim is None else F.linear(logits, torch.eye(logits.shape[1], device=logits.device))\n",
    "            return logits, e\n",
    "        return teacher_embed\n",
    "\n",
    "    class Hook:\n",
    "        def __init__(self, module):\n",
    "            self.feat = None\n",
    "            module.register_forward_hook(self._hook)\n",
    "        def _hook(self, m, inp, out):\n",
    "            self.feat = out\n",
    "    h = Hook(layer)\n",
    "\n",
    "    # simple projector after hook: GAP only to get vector\n",
    "    def teacher_embed(x):\n",
    "        logits = teacher_model(x)  # populates hook\n",
    "        if h.feat is None:\n",
    "            raise RuntimeError('Teacher feature hook not captured')\n",
    "        e = F.adaptive_avg_pool2d(h.feat, 1).flatten(1)\n",
    "        return logits, e\n",
    "\n",
    "    return teacher_embed\n",
    "\n",
    "\n",
    "def pkt_loss(e_s: torch.Tensor, e_t: torch.Tensor, sigma: float = 0.1, eps: float = 1e-12) -> torch.Tensor:\n",
    "    # L2-normalize embeddings\n",
    "    z_s = F.normalize(e_s, p=2, dim=1)  # N,d\n",
    "    z_t = F.normalize(e_t, p=2, dim=1)  # N,d\n",
    "\n",
    "    # Cosine similarity matrices (exclude self with mask later)\n",
    "    S_s = z_s @ z_s.t()   # N,N\n",
    "    S_t = z_t @ z_t.t()   # N,N\n",
    "\n",
    "    # Mask out diagonal (self-similarity) by setting to -inf before softmax\n",
    "    N = S_s.size(0)\n",
    "    mask = torch.eye(N, device=S_s.device).bool()\n",
    "    S_s = S_s.masked_fill(mask, float('-inf'))\n",
    "    S_t = S_t.masked_fill(mask, float('-inf'))\n",
    "\n",
    "    # Row-wise softmax with temperature sigma\n",
    "    P_s = F.softmax(S_s / sigma, dim=1).clamp_min(eps)  # N,N\n",
    "    P_t = F.softmax(S_t / sigma, dim=1).clamp_min(eps)  # N,N\n",
    "\n",
    "    # Row-wise KL: average over rows\n",
    "    loss = F.kl_div(P_s.log(), P_t, reduction='batchmean')\n",
    "    return loss\n",
    "\n",
    "\n",
    "student = SmallNetWithEmbed(NUM_CLASSES).to(DEVICE)\n",
    "teacher_embed = build_teacher_embedder(teacher)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t, e_t = teacher_embed(x)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, e_s = student(x)\n",
    "            loss_ce  = F.cross_entropy(logits_s, y)\n",
    "            loss_pkt = pkt_loss(e_s, e_t, sigma=SIGMA_PKT)\n",
    "            loss = loss_ce + LAMBDA_PKT * loss_pkt\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 9 (PKT)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v9.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v9 PKT) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Probabilistic Knowledge Transfer (PKT)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c40d0",
   "metadata": {
    "papermill": {
     "duration": 0.015971,
     "end_time": "2025-09-16T03:03:01.492705",
     "exception": false,
     "start_time": "2025-09-16T03:03:01.476734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Đánh giá toàn diện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3141719c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T03:03:01.526527Z",
     "iopub.status.busy": "2025-09-16T03:03:01.526202Z",
     "iopub.status.idle": "2025-09-16T03:03:21.591078Z",
     "shell.execute_reply": "2025-09-16T03:03:21.589901Z"
    },
    "papermill": {
     "duration": 20.083999,
     "end_time": "2025-09-16T03:03:21.592556",
     "exception": false,
     "start_time": "2025-09-16T03:03:01.508557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Bảng kết quả (metrics) =====\n",
      "     model    acc   loss    ece  agree_t  kl_to_t  cos_logits  entropy\n",
      "   teacher 0.9444 0.2141 0.0318      NaN      NaN         NaN   0.0697\n",
      "v1_vanilla 0.8413 0.5419 0.0651   0.8533   0.4258      0.8436   0.2650\n",
      "    v2_cee 0.8363 0.4892 0.0148   0.8482   0.3921      0.7720   0.4351\n",
      "   v3_feat 0.8349 0.4764 0.0090   0.8458   0.3872      0.7722   0.4488\n",
      "     v4_at 0.8073 0.5509 0.0097   0.8195   0.4582      0.7437   0.5392\n",
      "  v5_logit 0.8385 0.5159 0.0572   0.8520   0.4016      0.8748   0.3075\n",
      "    v6_fkd 0.8275 0.5258 0.0435   0.8398   0.4238      0.8220   0.3720\n",
      "    v7_rkd 0.8392 0.4861 0.0165   0.8491   0.3986      0.8354   0.4830\n",
      "    v8_crd 0.8430 0.4591 0.0063   0.8541   0.3654      0.7805   0.4537\n",
      "    v9_pkt 0.8376 0.4699 0.0071   0.8449   0.3875      0.8224   0.4987\n",
      "\n",
      "===== Bảng xếp hạng (rank) =====\n",
      "     model  acc_rank  loss_rank  ece_rank  agree_t_rank  kl_to_t_rank  cos_logits_rank  entropy_rank  avg_rank\n",
      "   teacher       1.0        1.0       7.0          10.0          10.0             10.0           1.0  5.714286\n",
      "v1_vanilla       3.0        9.0      10.0           2.0           8.0              2.0           2.0  5.142857\n",
      "    v2_cee       7.0        6.0       5.0           5.0           4.0              8.0           5.0  5.714286\n",
      "   v3_feat       8.0        4.0       3.0           6.0           2.0              7.0           6.0  5.142857\n",
      "     v4_at      10.0       10.0       4.0           9.0           9.0              9.0          10.0  8.714286\n",
      "  v5_logit       5.0        7.0       9.0           3.0           6.0              1.0           3.0  4.857143\n",
      "    v6_fkd       9.0        8.0       8.0           8.0           7.0              5.0           4.0  7.000000\n",
      "    v7_rkd       4.0        5.0       6.0           4.0           5.0              3.0           8.0  5.000000\n",
      "    v8_crd       2.0        2.0       1.0           1.0           1.0              6.0           7.0  2.857143\n",
      "    v9_pkt       6.0        3.0       2.0           7.0           3.0              4.0           9.0  4.857143\n"
     ]
    }
   ],
   "source": [
    "# %% Comprehensive Evaluation: Load checkpoints and compare models\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Expect checkpoints in CKPT_DIR\n",
    "ckpt_dir = CKPT_DIR if 'CKPT_DIR' in globals() else './checkpoints'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# Build teacher & student architectures (same as training)\n",
    "teacher_eval = build_teacher(NUM_CLASSES).to(DEVICE)\n",
    "student_eval = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Utility: evaluate metrics over a dataloader\n",
    "@torch.no_grad()\n",
    "def collect_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    logits_list, labels_list = [], []\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        out = model(x)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "        loss_sum += criterion(logits, y).item()\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        logits_list.append(logits.detach().cpu())\n",
    "        labels_list.append(y.detach().cpu())\n",
    "    logits_all = torch.cat(logits_list, dim=0)\n",
    "    labels_all = torch.cat(labels_list, dim=0)\n",
    "    acc = correct / max(1, total)\n",
    "    avg_loss = loss_sum / max(1, total)\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'loss': avg_loss,\n",
    "        'logits': logits_all,\n",
    "        'labels': labels_all,\n",
    "    }\n",
    "\n",
    "# Expected checkpoints\n",
    "models = OrderedDict([\n",
    "    (\"teacher\", os.path.join(ckpt_dir, \"kd_teacher.pth\")),\n",
    "    (\"v1_vanilla\", os.path.join(ckpt_dir, \"kd_student_v1.pth\")),\n",
    "    (\"v2_cee\", os.path.join(ckpt_dir, \"kd_student_v2.pth\")),\n",
    "    (\"v3_feat\", os.path.join(ckpt_dir, \"kd_student_v3.pth\")),\n",
    "    (\"v4_at\", os.path.join(ckpt_dir, \"kd_student_v4.pth\")),\n",
    "    (\"v5_logit\", os.path.join(ckpt_dir, \"kd_student_v5.pth\")),\n",
    "    (\"v6_fkd\", os.path.join(ckpt_dir, \"kd_student_v6.pth\")),\n",
    "    (\"v7_rkd\", os.path.join(ckpt_dir, \"kd_student_v7.pth\")),\n",
    "    (\"v8_crd\", os.path.join(ckpt_dir, \"kd_student_v8.pth\")),\n",
    "    (\"v9_pkt\", os.path.join(ckpt_dir, \"kd_student_v9.pth\")),\n",
    "])\n",
    "\n",
    "# Load teacher\n",
    "loaded = {}\n",
    "if os.path.isfile(models['teacher']):\n",
    "    teacher_eval.load_state_dict(torch.load(models['teacher'], map_location=DEVICE))\n",
    "    loaded['teacher'] = teacher_eval\n",
    "else:\n",
    "    print(f\"[WARN] Teacher checkpoint not found: {models['teacher']}\")\n",
    "\n",
    "# Load students into dict (same arch SmallNet)\n",
    "for name, path in models.items():\n",
    "    if name == 'teacher':\n",
    "        continue\n",
    "    if os.path.isfile(path):\n",
    "        m = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "        m.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "        loaded[name] = m\n",
    "    else:\n",
    "        print(f\"[WARN] Student checkpoint not found: {path}\")\n",
    "\n",
    "# Metrics to compute\n",
    "# - acc_test: accuracy on test set\n",
    "# - loss_test: CE loss on test set\n",
    "# - ece: Expected Calibration Error (10-bin)\n",
    "# - agree_t: agreement rate between student and teacher predictions\n",
    "# - kl_to_t: KL(student || teacher) on test logits (softmax distributions)\n",
    "# - cos_logits: cosine similarity between student and teacher logits\n",
    "# - entropy: average predictive entropy (uncertainty)\n",
    "\n",
    "\n",
    "def expected_calibration_error(probs: torch.Tensor, labels: torch.Tensor, n_bins: int = 10) -> float:\n",
    "    # probs: N,C ; labels: N\n",
    "    confidences, predictions = probs.max(dim=1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "    bins = torch.linspace(0, 1, steps=n_bins + 1)\n",
    "    ece = torch.zeros(1)\n",
    "    for i in range(n_bins):\n",
    "        in_bin = (confidences > bins[i]) & (confidences <= bins[i + 1]) if i < n_bins - 1 else (confidences > bins[i]) & (confidences <= bins[i + 1])\n",
    "        prop = in_bin.float().mean()\n",
    "        if prop.item() > 0:\n",
    "            acc_bin = accuracies[in_bin].float().mean()\n",
    "            conf_bin = confidences[in_bin].float().mean()\n",
    "            ece += torch.abs(conf_bin - acc_bin) * prop\n",
    "    return ece.item()\n",
    "\n",
    "\n",
    "def kl_divergence(p_logits: torch.Tensor, q_logits: torch.Tensor, T: float = 1.0) -> float:\n",
    "    # KL(P||Q) with temperature T\n",
    "    p = F.log_softmax(p_logits / T, dim=1)\n",
    "    q = F.softmax(q_logits / T, dim=1)\n",
    "    return F.kl_div(p, q, reduction='batchmean').item()\n",
    "\n",
    "\n",
    "def cosine_similarity_logits(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    a_flat = a.flatten(1)\n",
    "    b_flat = b.flatten(1)\n",
    "    a_n = F.normalize(a_flat, p=2, dim=1)\n",
    "    b_n = F.normalize(b_flat, p=2, dim=1)\n",
    "    return (a_n * b_n).sum(dim=1).mean().item()\n",
    "\n",
    "\n",
    "def avg_entropy_from_logits(logits: torch.Tensor) -> float:\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    entropy = -(probs * (probs.clamp_min(1e-12).log())).sum(dim=1)\n",
    "    return entropy.mean().item()\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "metrics = [\n",
    "    'acc', 'loss', 'ece', 'agree_t', 'kl_to_t', 'cos_logits', 'entropy'\n",
    "]\n",
    "\n",
    "# First collect teacher outputs\n",
    "teacher_out = None\n",
    "if 'teacher' in loaded:\n",
    "    teacher_out = collect_metrics(loaded['teacher'], test_loader, DEVICE)\n",
    "\n",
    "for name, model in loaded.items():\n",
    "    out = collect_metrics(model, test_loader, DEVICE)\n",
    "    probs = F.softmax(out['logits'], dim=1)\n",
    "    ece = expected_calibration_error(probs, out['labels'])\n",
    "\n",
    "    # Comparisons to teacher (only if teacher available and current is not teacher)\n",
    "    if teacher_out is not None and name != 'teacher':\n",
    "        agree = (out['logits'].argmax(1) == teacher_out['logits'].argmax(1)).float().mean().item()\n",
    "        kl = kl_divergence(out['logits'], teacher_out['logits'])\n",
    "        cos = cosine_similarity_logits(out['logits'], teacher_out['logits'])\n",
    "    else:\n",
    "        agree, kl, cos = np.nan, np.nan, np.nan\n",
    "\n",
    "    ent = avg_entropy_from_logits(out['logits'])\n",
    "\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'acc': round(out['acc'], 4),\n",
    "        'loss': round(out['loss'], 4),\n",
    "        'ece': round(ece, 4),\n",
    "        'agree_t': round(agree, 4) if not np.isnan(agree) else np.nan,\n",
    "        'kl_to_t': round(kl, 4) if not np.isnan(kl) else np.nan,\n",
    "        'cos_logits': round(cos, 4) if not np.isnan(cos) else np.nan,\n",
    "        'entropy': round(ent, 4),\n",
    "    })\n",
    "\n",
    "# Build results DataFrame (sorted by model order above)\n",
    "df = pd.DataFrame(results)\n",
    "# Optional: reorder rows to keep teacher first\n",
    "order = [k for k in models.keys() if k in df['model'].values]\n",
    "df['order_idx'] = df['model'].apply(lambda m: order.index(m) if m in order else 999)\n",
    "df = df.sort_values('order_idx').drop(columns=['order_idx']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n===== Bảng kết quả (metrics) =====\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Ranking table: for each metric, compute rank (best rank = 1).\n",
    "# For metrics where lower is better (loss, ece, kl, entropy), rank ascending. For higher-better (acc, agree_t, cos), rank descending.\n",
    "rank_prefs = {\n",
    "    'acc': 'desc',\n",
    "    'loss': 'asc',\n",
    "    'ece': 'asc',\n",
    "    'agree_t': 'desc',\n",
    "    'kl_to_t': 'asc',\n",
    "    'cos_logits': 'desc',\n",
    "    'entropy': 'asc',\n",
    "}\n",
    "\n",
    "rank_df = df.copy()\n",
    "for col, pref in rank_prefs.items():\n",
    "    series = rank_df[col]\n",
    "    if series.isna().all():\n",
    "        rank_df[col + '_rank'] = np.nan\n",
    "        continue\n",
    "    # For NaN values (e.g., teacher comparisons), assign worst rank\n",
    "    fill_val = series.max() + 1 if pref == 'asc' else series.min() - 1\n",
    "    series_filled = series.fillna(fill_val)\n",
    "    ascending = (pref == 'asc')\n",
    "    rank_df[col + '_rank'] = series_filled.rank(method='min', ascending=ascending)\n",
    "\n",
    "# Keep only rank columns and model name\n",
    "rank_cols = ['model'] + [c + '_rank' for c in rank_prefs.keys()]\n",
    "rank_table = rank_df[rank_cols]\n",
    "rank_table['avg_rank'] = rank_table[[c for c in rank_cols if c != 'model']].mean(axis=1)\n",
    "\n",
    "print(\"\\n===== Bảng xếp hạng (rank) =====\")\n",
    "print(rank_table.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3859.822261,
   "end_time": "2025-09-16T03:03:23.972186",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-16T01:59:04.149925",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
