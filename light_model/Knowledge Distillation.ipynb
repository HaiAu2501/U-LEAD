{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f61ce93",
   "metadata": {
    "id": "e1f2a85b",
    "papermill": {
     "duration": 0.004653,
     "end_time": "2025-09-13T15:35:04.399601",
     "exception": false,
     "start_time": "2025-09-13T15:35:04.394948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Knowledge Distillation on CIFAR-10\n",
    "\n",
    "Mục tiêu: Cài đặt nhiều phương pháp Knowledge Distillation (KD) khác nhau cho bài toán phân loại CIFAR-10.\n",
    "\n",
    "Yêu cầu chính:\n",
    "- Dataset: CIFAR-10 (train/test chuẩn của torchvision)\n",
    "- Model teacher: pretrained trên CIFAR-10; model student nhỏ hơn, chưa train.\n",
    "- Một biến chung `KD_EPOCHS` xác định số epoch train cho TẤT CẢ phương pháp KD.\n",
    "- Trước mỗi phương pháp có một cell markdown ghi tên phương pháp.\n",
    "- Mỗi phương pháp in ra: tổng thời gian train + accuracy trên train và test.\n",
    "\n",
    "Các phần dưới đây cung cấp phần setup dùng chung (dataloader, model, util, teacher), sau đó là từng phương pháp KD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edab35ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T15:35:04.408950Z",
     "iopub.status.busy": "2025-09-13T15:35:04.408669Z",
     "iopub.status.idle": "2025-09-13T15:46:34.197664Z",
     "shell.execute_reply": "2025-09-13T15:46:34.196713Z"
    },
    "id": "3d503ed3",
    "outputId": "7894f04e-bf6d-4fe1-e465-76c1ae67d25e",
    "papermill": {
     "duration": 689.79546,
     "end_time": "2025-09-13T15:46:34.198970",
     "exception": false,
     "start_time": "2025-09-13T15:35:04.403510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:04<00:00, 34.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher checkpoint not found. Training ResNet18 teacher from scratch on CIFAR-10 (select best by val acc).\n",
      "Teacher epoch 1/30 - train_acc: 0.2888 - val_acc: 0.4118 (saved best)\n",
      "Teacher epoch 2/30 - train_acc: 0.4345 - val_acc: 0.4606 (saved best)\n",
      "Teacher epoch 3/30 - train_acc: 0.5418 - val_acc: 0.5120 (saved best)\n",
      "Teacher epoch 4/30 - train_acc: 0.6231 - val_acc: 0.5976 (saved best)\n",
      "Teacher epoch 5/30 - train_acc: 0.6777 - val_acc: 0.6850 (saved best)\n",
      "Teacher epoch 6/30 - train_acc: 0.7232 - val_acc: 0.7180 (saved best)\n",
      "Teacher epoch 7/30 - train_acc: 0.7548 - val_acc: 0.7422 (saved best)\n",
      "Teacher epoch 8/30 - train_acc: 0.7722 - val_acc: 0.7016\n",
      "Teacher epoch 9/30 - train_acc: 0.7862 - val_acc: 0.7774 (saved best)\n",
      "Teacher epoch 10/30 - train_acc: 0.7995 - val_acc: 0.7924 (saved best)\n",
      "Teacher epoch 11/30 - train_acc: 0.8088 - val_acc: 0.7354\n",
      "Teacher epoch 12/30 - train_acc: 0.8199 - val_acc: 0.8038 (saved best)\n",
      "Teacher epoch 13/30 - train_acc: 0.8260 - val_acc: 0.7918\n",
      "Teacher epoch 14/30 - train_acc: 0.8343 - val_acc: 0.7996\n",
      "Teacher epoch 15/30 - train_acc: 0.8481 - val_acc: 0.8360 (saved best)\n",
      "Teacher epoch 16/30 - train_acc: 0.8558 - val_acc: 0.8076\n",
      "Teacher epoch 17/30 - train_acc: 0.8625 - val_acc: 0.8648 (saved best)\n",
      "Teacher epoch 18/30 - train_acc: 0.8727 - val_acc: 0.8448\n",
      "Teacher epoch 19/30 - train_acc: 0.8841 - val_acc: 0.8782 (saved best)\n",
      "Teacher epoch 20/30 - train_acc: 0.8926 - val_acc: 0.8730\n",
      "Teacher epoch 21/30 - train_acc: 0.9052 - val_acc: 0.8604\n",
      "Teacher epoch 22/30 - train_acc: 0.9148 - val_acc: 0.8962 (saved best)\n",
      "Teacher epoch 23/30 - train_acc: 0.9267 - val_acc: 0.9042 (saved best)\n",
      "Teacher epoch 24/30 - train_acc: 0.9360 - val_acc: 0.9064 (saved best)\n",
      "Teacher epoch 25/30 - train_acc: 0.9503 - val_acc: 0.9210 (saved best)\n",
      "Teacher epoch 26/30 - train_acc: 0.9624 - val_acc: 0.9264 (saved best)\n",
      "Teacher epoch 27/30 - train_acc: 0.9716 - val_acc: 0.9286 (saved best)\n",
      "Teacher epoch 28/30 - train_acc: 0.9788 - val_acc: 0.9348 (saved best)\n",
      "Teacher epoch 29/30 - train_acc: 0.9836 - val_acc: 0.9350 (saved best)\n",
      "Teacher epoch 30/30 - train_acc: 0.9857 - val_acc: 0.9366 (saved best)\n",
      "Saved best teacher to ./checkpoints/kd_teacher.pth. Best val_acc: 0.9366. Training time: 669.1s\n",
      "Teacher test acc: 0.9306\n",
      "Saved teacher checkpoint to ./checkpoints/kd_teacher.pth\n",
      "KD_EPOCHS = 30\n"
     ]
    }
   ],
   "source": [
    "# %% Shared Setup: dependencies, config, data, models, utils\n",
    "import os, time, math, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import copy\n",
    "\n",
    "# Reproducibility\n",
    "SEED = int(os.environ.get(\"SEED\", 42))\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Global epochs shared by all KD methods\n",
    "KD_EPOCHS = int(os.environ.get(\"KD_EPOCHS\", 30))  # chỉnh tại đây nếu muốn\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "NUM_CLASSES = 10\n",
    "VAL_RATIO = float(os.environ.get(\"VAL_RATIO\", 0.1))  # 10% train -> val split\n",
    "\n",
    "# Teacher training epochs if training from scratch\n",
    "TEACHER_EPOCHS = int(os.environ.get(\"TEACHER_EPOCHS\", 30))\n",
    "\n",
    "# Ensure checkpoints directory exists\n",
    "CKPT_DIR = os.environ.get(\"CKPT_DIR\", \"./checkpoints\")\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# Data: CIFAR-10\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "train_tf = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# Build full train set twice to allow different transforms for train vs validation\n",
    "train_full_aug = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "train_full_plain = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=False, transform=test_tf)\n",
    "\n",
    "# Create reproducible train/val split\n",
    "N = len(train_full_aug)\n",
    "val_size = max(1, int(VAL_RATIO * N))\n",
    "train_size = N - val_size\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "perm = torch.randperm(N, generator=gen)\n",
    "val_idx = perm[:val_size].tolist()\n",
    "train_idx = perm[val_size:].tolist()\n",
    "\n",
    "train_set = Subset(train_full_aug, train_idx)\n",
    "val_set = Subset(train_full_plain, val_idx)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Student model (smaller than ResNet18)\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Teacher model: minimal ResNet18 for CIFAR-10 (3x3 stem, no initial maxpool)\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # layers: 2,2,2,2\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * BasicBlock.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_teacher(num_classes=10):\n",
    "    # Build a CIFAR-10 style ResNet18 from scratch\n",
    "    return ResNet18(num_classes)\n",
    "\n",
    "# Point teacher checkpoint to checkpoints/kd_teacher.pth\n",
    "TEACHER_CKPT = os.path.join(CKPT_DIR, \"kd_teacher.pth\")\n",
    "\n",
    "# Train/eval utilities\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        out = model(x)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "        loss = criterion(logits, y)\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    acc = correct / total\n",
    "    avg_loss = loss_sum / total\n",
    "    return acc, avg_loss\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    train_acc: float\n",
    "    test_acc: float\n",
    "    train_time_sec: float\n",
    "\n",
    "\n",
    "def top1_acc(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "\n",
    "def train_ce(model: nn.Module, loader: DataLoader, optimizer, device: torch.device, scaler: Optional[GradScaler] = None):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                out = model(x)\n",
    "                logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "                loss = criterion(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model(x)\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    return total_correct / max(1, total_samples)\n",
    "\n",
    "def flatten_features(x):\n",
    "    return torch.flatten(x, 1)\n",
    "\n",
    "# Hooks to grab intermediate features (for feature distillation methods)\n",
    "class FeatureHook:\n",
    "    def __init__(self, module: nn.Module):\n",
    "        self.feat = None\n",
    "        module.register_forward_hook(self.hook)\n",
    "    def hook(self, module, input, output):\n",
    "        self.feat = output\n",
    "\n",
    "# Teacher build/load\n",
    "teacher = build_teacher(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "TEACHER_OPT_LR = float(os.environ.get(\"TEACHER_OPT_LR\", 0.1))\n",
    "TEACHER_WD = float(os.environ.get(\"TEACHER_WD\", 5e-4))\n",
    "\n",
    "if os.path.isfile(TEACHER_CKPT):\n",
    "    teacher.load_state_dict(torch.load(TEACHER_CKPT, map_location=DEVICE))\n",
    "    print(f\"Loaded teacher weights from {TEACHER_CKPT}\")\n",
    "else:\n",
    "    print(\"Teacher checkpoint not found. Training ResNet18 teacher from scratch on CIFAR-10 (select best by val acc).\")\n",
    "    optimizer_t = optim.SGD(teacher.parameters(), lr=TEACHER_OPT_LR, momentum=0.9, weight_decay=TEACHER_WD)\n",
    "    scheduler_t = optim.lr_scheduler.CosineAnnealingLR(optimizer_t, T_max=TEACHER_EPOCHS)\n",
    "    scaler_t = GradScaler(enabled=torch.cuda.is_available())\n",
    "    best_val, best_state = 0.0, None\n",
    "    start_t = time.time()\n",
    "    for e in range(TEACHER_EPOCHS):\n",
    "        acc_train = train_ce(teacher, train_loader, optimizer_t, DEVICE, scaler=scaler_t)\n",
    "        acc_val, _ = evaluate(teacher, val_loader, DEVICE)\n",
    "        if acc_val > best_val:\n",
    "            best_val = acc_val\n",
    "            best_state = copy.deepcopy(teacher.state_dict())\n",
    "            torch.save(best_state, TEACHER_CKPT)\n",
    "            tag = \" (saved best)\"\n",
    "        else:\n",
    "            tag = \"\"\n",
    "        scheduler_t.step()\n",
    "        print(f\"Teacher epoch {e+1}/{TEACHER_EPOCHS} - train_acc: {acc_train:.4f} - val_acc: {acc_val:.4f}{tag}\")\n",
    "    elapsed_t = time.time() - start_t\n",
    "    if best_state is not None:\n",
    "        teacher.load_state_dict(best_state)\n",
    "    print(f\"Saved best teacher to {TEACHER_CKPT}. Best val_acc: {best_val:.4f}. Training time: {elapsed_t:.1f}s\")\n",
    "\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "teacher.eval()\n",
    "\n",
    "acc_test, _ = evaluate(teacher, test_loader, DEVICE)\n",
    "print(f\"Teacher test acc: {acc_test:.4f}\")\n",
    "\n",
    "# Save teacher checkpoint into checkpoints directory (always ensure it exists)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "teacher_ckpt_path = TEACHER_CKPT\n",
    "torch.save(teacher.state_dict(), teacher_ckpt_path)\n",
    "print(f\"Saved teacher checkpoint to {teacher_ckpt_path}\")\n",
    "\n",
    "print(\"KD_EPOCHS =\", KD_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81538a0",
   "metadata": {
    "id": "795b9274",
    "papermill": {
     "duration": 0.006866,
     "end_time": "2025-09-13T15:46:34.212870",
     "exception": false,
     "start_time": "2025-09-13T15:46:34.206004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## So sánh mô hình teacher và student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae529c7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T15:46:34.228112Z",
     "iopub.status.busy": "2025-09-13T15:46:34.227572Z",
     "iopub.status.idle": "2025-09-13T15:46:35.229532Z",
     "shell.execute_reply": "2025-09-13T15:46:35.228665Z"
    },
    "id": "eefe070d",
    "outputId": "ae9ec1ef-2829-4629-b0f5-d3da32cc2207",
    "papermill": {
     "duration": 1.011163,
     "end_time": "2025-09-13T15:46:35.230752",
     "exception": false,
     "start_time": "2025-09-13T15:46:34.219589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So sánh mô hình teacher và student (đầu vào 32x32):\n",
      "\n",
      "[Teacher (ResNet18)]\n",
      "  Params: 11173962 (trainable: 0) -> 11.17M params\n",
      "  Layers (Conv+FC): 21\n",
      "  MACs (32x32): 555.42M  |  FLOPs~ 1.11B\n",
      "  Latency: ~0.161 ms / image (batch=128, avg over repeats)\n",
      "\n",
      "[Student (SmallNet)]\n",
      "  Params: 141354 (trainable: 141354) -> 141.35K params\n",
      "  Layers (Conv+FC): 6\n",
      "  MACs (32x32): 29.20M  |  FLOPs~ 58.40M\n",
      "  Latency: ~0.016 ms / image (batch=128, avg over repeats)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  # %% So sánh teacher vs student: tham số, layer, FLOPs/MACs, Latency\n",
    "from collections import defaultdict\n",
    "\n",
    "# Choose the canonical student used by most methods\n",
    "student_ref = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "def count_params(m: nn.Module):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "def count_layers(m: nn.Module):\n",
    "    # count conv + linear layers as \"layers\"\n",
    "    return sum(isinstance(mod, (nn.Conv2d, nn.Linear)) for mod in m.modules())\n",
    "\n",
    "\n",
    "# Lightweight FLOPs/MACs estimation using hooks (counts MACs ~ multiply-adds)\n",
    "def estimate_macs(model: nn.Module, input_size=(1, 3, 32, 32)):\n",
    "    macs = 0\n",
    "\n",
    "    def conv_hook(self, inp, out):\n",
    "        nonlocal macs\n",
    "        # inp[0]: N,Cin,Hin,Win ; out: N,Cout,Hout,Wout\n",
    "        x = inp[0]\n",
    "        N, Cin, Hin, Win = x.shape\n",
    "        Cout, Hout, Wout = out.shape[1:]\n",
    "        kH, kW = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
    "        # MACs per output element: Cin * kH * kW\n",
    "        macs += N * Cout * Hout * Wout * Cin * kH * kW\n",
    "\n",
    "    def linear_hook(self, inp, out):\n",
    "        nonlocal macs\n",
    "        # inp[0]: N, in_features ; out: N, out_features\n",
    "        N, in_f = inp[0].shape\n",
    "        out_f = out.shape[1]\n",
    "        macs += N * in_f * out_f\n",
    "\n",
    "    hooks = []\n",
    "    for mod in model.modules():\n",
    "        if isinstance(mod, nn.Conv2d):\n",
    "            hooks.append(mod.register_forward_hook(conv_hook))\n",
    "        elif isinstance(mod, nn.Linear):\n",
    "            hooks.append(mod.register_forward_hook(linear_hook))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(*input_size, device=DEVICE)\n",
    "        _ = model(dummy)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # FLOPs ~ 2 * MACs if counting MUL+ADD as two ops. Report both.\n",
    "    flops = 2 * macs\n",
    "    return macs, flops\n",
    "\n",
    "\n",
    "def pretty(n):\n",
    "    # format large numbers with units\n",
    "    for unit in [\"\", \"K\", \"M\", \"B\", \"T\"]:\n",
    "        if abs(n) < 1000:\n",
    "            return f\"{n:.2f}{unit}\"\n",
    "        n /= 1000\n",
    "    return f\"{n:.2f}P\"\n",
    "\n",
    "\n",
    "def measure_latency_ms_per_image(model: nn.Module, batch_size: int = 128, repeats: int = 30, warmup: int = 10):\n",
    "    model.eval()\n",
    "    x = torch.randn(batch_size, 3, 32, 32, device=DEVICE)\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    # Timed runs\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeats):\n",
    "            t0 = time.perf_counter()\n",
    "            _ = model(x)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            t1 = time.perf_counter()\n",
    "            times.append((t1 - t0))\n",
    "    avg_s = sum(times) / len(times)\n",
    "    ms_per_image = (avg_s / batch_size) * 1000.0\n",
    "    return ms_per_image\n",
    "\n",
    "\n",
    "def report_model_stats(name: str, m: nn.Module):\n",
    "    total, trainable = count_params(m)\n",
    "    layers = count_layers(m)\n",
    "    macs, flops = estimate_macs(m)\n",
    "    latency = measure_latency_ms_per_image(m)\n",
    "    print(\n",
    "        f\"[{name}]\\n\"\n",
    "        f\"  Params: {total} (trainable: {trainable}) -> {pretty(total)} params\\n\"\n",
    "        f\"  Layers (Conv+FC): {layers}\\n\"\n",
    "        f\"  MACs (32x32): {pretty(macs)}  |  FLOPs~ {pretty(flops)}\\n\"\n",
    "        f\"  Latency: ~{latency:.3f} ms / image (batch=128, avg over repeats)\\n\"\n",
    "    )\n",
    "\n",
    "print(\"So sánh mô hình teacher và student (đầu vào 32x32):\\n\")\n",
    "report_model_stats(\"Teacher (ResNet18)\", teacher)\n",
    "report_model_stats(\"Student (SmallNet)\", student_ref)\n",
    "\n",
    "# Cleanup\n",
    "del student_ref\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a84662",
   "metadata": {
    "id": "81a6e5cb",
    "papermill": {
     "duration": 0.006628,
     "end_time": "2025-09-13T15:46:35.244527",
     "exception": false,
     "start_time": "2025-09-13T15:46:35.237899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 1: Vanilla KD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2a306",
   "metadata": {},
   "source": [
    "**Ý tưởng:** Để student học theo \"soft targets\" của  thay vì chỉ dựa vào nhãn cứng. Soft targets giữ lại thông tin về độ tự tin giữa các lớp.\n",
    "\n",
    "Ký hiệu cho một mẫu $(x, y)$:\n",
    "- $z_t$ = logits của teacher, $z_s$ = logits của student.\n",
    "- Nhiệt độ (temperature) $\\tau>0$ làm mềm phân phối:  \n",
    "  $$\n",
    "  p_t^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_t}{\\tau}\\right), \\quad\n",
    "  p_s^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_s}{\\tau}\\right).\n",
    "  $$\n",
    "\n",
    "Hàm mất mát KD dùng KL-divergence giữa phân phối mềm của teacher và student, có hệ số hiệu chỉnh $\\tau^2$:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{KD}} = \\tau^2 \\, \\mathrm{KL}\\big( p_t^{(\\tau)} \\,\\Vert\\, p_s^{(\\tau)} \\big).\n",
    "$$\n",
    "\n",
    "Kết hợp với cross-entropy (CE) chuẩn theo nhãn thật $y$:\n",
    "$$\n",
    "\\mathcal{L} = \\alpha\\,\\mathcal{L}_{\\mathrm{KD}} + (1-\\alpha)\\, \\mathrm{CE}(z_s, y).\n",
    "$$\n",
    "- $\\alpha\\in[0,1]$ điều chỉnh tỷ trọng giữa “học theo giáo viên” và “học theo nhãn thật”.\n",
    "- $\\tau$ lớn làm phân phối mềm hơn (giảm cực đoan), giúp student học được cấu trúc liên lớp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bab5a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T15:46:35.259227Z",
     "iopub.status.busy": "2025-09-13T15:46:35.258697Z",
     "iopub.status.idle": "2025-09-13T15:53:22.398220Z",
     "shell.execute_reply": "2025-09-13T15:53:22.397106Z"
    },
    "id": "9950493c",
    "outputId": "41b0cf57-a916-48b3-fb0c-dd1a651d53e4",
    "papermill": {
     "duration": 407.148379,
     "end_time": "2025-09-13T15:53:22.399549",
     "exception": false,
     "start_time": "2025-09-13T15:46:35.251170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - train_acc: 0.4633 - val_acc: 0.3644 (best)\n",
      "Epoch 2/30 - train_acc: 0.6292 - val_acc: 0.5746 (best)\n",
      "Epoch 3/30 - train_acc: 0.6940 - val_acc: 0.5988 (best)\n",
      "Epoch 4/30 - train_acc: 0.7397 - val_acc: 0.7426 (best)\n",
      "Epoch 5/30 - train_acc: 0.7617 - val_acc: 0.7344\n",
      "Epoch 6/30 - train_acc: 0.7769 - val_acc: 0.6634\n",
      "Epoch 7/30 - train_acc: 0.7878 - val_acc: 0.6988\n",
      "Epoch 8/30 - train_acc: 0.7945 - val_acc: 0.7132\n",
      "Epoch 9/30 - train_acc: 0.8026 - val_acc: 0.7936 (best)\n",
      "Epoch 10/30 - train_acc: 0.8045 - val_acc: 0.7488\n",
      "Epoch 11/30 - train_acc: 0.8142 - val_acc: 0.7714\n",
      "Epoch 12/30 - train_acc: 0.8196 - val_acc: 0.7284\n",
      "Epoch 13/30 - train_acc: 0.8235 - val_acc: 0.7932\n",
      "Epoch 14/30 - train_acc: 0.8286 - val_acc: 0.7732\n",
      "Epoch 15/30 - train_acc: 0.8347 - val_acc: 0.7972 (best)\n",
      "Epoch 16/30 - train_acc: 0.8377 - val_acc: 0.8274 (best)\n",
      "Epoch 17/30 - train_acc: 0.8468 - val_acc: 0.7976\n",
      "Epoch 18/30 - train_acc: 0.8525 - val_acc: 0.8448 (best)\n",
      "Epoch 19/30 - train_acc: 0.8589 - val_acc: 0.8222\n",
      "Epoch 20/30 - train_acc: 0.8632 - val_acc: 0.7886\n",
      "Epoch 21/30 - train_acc: 0.8700 - val_acc: 0.8594 (best)\n",
      "Epoch 22/30 - train_acc: 0.8754 - val_acc: 0.8304\n",
      "Epoch 23/30 - train_acc: 0.8790 - val_acc: 0.8436\n",
      "Epoch 24/30 - train_acc: 0.8864 - val_acc: 0.8512\n",
      "Epoch 25/30 - train_acc: 0.8936 - val_acc: 0.8680 (best)\n",
      "Epoch 26/30 - train_acc: 0.8981 - val_acc: 0.8838 (best)\n",
      "Epoch 27/30 - train_acc: 0.9043 - val_acc: 0.8920 (best)\n",
      "Epoch 28/30 - train_acc: 0.9081 - val_acc: 0.8936 (best)\n",
      "Epoch 29/30 - train_acc: 0.9102 - val_acc: 0.8962 (best)\n",
      "Epoch 30/30 - train_acc: 0.9125 - val_acc: 0.8962\n",
      "Saved student checkpoint (v1) to ./checkpoints/kd_student_v1.pth\n",
      "{'method': 'Vanilla KD', 'train_time_sec': 397.8, 'train_acc': 0.9169, 'val_acc': 0.8962, 'test_acc': 0.8856}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Vanilla KD\n",
    "# Hyperparams for KD\n",
    "T = 4.0\n",
    "ALPHA = 0.7\n",
    "LR = 0.1\n",
    "\n",
    "# KD loss function for Vanilla KD\n",
    "def kd_loss_vanilla(logits_s, logits_t, y, T=4.0, alpha=0.5):\n",
    "    ce = F.cross_entropy(logits_s, y)\n",
    "    p_s = F.log_softmax(logits_s / T, dim=1)\n",
    "    p_t = F.softmax(logits_t / T, dim=1)\n",
    "    kd = F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "    return alpha * kd + (1 - alpha) * ce\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = kd_loss_vanilla(logits_s, logits_t, y, T=T, alpha=ALPHA)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 1\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v1.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v1) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Vanilla KD\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb3a49",
   "metadata": {
    "id": "afb52880",
    "papermill": {
     "duration": 0.062107,
     "end_time": "2025-09-13T15:53:22.470096",
     "exception": false,
     "start_time": "2025-09-13T15:53:22.407989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 2: Hard-label Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7178e32",
   "metadata": {},
   "source": [
    "**Ý tưởng:** Biến dự đoán của teacher thành ''nhãn cứng'' giả (pseudo-label) rồi kết hợp với nhãn thật.\n",
    "\n",
    "Với một mẫu $(x, y)$:\n",
    "- $z_t$ = logits của teacher, nhãn giả của teacher là:  \n",
    "  $$\n",
    "  \\tilde{y} = \\arg\\max\\limits_{c}\\; z_{t,c}.\n",
    "  $$\n",
    "- $z_s$ = logits của student.\n",
    "\n",
    "Hàm mất mát kết hợp hai cross-entropy:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{CEE}} = \\beta\\,\\mathrm{CE}(z_s, \\tilde{y}) + (1-\\beta)\\, \\mathrm{CE}(z_s, y).\n",
    "$$\n",
    "- Thành phần $\\mathrm{CE}(z_s, y)$ giúp bám sát nhãn thật.\n",
    "- Thành phần $\\mathrm{CE}(z_s, \\tilde{y})$ ép student bắt chước dự đoán mạnh nhất của teacher (hard target).\n",
    "- $\\beta$ điều chỉnh mức tin cậy vào teacher.\n",
    "\n",
    "Khác với Vanilla KD, CEE không dùng nhiệt độ hay phân phối mềm; nó chỉ dựa vào lớp có xác suất cao nhất của teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0960f92f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T15:53:22.488012Z",
     "iopub.status.busy": "2025-09-13T15:53:22.487351Z",
     "iopub.status.idle": "2025-09-13T16:00:10.360563Z",
     "shell.execute_reply": "2025-09-13T16:00:10.359764Z"
    },
    "id": "403fd21e",
    "outputId": "c6e0b59b-97a2-47a1-b050-bf9fd850b797",
    "papermill": {
     "duration": 407.883745,
     "end_time": "2025-09-13T16:00:10.361935",
     "exception": false,
     "start_time": "2025-09-13T15:53:22.478190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - train_acc: 0.4465 - val_acc: 0.5266 (best)\n",
      "Epoch 2/30 - train_acc: 0.6142 - val_acc: 0.3372\n",
      "Epoch 3/30 - train_acc: 0.6719 - val_acc: 0.6370 (best)\n",
      "Epoch 4/30 - train_acc: 0.7086 - val_acc: 0.6272\n",
      "Epoch 5/30 - train_acc: 0.7296 - val_acc: 0.5376\n",
      "Epoch 6/30 - train_acc: 0.7416 - val_acc: 0.5126\n",
      "Epoch 7/30 - train_acc: 0.7536 - val_acc: 0.6584 (best)\n",
      "Epoch 8/30 - train_acc: 0.7603 - val_acc: 0.6754 (best)\n",
      "Epoch 9/30 - train_acc: 0.7659 - val_acc: 0.7246 (best)\n",
      "Epoch 10/30 - train_acc: 0.7746 - val_acc: 0.7616 (best)\n",
      "Epoch 11/30 - train_acc: 0.7783 - val_acc: 0.7246\n",
      "Epoch 12/30 - train_acc: 0.7826 - val_acc: 0.7444\n",
      "Epoch 13/30 - train_acc: 0.7916 - val_acc: 0.7192\n",
      "Epoch 14/30 - train_acc: 0.7963 - val_acc: 0.7408\n",
      "Epoch 15/30 - train_acc: 0.8026 - val_acc: 0.6566\n",
      "Epoch 16/30 - train_acc: 0.8071 - val_acc: 0.7598\n",
      "Epoch 17/30 - train_acc: 0.8136 - val_acc: 0.5882\n",
      "Epoch 18/30 - train_acc: 0.8224 - val_acc: 0.7972 (best)\n",
      "Epoch 19/30 - train_acc: 0.8291 - val_acc: 0.7998 (best)\n",
      "Epoch 20/30 - train_acc: 0.8355 - val_acc: 0.8078 (best)\n",
      "Epoch 21/30 - train_acc: 0.8455 - val_acc: 0.7858\n",
      "Epoch 22/30 - train_acc: 0.8519 - val_acc: 0.8326 (best)\n",
      "Epoch 23/30 - train_acc: 0.8586 - val_acc: 0.8318\n",
      "Epoch 24/30 - train_acc: 0.8676 - val_acc: 0.8562 (best)\n",
      "Epoch 25/30 - train_acc: 0.8758 - val_acc: 0.8502\n",
      "Epoch 26/30 - train_acc: 0.8838 - val_acc: 0.8732 (best)\n",
      "Epoch 27/30 - train_acc: 0.8916 - val_acc: 0.8742 (best)\n",
      "Epoch 28/30 - train_acc: 0.8964 - val_acc: 0.8782 (best)\n",
      "Epoch 29/30 - train_acc: 0.9045 - val_acc: 0.8800 (best)\n",
      "Epoch 30/30 - train_acc: 0.9038 - val_acc: 0.8814 (best)\n",
      "Saved student checkpoint (v2) to ./checkpoints/kd_student_v2.pth\n",
      "{'method': 'Hard-label Distillation (CEE)', 'train_time_sec': 398.33, 'train_acc': 0.9128, 'val_acc': 0.8814, 'test_acc': 0.8692}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Hard-label Distillation (CEE)\n",
    "BETA = 0.7\n",
    "LR = 0.1\n",
    "\n",
    "# CEE loss function\n",
    "def cee_loss(logits_s, logits_t, y, beta=0.7):\n",
    "    # CEE: combine CE(student, y) and CE(student, teacher_label)\n",
    "    ce_y = F.cross_entropy(logits_s, y)\n",
    "    pseudo = logits_t.argmax(dim=1)\n",
    "    ce_t = F.cross_entropy(logits_s, pseudo)\n",
    "    return beta * ce_t + (1 - beta) * ce_y\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = cee_loss(logits_s, logits_t, y, beta=BETA)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 2\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v2.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v2) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Hard-label Distillation (CEE)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa623fd6",
   "metadata": {
    "id": "4d6604d1",
    "papermill": {
     "duration": 0.009111,
     "end_time": "2025-09-13T16:00:10.380509",
     "exception": false,
     "start_time": "2025-09-13T16:00:10.371398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 3: Feature Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be4057",
   "metadata": {},
   "source": [
    "**Ý tưởng:** Khớp đặc trưng trung gian (feature maps) giữa teacher và student để student học biểu diễn gần giống teacher.\n",
    "\n",
    "Ký hiệu theo minibatch:\n",
    "- $F_t \\in \\mathbb{R}^{N\\times C_t\\times H_t\\times W_t}$: đặc trưng của teacher ở một tầng (ví dụ layer cuối conv).\n",
    "- $F_s \\in \\mathbb{R}^{N\\times C_s\\times H_s\\times W_s}$: đặc trưng của student ở tầng tương ứng.\n",
    "- Do số kênh/không gian khác nhau, ta dùng một “đầu chiếu” $g_s(\\cdot)$ để đưa $F_s$ về không gian của $F_t$ và/hoặc nội suy không gian về cùng kích thước.\n",
    "- Chuẩn hoá theo kênh để giảm lệch về biên độ:  \n",
    "  $$\n",
    "  \\widehat{F} = \\frac{F}{\\sqrt{\\sum\\limits_{c} F_c^2}+\\varepsilon}.\n",
    "  $$\n",
    "\n",
    "Hàm mất mát tổng hợp:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s, y) + \\lambda_{\\mathrm{feat}}\\, \\big\\|\\, \\widehat{g_s(F_s)} - \\widehat{F_t} \\,\\big\\|_2^2.\n",
    "$$\n",
    "- CE đảm bảo mục tiêu phân loại; \n",
    "- MSE giữa đặc trưng đã chuẩn hoá giúp student học cấu trúc biểu diễn của teacher.\n",
    "- Trong thực nghiệm thường tăng dần hệ số $\\lambda_{\\mathrm{feat}}$ (ramp-up) để ổn định huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf5e11e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T16:00:10.400799Z",
     "iopub.status.busy": "2025-09-13T16:00:10.400492Z",
     "iopub.status.idle": "2025-09-13T16:07:12.881021Z",
     "shell.execute_reply": "2025-09-13T16:07:12.879990Z"
    },
    "id": "3e2de962",
    "outputId": "9322bf2f-8e29-4e06-f2f1-f8bb5422e915",
    "papermill": {
     "duration": 422.492486,
     "end_time": "2025-09-13T16:07:12.882251",
     "exception": false,
     "start_time": "2025-09-13T16:00:10.389765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - train_acc: 0.4514 - val_acc: 0.4994 - feat_w: 3.3 (best)\n",
      "Epoch 2/30 - train_acc: 0.6111 - val_acc: 0.5712 - feat_w: 6.7 (best)\n",
      "Epoch 3/30 - train_acc: 0.6723 - val_acc: 0.6176 - feat_w: 10.0 (best)\n",
      "Epoch 4/30 - train_acc: 0.7047 - val_acc: 0.6026 - feat_w: 13.3\n",
      "Epoch 5/30 - train_acc: 0.7264 - val_acc: 0.6438 - feat_w: 16.7 (best)\n",
      "Epoch 6/30 - train_acc: 0.7403 - val_acc: 0.7244 - feat_w: 20.0 (best)\n",
      "Epoch 7/30 - train_acc: 0.7469 - val_acc: 0.6620 - feat_w: 23.3\n",
      "Epoch 8/30 - train_acc: 0.7558 - val_acc: 0.6856 - feat_w: 26.7\n",
      "Epoch 9/30 - train_acc: 0.7637 - val_acc: 0.7040 - feat_w: 30.0\n",
      "Epoch 10/30 - train_acc: 0.7716 - val_acc: 0.5898 - feat_w: 33.3\n",
      "Epoch 11/30 - train_acc: 0.7759 - val_acc: 0.7238 - feat_w: 36.7\n",
      "Epoch 12/30 - train_acc: 0.7840 - val_acc: 0.7352 - feat_w: 40.0 (best)\n",
      "Epoch 13/30 - train_acc: 0.7898 - val_acc: 0.7258 - feat_w: 43.3\n",
      "Epoch 14/30 - train_acc: 0.7946 - val_acc: 0.6664 - feat_w: 46.7\n",
      "Epoch 15/30 - train_acc: 0.8035 - val_acc: 0.7706 - feat_w: 50.0 (best)\n",
      "Epoch 16/30 - train_acc: 0.8082 - val_acc: 0.7860 - feat_w: 50.0 (best)\n",
      "Epoch 17/30 - train_acc: 0.8144 - val_acc: 0.7464 - feat_w: 50.0\n",
      "Epoch 18/30 - train_acc: 0.8217 - val_acc: 0.7908 - feat_w: 50.0 (best)\n",
      "Epoch 19/30 - train_acc: 0.8288 - val_acc: 0.7856 - feat_w: 50.0\n",
      "Epoch 20/30 - train_acc: 0.8367 - val_acc: 0.7970 - feat_w: 50.0 (best)\n",
      "Epoch 21/30 - train_acc: 0.8443 - val_acc: 0.7848 - feat_w: 50.0\n",
      "Epoch 22/30 - train_acc: 0.8534 - val_acc: 0.7936 - feat_w: 50.0\n",
      "Epoch 23/30 - train_acc: 0.8599 - val_acc: 0.8470 - feat_w: 50.0 (best)\n",
      "Epoch 24/30 - train_acc: 0.8702 - val_acc: 0.8354 - feat_w: 50.0\n",
      "Epoch 25/30 - train_acc: 0.8782 - val_acc: 0.8676 - feat_w: 50.0 (best)\n",
      "Epoch 26/30 - train_acc: 0.8867 - val_acc: 0.8672 - feat_w: 50.0\n",
      "Epoch 27/30 - train_acc: 0.8935 - val_acc: 0.8740 - feat_w: 50.0 (best)\n",
      "Epoch 28/30 - train_acc: 0.8987 - val_acc: 0.8806 - feat_w: 50.0 (best)\n",
      "Epoch 29/30 - train_acc: 0.9031 - val_acc: 0.8842 - feat_w: 50.0 (best)\n",
      "Epoch 30/30 - train_acc: 0.9055 - val_acc: 0.8878 - feat_w: 50.0 (best)\n",
      "Saved student checkpoint (v3) to ./checkpoints/kd_student_v3.pth\n",
      "{'method': 'Feature Distillation (MSE)', 'train_time_sec': 412.97, 'train_acc': 0.9148, 'val_acc': 0.8878, 'test_acc': 0.8712}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Feature Distillation (penultimate features)\n",
    "# We'll tap student at its final conv output (before average pool) and teacher at layer4 output.\n",
    "LR = 0.1\n",
    "W_FEAT = 50.0  # lower weight; we will ramp it up during training\n",
    "\n",
    "# Keep student architecture unchanged; just expose features\n",
    "class StudentExposeFeat(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)  # N,128,4,4\n",
    "        logits = self.classifier(f)\n",
    "        return logits, f\n",
    "\n",
    "student = StudentExposeFeat(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Teacher hook at last conv block (layer4)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Separate projection head (FitNet-style regressor) outside the student\n",
    "proj_s = nn.Sequential(\n",
    "    nn.Conv2d(128, 512, kernel_size=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace=True),\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.SGD(list(student.parameters()) + list(proj_s.parameters()), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# channel-wise L2 normalization helper\n",
    "def norm_channel(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    return x / (x.pow(2).sum(dim=1, keepdim=True).sqrt().clamp_min(eps))\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train(); proj_s.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    # linear ramp-up for feature loss (first half epochs)\n",
    "    ramp = min(1.0, (epoch + 1) / max(1, KD_EPOCHS // 2))\n",
    "    feat_w = W_FEAT * ramp\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook_t.feat\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            f_s_proj = proj_s(f_s)  # project to 512 channels\n",
    "            # Align spatial dims if needed using adaptive avgpool to teacher spatial size\n",
    "            if f_t is None:\n",
    "                raise RuntimeError(\"Teacher feature hook not captured.\")\n",
    "            if f_s_proj.shape[-2:] != f_t.shape[-2:]:\n",
    "                f_s_resized = F.adaptive_avg_pool2d(f_s_proj, f_t.shape[-2:])\n",
    "            else:\n",
    "                f_s_resized = f_s_proj\n",
    "            # Normalize features along channel dimension to reduce scale mismatch\n",
    "            nf_s = norm_channel(f_s_resized)\n",
    "            nf_t = norm_channel(f_t)\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            loss_feat = F.mse_loss(nf_s, nf_t.detach())\n",
    "            loss = loss_ce + feat_w * loss_feat\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    student.eval(); proj_s.eval()\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = {\n",
    "            'student': copy.deepcopy(student.state_dict()),\n",
    "            'proj_s': copy.deepcopy(proj_s.state_dict()),\n",
    "        }\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f} - feat_w: {feat_w:.1f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state['student'])\n",
    "    proj_s.load_state_dict(best_state['proj_s'])\n",
    "\n",
    "# Save student checkpoint for method 3\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v3.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v3) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Feature Distillation (MSE)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea6e93",
   "metadata": {
    "id": "bfb25fc4",
    "papermill": {
     "duration": 0.010681,
     "end_time": "2025-09-13T16:07:12.903895",
     "exception": false,
     "start_time": "2025-09-13T16:07:12.893214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 4: Attention Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f46f2cb",
   "metadata": {},
   "source": [
    "**Ý tưởng:** Khớp toàn bộ feature, AT khớp \"bản đồ chú ý\" (attention map) – độ mạnh tổng hợp theo kênh ở từng vị trí không gian.\n",
    "\n",
    "Cho feature $F \\in \\mathbb{R}^{N\\times C\\times H\\times W}$, bản đồ chú ý $A \\in \\mathbb{R}^{N\\times H\\times W}$ được định nghĩa (một biến thể hay dùng):\n",
    "$$\n",
    "A = \\frac{\\tfrac{1}{C}\\sum\\limits_{c=1}^C F_c^2}{\\left\\|\\, \\tfrac{1}{C}\\sum\\limits_{c=1}^C F_c^2 \\right\\|_2 + \\varepsilon}.\n",
    "$$\n",
    "\n",
    "Với $A_s, A_t$ lần lượt của student và teacher (đã chuẩn hoá), hàm mất mát AT:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{AT}} = \\lambda_{\\mathrm{AT}}\\, \\big\\| A_s - A_t \\big\\|_2^2.\n",
    "$$\n",
    "Tổng mất mát:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s, y) + \\mathcal{L}_{\\mathrm{AT}}.\n",
    "$$\n",
    "\n",
    "AT truyền \"nơi nào quan trọng\" trong ảnh theo teacher. Student học tập trung vào vùng hữu ích thay vì khớp mọi chi tiết của feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c2dc1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T16:07:12.926554Z",
     "iopub.status.busy": "2025-09-13T16:07:12.925943Z",
     "iopub.status.idle": "2025-09-13T16:14:03.864465Z",
     "shell.execute_reply": "2025-09-13T16:14:03.863572Z"
    },
    "id": "c10bc1cb",
    "outputId": "9b99cce7-8dbc-439e-d55f-319d44f92554",
    "papermill": {
     "duration": 410.951406,
     "end_time": "2025-09-13T16:14:03.865787",
     "exception": false,
     "start_time": "2025-09-13T16:07:12.914381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - train_acc: 0.3869 - val_acc: 0.4340 (best)\n",
      "Epoch 2/30 - train_acc: 0.5637 - val_acc: 0.5146 (best)\n",
      "Epoch 3/30 - train_acc: 0.6471 - val_acc: 0.6492 (best)\n",
      "Epoch 4/30 - train_acc: 0.6968 - val_acc: 0.6504 (best)\n",
      "Epoch 5/30 - train_acc: 0.7216 - val_acc: 0.6976 (best)\n",
      "Epoch 6/30 - train_acc: 0.7377 - val_acc: 0.7000 (best)\n",
      "Epoch 7/30 - train_acc: 0.7529 - val_acc: 0.5634\n",
      "Epoch 8/30 - train_acc: 0.7649 - val_acc: 0.6014\n",
      "Epoch 9/30 - train_acc: 0.7752 - val_acc: 0.6250\n",
      "Epoch 10/30 - train_acc: 0.7794 - val_acc: 0.7224 (best)\n",
      "Epoch 11/30 - train_acc: 0.7850 - val_acc: 0.7570 (best)\n",
      "Epoch 12/30 - train_acc: 0.7955 - val_acc: 0.7372\n",
      "Epoch 13/30 - train_acc: 0.8000 - val_acc: 0.7486\n",
      "Epoch 14/30 - train_acc: 0.8070 - val_acc: 0.7240\n",
      "Epoch 15/30 - train_acc: 0.8125 - val_acc: 0.7864 (best)\n",
      "Epoch 16/30 - train_acc: 0.8195 - val_acc: 0.7826\n",
      "Epoch 17/30 - train_acc: 0.8241 - val_acc: 0.8084 (best)\n",
      "Epoch 18/30 - train_acc: 0.8312 - val_acc: 0.8128 (best)\n",
      "Epoch 19/30 - train_acc: 0.8368 - val_acc: 0.8116\n",
      "Epoch 20/30 - train_acc: 0.8415 - val_acc: 0.8126\n",
      "Epoch 21/30 - train_acc: 0.8518 - val_acc: 0.8406 (best)\n",
      "Epoch 22/30 - train_acc: 0.8537 - val_acc: 0.8274\n",
      "Epoch 23/30 - train_acc: 0.8604 - val_acc: 0.8510 (best)\n",
      "Epoch 24/30 - train_acc: 0.8660 - val_acc: 0.8570 (best)\n",
      "Epoch 25/30 - train_acc: 0.8701 - val_acc: 0.8506\n",
      "Epoch 26/30 - train_acc: 0.8783 - val_acc: 0.8610 (best)\n",
      "Epoch 27/30 - train_acc: 0.8838 - val_acc: 0.8692 (best)\n",
      "Epoch 28/30 - train_acc: 0.8848 - val_acc: 0.8714 (best)\n",
      "Epoch 29/30 - train_acc: 0.8870 - val_acc: 0.8708\n",
      "Epoch 30/30 - train_acc: 0.8904 - val_acc: 0.8754 (best)\n",
      "Saved student checkpoint (v4) to ./checkpoints/kd_student_v4.pth\n",
      "{'method': 'Attention Transfer', 'train_time_sec': 401.28, 'train_acc': 0.8965, 'val_acc': 0.8754, 'test_acc': 0.8585}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Attention Transfer\n",
    "LR = 0.05\n",
    "W_AT = 250.0\n",
    "\n",
    "# Attention Transfer loss function\n",
    "def attention_transfer_loss(f_s, f_t, w=1.0, eps=1e-6):\n",
    "    # Attention Transfer (Zagoruyko & Komodakis): match normalized spatial attention maps\n",
    "    def att_map(f):\n",
    "        # f: N, C, H, W -> N, H, W\n",
    "        am = f.pow(2).mean(dim=1)\n",
    "        am = am / (am.flatten(1).norm(p=2, dim=1, keepdim=True).clamp_min(eps).view(-1,1,1))\n",
    "        return am\n",
    "    a_s, a_t = att_map(f_s), att_map(f_t)\n",
    "    return w * F.mse_loss(a_s, a_t.detach())\n",
    "\n",
    "class StudentWithFeatAT(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        logits = self.classifier(f)\n",
    "        return logits, f\n",
    "\n",
    "student = StudentWithFeatAT(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            if f_t is None:\n",
    "                raise RuntimeError(\"Teacher feature hook not captured.\")\n",
    "            if f_s.shape[-2:] != f_t.shape[-2:]:\n",
    "                f_s_resized = F.adaptive_avg_pool2d(f_s, f_t.shape[-2:])\n",
    "            else:\n",
    "                f_s_resized = f_s\n",
    "            loss = F.cross_entropy(logits_s, y) + attention_transfer_loss(f_s_resized, f_t, w=W_AT)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 4\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v4.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v4) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Attention Transfer\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef97e48",
   "metadata": {
    "id": "60690a71",
    "papermill": {
     "duration": 0.011048,
     "end_time": "2025-09-13T16:14:03.888967",
     "exception": false,
     "start_time": "2025-09-13T16:14:03.877919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 5: Logit Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78ba40",
   "metadata": {},
   "source": [
    "**Ý tưởng:** Ép vector logit của student gần với logit của teacher bằng tổn thất $L_2$, đồng thời giữ CE theo nhãn thật.\n",
    "\n",
    "Với một mẫu $(x, y)$:\n",
    "- $z_t, z_s \\in \\mathbb{R}^C$ là logits (trước softmax) của teacher và student.\n",
    "- Tổn thất logit matching:\n",
    "  $$\n",
    "  \\mathcal{L}_{\\mathrm{logit}} = \\big\\| z_s - z_t \\big\\|_2^2.\n",
    "  $$\n",
    "- Tổng mất mát:\n",
    "  $$\n",
    "  \\mathcal{L} = \\mathrm{CE}(z_s, y) + \\lambda\\, \\mathcal{L}_{\\mathrm{logit}}.\n",
    "  $$\n",
    "\n",
    "Khác Vanilla KD (dùng KL trên phân phối mềm), cách này làm việc trực tiếp ở không gian logit, thường đơn giản và ổn định nhưng có thể kém nhạy với cấu trúc phân phối so với KD dùng temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7416e776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T16:14:03.913883Z",
     "iopub.status.busy": "2025-09-13T16:14:03.913294Z",
     "iopub.status.idle": "2025-09-13T16:20:52.272195Z",
     "shell.execute_reply": "2025-09-13T16:20:52.271268Z"
    },
    "id": "698fb456",
    "outputId": "84ef92ac-0829-40e3-8200-4d573e330ee7",
    "papermill": {
     "duration": 408.373401,
     "end_time": "2025-09-13T16:20:52.273485",
     "exception": false,
     "start_time": "2025-09-13T16:14:03.900084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - train_acc: 0.4401 - val_acc: 0.5154 (best)\n",
      "Epoch 2/30 - train_acc: 0.6310 - val_acc: 0.6338 (best)\n",
      "Epoch 3/30 - train_acc: 0.7016 - val_acc: 0.7268 (best)\n",
      "Epoch 4/30 - train_acc: 0.7484 - val_acc: 0.7278 (best)\n",
      "Epoch 5/30 - train_acc: 0.7662 - val_acc: 0.7034\n",
      "Epoch 6/30 - train_acc: 0.7800 - val_acc: 0.6726\n",
      "Epoch 7/30 - train_acc: 0.7914 - val_acc: 0.6998\n",
      "Epoch 8/30 - train_acc: 0.7952 - val_acc: 0.6910\n",
      "Epoch 9/30 - train_acc: 0.8066 - val_acc: 0.6754\n",
      "Epoch 10/30 - train_acc: 0.8139 - val_acc: 0.7828 (best)\n",
      "Epoch 11/30 - train_acc: 0.8192 - val_acc: 0.7890 (best)\n",
      "Epoch 12/30 - train_acc: 0.8188 - val_acc: 0.7762\n",
      "Epoch 13/30 - train_acc: 0.8293 - val_acc: 0.7506\n",
      "Epoch 14/30 - train_acc: 0.8362 - val_acc: 0.8058 (best)\n",
      "Epoch 15/30 - train_acc: 0.8406 - val_acc: 0.8272 (best)\n",
      "Epoch 16/30 - train_acc: 0.8420 - val_acc: 0.7770\n",
      "Epoch 17/30 - train_acc: 0.8506 - val_acc: 0.8074\n",
      "Epoch 18/30 - train_acc: 0.8556 - val_acc: 0.8448 (best)\n",
      "Epoch 19/30 - train_acc: 0.8612 - val_acc: 0.8428\n",
      "Epoch 20/30 - train_acc: 0.8677 - val_acc: 0.8602 (best)\n",
      "Epoch 21/30 - train_acc: 0.8718 - val_acc: 0.8566\n",
      "Epoch 22/30 - train_acc: 0.8775 - val_acc: 0.8404\n",
      "Epoch 23/30 - train_acc: 0.8832 - val_acc: 0.8668 (best)\n",
      "Epoch 24/30 - train_acc: 0.8894 - val_acc: 0.8772 (best)\n",
      "Epoch 25/30 - train_acc: 0.8945 - val_acc: 0.8766\n",
      "Epoch 26/30 - train_acc: 0.8982 - val_acc: 0.8892 (best)\n",
      "Epoch 27/30 - train_acc: 0.9040 - val_acc: 0.8938 (best)\n",
      "Epoch 28/30 - train_acc: 0.9072 - val_acc: 0.8970 (best)\n",
      "Epoch 29/30 - train_acc: 0.9089 - val_acc: 0.9002 (best)\n",
      "Epoch 30/30 - train_acc: 0.9110 - val_acc: 0.9022 (best)\n",
      "Saved student checkpoint (v5) to ./checkpoints/kd_student_v5.pth\n",
      "{'method': 'Logit Matching (L2) + CE', 'train_time_sec': 398.94, 'train_acc': 0.9185, 'val_acc': 0.9022, 'test_acc': 0.8875}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Logit Matching (L2) + CE\n",
    "LR = 0.1\n",
    "W_LOGIT = 1.0\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = F.cross_entropy(logits_s, y) + W_LOGIT * F.mse_loss(logits_s, logits_t)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 5\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v5.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v5) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Logit Matching (L2) + CE\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad92d0f9",
   "metadata": {
    "id": "ebfcc591",
    "papermill": {
     "duration": 0.012496,
     "end_time": "2025-09-13T16:20:52.299096",
     "exception": false,
     "start_time": "2025-09-13T16:20:52.286600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 6: Soft-only KD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a15b17",
   "metadata": {},
   "source": [
    "Khác Vanilla KD, phương án này chỉ dùng thành phần KD, bỏ CE theo nhãn thật.\n",
    "\n",
    "Với $(x, y)$:\n",
    "- $z_t, z_s$ là logits của teacher và student.\n",
    "- Phân phối mềm với nhiệt độ $\\tau$:  \n",
    "  $$\n",
    "  p_t^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_t}{\\tau}\\right), \\quad p_s^{(\\tau)} = \\mathrm{softmax}\\!\\left(\\frac{z_s}{\\tau}\\right).\n",
    "  $$\n",
    "- Tổn thất:\n",
    "  $$\n",
    "  \\mathcal{L} = \\tau^2 \\, \\mathrm{KL}\\big( p_t^{(\\tau)} \\,\\Vert\\, p_s^{(\\tau)} \\big).\n",
    "  $$\n",
    "\n",
    "Ưu/nhược điểm:\n",
    "- Ưu: học hoàn toàn theo tri thức mềm của teacher; hữu ích khi nhãn thật nhiễu.\n",
    "- Nhược: thiếu ràng buộc trực tiếp tới nhãn thật, có thể kém chính xác nếu teacher còn hạn chế hoặc $\\tau$ không phù hợp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a66f9ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T16:20:52.325250Z",
     "iopub.status.busy": "2025-09-13T16:20:52.324967Z",
     "iopub.status.idle": "2025-09-13T16:27:40.105326Z",
     "shell.execute_reply": "2025-09-13T16:27:40.104309Z"
    },
    "id": "e880452c",
    "outputId": "ee25de2a-38be-48f7-880b-b139844f4a1f",
    "papermill": {
     "duration": 407.795177,
     "end_time": "2025-09-13T16:27:40.106647",
     "exception": false,
     "start_time": "2025-09-13T16:20:52.311470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - train_acc: 0.4503 - val_acc: 0.5562 (best)\n",
      "Epoch 2/30 - train_acc: 0.6387 - val_acc: 0.6218 (best)\n",
      "Epoch 3/30 - train_acc: 0.7127 - val_acc: 0.7342 (best)\n",
      "Epoch 4/30 - train_acc: 0.7446 - val_acc: 0.7070\n",
      "Epoch 5/30 - train_acc: 0.7614 - val_acc: 0.6570\n",
      "Epoch 6/30 - train_acc: 0.7754 - val_acc: 0.6714\n",
      "Epoch 7/30 - train_acc: 0.7869 - val_acc: 0.7396 (best)\n",
      "Epoch 8/30 - train_acc: 0.7942 - val_acc: 0.7660 (best)\n",
      "Epoch 9/30 - train_acc: 0.8023 - val_acc: 0.7558\n",
      "Epoch 10/30 - train_acc: 0.8064 - val_acc: 0.7590\n",
      "Epoch 11/30 - train_acc: 0.8125 - val_acc: 0.7782 (best)\n",
      "Epoch 12/30 - train_acc: 0.8178 - val_acc: 0.7506\n",
      "Epoch 13/30 - train_acc: 0.8223 - val_acc: 0.7528\n",
      "Epoch 14/30 - train_acc: 0.8316 - val_acc: 0.7946 (best)\n",
      "Epoch 15/30 - train_acc: 0.8358 - val_acc: 0.7478\n",
      "Epoch 16/30 - train_acc: 0.8403 - val_acc: 0.7970 (best)\n",
      "Epoch 17/30 - train_acc: 0.8464 - val_acc: 0.8232 (best)\n",
      "Epoch 18/30 - train_acc: 0.8518 - val_acc: 0.8330 (best)\n",
      "Epoch 19/30 - train_acc: 0.8580 - val_acc: 0.8266\n",
      "Epoch 20/30 - train_acc: 0.8631 - val_acc: 0.8584 (best)\n",
      "Epoch 21/30 - train_acc: 0.8705 - val_acc: 0.8412\n",
      "Epoch 22/30 - train_acc: 0.8734 - val_acc: 0.8474\n",
      "Epoch 23/30 - train_acc: 0.8811 - val_acc: 0.8642 (best)\n",
      "Epoch 24/30 - train_acc: 0.8877 - val_acc: 0.8752 (best)\n",
      "Epoch 25/30 - train_acc: 0.8937 - val_acc: 0.8740\n",
      "Epoch 26/30 - train_acc: 0.8982 - val_acc: 0.8844 (best)\n",
      "Epoch 27/30 - train_acc: 0.9028 - val_acc: 0.8910 (best)\n",
      "Epoch 28/30 - train_acc: 0.9056 - val_acc: 0.8932 (best)\n",
      "Epoch 29/30 - train_acc: 0.9065 - val_acc: 0.8936 (best)\n",
      "Epoch 30/30 - train_acc: 0.9091 - val_acc: 0.8962 (best)\n",
      "Saved student checkpoint (v6) to ./checkpoints/kd_student_v6.pth\n",
      "{'method': 'Soft-only KD', 'train_time_sec': 398.27, 'train_acc': 0.9144, 'val_acc': 0.8962, 'test_acc': 0.8898}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Soft-only KD\n",
    "T = 4.0\n",
    "LR = 0.1\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            # Only KD loss (alpha=1)\n",
    "            p_s = F.log_softmax(logits_s / T, dim=1)\n",
    "            p_t = F.softmax(logits_t / T, dim=1)\n",
    "            loss = F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 6\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v6.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v6) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Soft-only KD\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79aceb",
   "metadata": {
    "id": "9552435b",
    "papermill": {
     "duration": 0.013789,
     "end_time": "2025-09-13T16:27:40.134945",
     "exception": false,
     "start_time": "2025-09-13T16:27:40.121156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 7: Relational Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c17afb9",
   "metadata": {},
   "source": [
    "**Ý tưởng:** RKD không khớp trực tiếp đặc trưng/logit mà khớp các “quan hệ” giữa các mẫu trong batch.\n",
    "\n",
    "Biểu diễn vector hoá (sau pool) và chuẩn hoá $L_2$:  \n",
    "$$\n",
    "z_s = \\mathrm{norm}(\\mathrm{GAP}(F_s)), \\quad z_t = \\mathrm{norm}(\\mathrm{GAP}(F_t)).\n",
    "$$\n",
    "với $\\mathrm{GAP}$ là global average pooling, $\\mathrm{norm}$ là chuẩn hoá $L_2$.\n",
    "\n",
    "1) Khoảng cách cặp (pairwise distance):\n",
    "- Ma trận khoảng cách Euclid:  \n",
    "  $$\n",
    "  D(z)_{ij} = \\| z_i - z_j \\|_2.\n",
    "  $$\n",
    "- Chuẩn hoá theo trung bình phần tử dương để bất biến tỉ lệ:  \n",
    "  $$\n",
    "  \\tilde{D}_t = \\frac{D(z_t)}{\\mathrm{mean}\\big( D(z_t)_{ij} : D(z_t)_{ij}>0 \\big)}, \\quad\n",
    "  \\tilde{D}_s = \\frac{D(z_s)}{\\mathrm{mean}\\big( D(z_s)_{ij} : D(z_s)_{ij}>0 \\big)}.\n",
    "  $$\n",
    "- Mất mát khoảng cách:  \n",
    "  $$\n",
    "  \\mathcal{L}_{\\mathrm{dist}} = \\mathrm{SmoothL1}(\\tilde{D}_s, \\tilde{D}_t).\n",
    "  $$\n",
    "\n",
    "2) Góc bộ ba (triplet angle):\n",
    "- Với mọi $i, j, k$:  \n",
    "  $$\n",
    "  v_{ij} = z_j - z_i,\\; v_{ik} = z_k - z_i,\\; \\cos\\angle(jik) = \\frac{v_{ij}^\\top v_{ik}}{\\|v_{ij}\\|\\,\\|v_{ik}\\|}.\n",
    "  $$\n",
    "- Tập hợp vào tensor $A(z)$ với $A_{i,j,k} = \\cos(\\angle jik)$.  \n",
    "  $$\n",
    "  \\mathcal{L}_{\\mathrm{angle}} = \\mathrm{SmoothL1}\\big( A(z_s), A(z_t) \\big).\n",
    "  $$\n",
    "\n",
    "Tổng mất mát:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s^{\\text{logit}}, y) + \\lambda_d\\, \\mathcal{L}_{\\mathrm{dist}} + \\lambda_a\\, \\mathcal{L}_{\\mathrm{angle}}.\n",
    "$$\n",
    "Trong đó $z_s^{\\text{logit}}$ là logits cho CE; $z_s, z_t$ cho phần RKD được lấy từ feature đã pool và chuẩn hoá. Cách này truyền cấu trúc hình học của không gian biểu diễn từ teacher sang student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f142c64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T16:27:40.165306Z",
     "iopub.status.busy": "2025-09-13T16:27:40.165030Z",
     "iopub.status.idle": "2025-09-13T16:35:52.416014Z",
     "shell.execute_reply": "2025-09-13T16:35:52.415107Z"
    },
    "id": "55cb670f",
    "outputId": "04ce6059-0c1c-4436-fe81-35e7c6353938",
    "papermill": {
     "duration": 492.268028,
     "end_time": "2025-09-13T16:35:52.417128",
     "exception": false,
     "start_time": "2025-09-13T16:27:40.149100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - train_acc: 0.4755 - val_acc: 0.5512 (best)\n",
      "Epoch 2/30 - train_acc: 0.6438 - val_acc: 0.5610 (best)\n",
      "Epoch 3/30 - train_acc: 0.7024 - val_acc: 0.7006 (best)\n",
      "Epoch 4/30 - train_acc: 0.7382 - val_acc: 0.7430 (best)\n",
      "Epoch 5/30 - train_acc: 0.7653 - val_acc: 0.7748 (best)\n",
      "Epoch 6/30 - train_acc: 0.7792 - val_acc: 0.7082\n",
      "Epoch 7/30 - train_acc: 0.7928 - val_acc: 0.7676\n",
      "Epoch 8/30 - train_acc: 0.7999 - val_acc: 0.7142\n",
      "Epoch 9/30 - train_acc: 0.8072 - val_acc: 0.7944 (best)\n",
      "Epoch 10/30 - train_acc: 0.8116 - val_acc: 0.7824\n",
      "Epoch 11/30 - train_acc: 0.8193 - val_acc: 0.7690\n",
      "Epoch 12/30 - train_acc: 0.8211 - val_acc: 0.7254\n",
      "Epoch 13/30 - train_acc: 0.8294 - val_acc: 0.7896\n",
      "Epoch 14/30 - train_acc: 0.8362 - val_acc: 0.7930\n",
      "Epoch 15/30 - train_acc: 0.8376 - val_acc: 0.7882\n",
      "Epoch 16/30 - train_acc: 0.8440 - val_acc: 0.8168 (best)\n",
      "Epoch 17/30 - train_acc: 0.8517 - val_acc: 0.8070\n",
      "Epoch 18/30 - train_acc: 0.8527 - val_acc: 0.8226 (best)\n",
      "Epoch 19/30 - train_acc: 0.8604 - val_acc: 0.8358 (best)\n",
      "Epoch 20/30 - train_acc: 0.8644 - val_acc: 0.8472 (best)\n",
      "Epoch 21/30 - train_acc: 0.8703 - val_acc: 0.8372\n",
      "Epoch 22/30 - train_acc: 0.8741 - val_acc: 0.8422\n",
      "Epoch 23/30 - train_acc: 0.8785 - val_acc: 0.8532 (best)\n",
      "Epoch 24/30 - train_acc: 0.8832 - val_acc: 0.8552 (best)\n",
      "Epoch 25/30 - train_acc: 0.8882 - val_acc: 0.8770 (best)\n",
      "Epoch 26/30 - train_acc: 0.8952 - val_acc: 0.8772 (best)\n",
      "Epoch 27/30 - train_acc: 0.8959 - val_acc: 0.8730\n",
      "Epoch 28/30 - train_acc: 0.8991 - val_acc: 0.8822 (best)\n",
      "Epoch 29/30 - train_acc: 0.9005 - val_acc: 0.8850 (best)\n",
      "Epoch 30/30 - train_acc: 0.9048 - val_acc: 0.8836\n",
      "Saved student checkpoint (v7 RKD) to ./checkpoints/kd_student_v7.pth\n",
      "{'method': 'Relational Knowledge Distillation (RKD)', 'train_time_sec': 482.88, 'train_acc': 0.9107, 'val_acc': 0.885, 'test_acc': 0.8786}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Relational Knowledge Distillation (RKD)\n",
    "# Reference: Park et al., CVPR 2019. We implement RKD with both distance and angle losses.\n",
    "# Idea: match relational structures between samples (pairwise distances and triplet angles) in feature space.\n",
    "\n",
    "LR = 0.05\n",
    "W_RKD_DIST = 25.0\n",
    "W_RKD_ANGLE = 50.0\n",
    "\n",
    "# Student wrapper to expose features before global pooling\n",
    "class StudentWithFeatRKD(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)         # N, 128, H, W\n",
    "        logits = self.classifier(f)  # N, C\n",
    "        return logits, f\n",
    "\n",
    "student = StudentWithFeatRKD(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Teacher feature hook (layer4 output)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Helper: global-average pool to vectors and L2-normalize\n",
    "def to_vec_norm(fm: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # fm: N, C, H, W -> N, C\n",
    "    v = F.adaptive_avg_pool2d(fm, 1).flatten(1)\n",
    "    v = F.normalize(v, dim=1, eps=eps)\n",
    "    return v\n",
    "\n",
    "# RKD: Distance loss (pairwise)\n",
    "def rkd_distance(z_s: torch.Tensor, z_t: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # pairwise Euclidean distances\n",
    "    with torch.no_grad():\n",
    "        # teacher pairwise distances normalized by mean\n",
    "        d_t = torch.cdist(z_t, z_t, p=2)\n",
    "        mean_t = d_t[d_t>0].mean().clamp_min(eps)\n",
    "        d_t = d_t / mean_t\n",
    "    d_s = torch.cdist(z_s, z_s, p=2)\n",
    "    mean_s = d_s[d_s>0].mean().clamp_min(eps)\n",
    "    d_s = d_s / mean_s\n",
    "    return F.smooth_l1_loss(d_s, d_t)\n",
    "\n",
    "# RKD: Angle loss (triplet angles)\n",
    "def rkd_angle(z_s: torch.Tensor, z_t: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # For each anchor i, compute vectors to j and k: v_ij, v_ik, then their angle via cosine\n",
    "    def angle_matrix(z: torch.Tensor) -> torch.Tensor:\n",
    "        # z: N, D\n",
    "        n = z.size(0)\n",
    "        # compute pairwise differences: v_ij = z_j - z_i -> shape (N,N,D)\n",
    "        diff = z.unsqueeze(1) - z.unsqueeze(0)\n",
    "        # normalize along D\n",
    "        diff = F.normalize(diff, dim=2, eps=eps)\n",
    "        # cosine between v_ij and v_ik for all (j,k): cos = v_ij · v_ik\n",
    "        # angle tensor A where A[i,j,k] = cos(angle_jik)\n",
    "        A = torch.einsum('ijd,ikd->ijk', diff, diff)\n",
    "        return A\n",
    "    with torch.no_grad():\n",
    "        A_t = angle_matrix(z_t)\n",
    "    A_s = angle_matrix(z_s)\n",
    "    return F.smooth_l1_loss(A_s, A_t)\n",
    "\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            # CE term for classification\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            # RKD on pooled features\n",
    "            z_s = to_vec_norm(f_s)\n",
    "            z_t = to_vec_norm(f_t)\n",
    "            loss_dist = rkd_distance(z_s, z_t)\n",
    "            loss_ang = rkd_angle(z_s, z_t)\n",
    "            loss = loss_ce + W_RKD_DIST * loss_dist + W_RKD_ANGLE * loss_ang\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 7 (RKD)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v7.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v7 RKD) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Relational Knowledge Distillation (RKD)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0ae39",
   "metadata": {
    "id": "Dx7AI67eYJwI",
    "papermill": {
     "duration": 0.015026,
     "end_time": "2025-09-13T16:35:52.448315",
     "exception": false,
     "start_time": "2025-09-13T16:35:52.433289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp 8: Contrastive Representation Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210137d5",
   "metadata": {},
   "source": [
    "**Ý tưởng:** CRD khớp biểu diễn thông qua mục tiêu tương phản (contrastive) kiểu InfoNCE giữa nhúng (embedding) của student và teacher.\n",
    "\n",
    "Giả sử có $N$ mẫu trong batch. Sau khi chiếu về cùng không gian bởi hai đầu chiếu học được $h_s(\\cdot), h_t(\\cdot)$:\n",
    "- $e_s = h_s(F_s) \\in \\mathbb{R}^{N\\times d}$, $e_t = h_t(F_t) \\in \\mathbb{R}^{N\\times d}$.\n",
    "- Chuẩn hoá $L_2$: $z_s = e_s/\\|e_s\\|$, $z_t = e_t/\\|e_t\\|$ (theo từng mẫu).\n",
    "\n",
    "Với mỗi $i$, ta coi $z_s[i]$ là query và $z_t[i]$ là positive; các $z_t[j]$ ($j\\neq i$) là negatives. Logits tương phản:\n",
    "$$\n",
    "\\ell_{i,j} = \\frac{ z_s[i]^\\top z_t[j] }{\\tau}, \\quad j=1,\\dots,N.\n",
    "$$\n",
    "Tổn thất InfoNCE trong-batch:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{CRD}} = \\frac{1}{N} \\sum_{i=1}^N \\mathrm{CE}\\big( \\ell_{i,:}, \\, j^*=i \\big).\n",
    "$$\n",
    "\n",
    "Tổng mất mát:\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{CE}(z_s^{\\text{logit}}, y) + \\lambda_{\\mathrm{CRD}}\\, \\mathcal{L}_{\\mathrm{CRD}}.\n",
    "$$\n",
    "- $\\tau$ là nhiệt độ điều chỉnh độ sắc của phân bố tương phản.\n",
    "- Mục tiêu: kéo cặp (student, teacher) của cùng mẫu lại gần, đẩy xa cặp khác mẫu, giúp student học không gian biểu diễn tương đồng teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a778f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T16:35:52.480164Z",
     "iopub.status.busy": "2025-09-13T16:35:52.479862Z",
     "iopub.status.idle": "2025-09-13T16:42:58.218080Z",
     "shell.execute_reply": "2025-09-13T16:42:58.217140Z"
    },
    "id": "zzaGwMl_UrqN",
    "outputId": "c046f154-df8a-4f47-fd2f-f2c633309eaa",
    "papermill": {
     "duration": 425.755766,
     "end_time": "2025-09-13T16:42:58.219256",
     "exception": false,
     "start_time": "2025-09-13T16:35:52.463490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - train_acc: 0.4303 - val_acc: 0.5132 (best)\n",
      "Epoch 2/30 - train_acc: 0.6118 - val_acc: 0.5792 (best)\n",
      "Epoch 3/30 - train_acc: 0.6733 - val_acc: 0.6496 (best)\n",
      "Epoch 4/30 - train_acc: 0.7168 - val_acc: 0.6300\n",
      "Epoch 5/30 - train_acc: 0.7348 - val_acc: 0.5584\n",
      "Epoch 6/30 - train_acc: 0.7520 - val_acc: 0.6506 (best)\n",
      "Epoch 7/30 - train_acc: 0.7678 - val_acc: 0.7274 (best)\n",
      "Epoch 8/30 - train_acc: 0.7754 - val_acc: 0.6372\n",
      "Epoch 9/30 - train_acc: 0.7816 - val_acc: 0.6496\n",
      "Epoch 10/30 - train_acc: 0.7903 - val_acc: 0.7306 (best)\n",
      "Epoch 11/30 - train_acc: 0.7972 - val_acc: 0.7634 (best)\n",
      "Epoch 12/30 - train_acc: 0.8039 - val_acc: 0.6856\n",
      "Epoch 13/30 - train_acc: 0.8070 - val_acc: 0.6754\n",
      "Epoch 14/30 - train_acc: 0.8106 - val_acc: 0.7972 (best)\n",
      "Epoch 15/30 - train_acc: 0.8174 - val_acc: 0.7448\n",
      "Epoch 16/30 - train_acc: 0.8247 - val_acc: 0.8004 (best)\n",
      "Epoch 17/30 - train_acc: 0.8293 - val_acc: 0.8058 (best)\n",
      "Epoch 18/30 - train_acc: 0.8369 - val_acc: 0.8060 (best)\n",
      "Epoch 19/30 - train_acc: 0.8421 - val_acc: 0.8252 (best)\n",
      "Epoch 20/30 - train_acc: 0.8495 - val_acc: 0.8206\n",
      "Epoch 21/30 - train_acc: 0.8503 - val_acc: 0.8266 (best)\n",
      "Epoch 22/30 - train_acc: 0.8602 - val_acc: 0.8114\n",
      "Epoch 23/30 - train_acc: 0.8664 - val_acc: 0.8524 (best)\n",
      "Epoch 24/30 - train_acc: 0.8756 - val_acc: 0.8552 (best)\n",
      "Epoch 25/30 - train_acc: 0.8794 - val_acc: 0.8728 (best)\n",
      "Epoch 26/30 - train_acc: 0.8892 - val_acc: 0.8744 (best)\n",
      "Epoch 27/30 - train_acc: 0.8937 - val_acc: 0.8742\n",
      "Epoch 28/30 - train_acc: 0.8994 - val_acc: 0.8808 (best)\n",
      "Epoch 29/30 - train_acc: 0.9034 - val_acc: 0.8838 (best)\n",
      "Epoch 30/30 - train_acc: 0.9071 - val_acc: 0.8872 (best)\n",
      "Saved student checkpoint (v8) to ./checkpoints/kd_student_v8.pth\n",
      "{'method': 'Contrastive Representation Distillation (CRD)', 'train_time_sec': 416.08, 'train_acc': 0.9149, 'val_acc': 0.8872, 'test_acc': 0.8753}\n"
     ]
    }
   ],
   "source": [
    "# %% Train: Contrastive Representation Distillation (CRD, in-batch)\n",
    "# Ref: Tian et al., ICLR 2020. We implement a lightweight in-batch CRD:\n",
    "#   - Take penultimate conv features from student and teacher\n",
    "#   - Project to a shared embedding space with small MLP heads\n",
    "#   - Use InfoNCE with in-batch negatives (z_s vs z_t of all samples)\n",
    "#   - Optimize CE + W_CRD * CRD\n",
    "\n",
    "LR = 0.1\n",
    "W_CRD = 1.0\n",
    "TAU = 0.07   # temperature for contrastive logits\n",
    "EMB_DIM = 128\n",
    "\n",
    "# Student wrapper to expose features\n",
    "class StudentWithFeatCRD(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)         # N, 128, H, W\n",
    "        logits = self.classifier(f)  # N, C\n",
    "        return logits, f\n",
    "\n",
    "student = StudentWithFeatCRD(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Teacher feature hook (layer4)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Projection heads (trainable). Teacher backbone is frozen, but this head is trainable.\n",
    "proj_s = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128, EMB_DIM),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(EMB_DIM, EMB_DIM),\n",
    ").to(DEVICE)\n",
    "\n",
    "proj_t = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, EMB_DIM),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(EMB_DIM, EMB_DIM),\n",
    ").to(DEVICE)\n",
    "\n",
    "# Optimizer includes student + projection heads\n",
    "optimizer = optim.SGD(list(student.parameters()) + list(proj_s.parameters()) + list(proj_t.parameters()),\n",
    "                      lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# Contrastive loss (InfoNCE) using in-batch negatives\n",
    "def crd_loss(emb_s: torch.Tensor, emb_t: torch.Tensor, tau: float = 0.07) -> torch.Tensor:\n",
    "    # Normalize\n",
    "    zs = F.normalize(emb_s, dim=1)\n",
    "    zt = F.normalize(emb_t, dim=1)\n",
    "    # Similarity logits: N x N\n",
    "    logits = (zs @ zt.t()) / tau\n",
    "    targets = torch.arange(logits.size(0), device=logits.device)\n",
    "    return F.cross_entropy(logits, targets)\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train(); proj_s.train(); proj_t.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook\n",
    "            f_t = hook_t.feat  # N, 512, Ht, Wt\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)  # f_s: N, 128, Hs, Ws\n",
    "            # Project to embeddings (vector)\n",
    "            z_s = proj_s(f_s)\n",
    "            z_t = proj_t(f_t.detach())\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            loss_con = crd_loss(z_s, z_t, tau=TAU)\n",
    "            loss = loss_ce + W_CRD * loss_con\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    # Eval on validation using classifier head only\n",
    "    student.eval(); proj_s.eval(); proj_t.eval()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = {\n",
    "            'student': copy.deepcopy(student.state_dict()),\n",
    "            'proj_s': copy.deepcopy(proj_s.state_dict()),\n",
    "            'proj_t': copy.deepcopy(proj_t.state_dict()),\n",
    "        }\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state['student'])\n",
    "    proj_s.load_state_dict(best_state['proj_s'])\n",
    "    proj_t.load_state_dict(best_state['proj_t'])\n",
    "\n",
    "# Save student checkpoint for method 8\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v8.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v8) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Contrastive Representation Distillation (CRD)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0590d5",
   "metadata": {
    "papermill": {
     "duration": 0.016012,
     "end_time": "2025-09-13T16:42:58.251994",
     "exception": false,
     "start_time": "2025-09-13T16:42:58.235982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Đánh giá toàn diện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "addb61d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T16:42:58.286409Z",
     "iopub.status.busy": "2025-09-13T16:42:58.286134Z",
     "iopub.status.idle": "2025-09-13T16:43:14.944065Z",
     "shell.execute_reply": "2025-09-13T16:43:14.942893Z"
    },
    "papermill": {
     "duration": 16.677209,
     "end_time": "2025-09-13T16:43:14.945450",
     "exception": false,
     "start_time": "2025-09-13T16:42:58.268241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Bảng kết quả (metrics) =====\n",
      "     model    acc   loss    ece  agree_t  kl_to_t  cos_logits  entropy\n",
      "   teacher 0.9306 0.2282 0.0256      NaN      NaN         NaN   0.1225\n",
      "v1_vanilla 0.8856 0.3632 0.0429   0.8976   0.2415      0.9145   0.2009\n",
      "    v2_cee 0.8692 0.3867 0.0193   0.8787   0.2752      0.8604   0.3230\n",
      "   v3_feat 0.8712 0.3846 0.0151   0.8807   0.2714      0.8593   0.3284\n",
      "     v4_at 0.8585 0.4088 0.0113   0.8717   0.2875      0.8516   0.3755\n",
      "  v5_logit 0.8875 0.3514 0.0364   0.9020   0.2173      0.9355   0.2153\n",
      "   v6_soft 0.8898 0.3527 0.0410   0.9028   0.2330      0.9183   0.1939\n",
      "    v7_rkd 0.8786 0.3628 0.0192   0.8878   0.2535      0.9132   0.3424\n",
      "    v8_crd 0.8753 0.3638 0.0098   0.8896   0.2457      0.8722   0.3288\n",
      "\n",
      "===== Bảng xếp hạng (rank) =====\n",
      "     model  acc_rank  loss_rank  ece_rank  agree_t_rank  kl_to_t_rank  cos_logits_rank  entropy_rank  avg_rank\n",
      "   teacher       1.0        1.0       6.0           9.0           9.0              9.0           1.0  5.142857\n",
      "v1_vanilla       4.0        5.0       9.0           3.0           3.0              3.0           3.0  4.285714\n",
      "    v2_cee       8.0        8.0       5.0           7.0           7.0              6.0           5.0  6.571429\n",
      "   v3_feat       7.0        7.0       3.0           6.0           6.0              7.0           6.0  6.000000\n",
      "     v4_at       9.0        9.0       2.0           8.0           8.0              8.0           9.0  7.571429\n",
      "  v5_logit       3.0        2.0       7.0           2.0           1.0              1.0           4.0  2.857143\n",
      "   v6_soft       2.0        3.0       8.0           1.0           2.0              2.0           2.0  2.857143\n",
      "    v7_rkd       5.0        4.0       4.0           5.0           5.0              4.0           8.0  5.000000\n",
      "    v8_crd       6.0        6.0       1.0           4.0           4.0              5.0           7.0  4.714286\n"
     ]
    }
   ],
   "source": [
    "# %% Comprehensive Evaluation: Load checkpoints and compare models\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Expect checkpoints in CKPT_DIR\n",
    "ckpt_dir = CKPT_DIR if 'CKPT_DIR' in globals() else './checkpoints'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# Build teacher & student architectures (same as training)\n",
    "teacher_eval = build_teacher(NUM_CLASSES).to(DEVICE)\n",
    "student_eval = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Utility: evaluate metrics over a dataloader\n",
    "@torch.no_grad()\n",
    "def collect_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    logits_list, labels_list = [], []\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        out = model(x)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "        loss_sum += criterion(logits, y).item()\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        logits_list.append(logits.detach().cpu())\n",
    "        labels_list.append(y.detach().cpu())\n",
    "    logits_all = torch.cat(logits_list, dim=0)\n",
    "    labels_all = torch.cat(labels_list, dim=0)\n",
    "    acc = correct / max(1, total)\n",
    "    avg_loss = loss_sum / max(1, total)\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'loss': avg_loss,\n",
    "        'logits': logits_all,\n",
    "        'labels': labels_all,\n",
    "    }\n",
    "\n",
    "# Expected checkpoints\n",
    "models = OrderedDict([\n",
    "    (\"teacher\", os.path.join(ckpt_dir, \"kd_teacher.pth\")),\n",
    "    (\"v1_vanilla\", os.path.join(ckpt_dir, \"kd_student_v1.pth\")),\n",
    "    (\"v2_cee\", os.path.join(ckpt_dir, \"kd_student_v2.pth\")),\n",
    "    (\"v3_feat\", os.path.join(ckpt_dir, \"kd_student_v3.pth\")),\n",
    "    (\"v4_at\", os.path.join(ckpt_dir, \"kd_student_v4.pth\")),\n",
    "    (\"v5_logit\", os.path.join(ckpt_dir, \"kd_student_v5.pth\")),\n",
    "    (\"v6_soft\", os.path.join(ckpt_dir, \"kd_student_v6.pth\")),\n",
    "    (\"v7_rkd\", os.path.join(ckpt_dir, \"kd_student_v7.pth\")),\n",
    "    (\"v8_crd\", os.path.join(ckpt_dir, \"kd_student_v8.pth\")),\n",
    "])\n",
    "\n",
    "# Load teacher\n",
    "loaded = {}\n",
    "if os.path.isfile(models['teacher']):\n",
    "    teacher_eval.load_state_dict(torch.load(models['teacher'], map_location=DEVICE))\n",
    "    loaded['teacher'] = teacher_eval\n",
    "else:\n",
    "    print(f\"[WARN] Teacher checkpoint not found: {models['teacher']}\")\n",
    "\n",
    "# Load students into dict (same arch SmallNet)\n",
    "for name, path in models.items():\n",
    "    if name == 'teacher':\n",
    "        continue\n",
    "    if os.path.isfile(path):\n",
    "        m = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "        m.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "        loaded[name] = m\n",
    "    else:\n",
    "        print(f\"[WARN] Student checkpoint not found: {path}\")\n",
    "\n",
    "# Metrics to compute\n",
    "# - acc_test: accuracy on test set\n",
    "# - loss_test: CE loss on test set\n",
    "# - ece: Expected Calibration Error (10-bin)\n",
    "# - agree_t: agreement rate between student and teacher predictions\n",
    "# - kl_to_t: KL(student || teacher) on test logits (softmax distributions)\n",
    "# - cos_logits: cosine similarity between student and teacher logits\n",
    "# - entropy: average predictive entropy (uncertainty)\n",
    "\n",
    "\n",
    "def expected_calibration_error(probs: torch.Tensor, labels: torch.Tensor, n_bins: int = 10) -> float:\n",
    "    # probs: N,C ; labels: N\n",
    "    confidences, predictions = probs.max(dim=1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "    bins = torch.linspace(0, 1, steps=n_bins + 1)\n",
    "    ece = torch.zeros(1)\n",
    "    for i in range(n_bins):\n",
    "        in_bin = (confidences > bins[i]) & (confidences <= bins[i + 1]) if i < n_bins - 1 else (confidences > bins[i]) & (confidences <= bins[i + 1])\n",
    "        prop = in_bin.float().mean()\n",
    "        if prop.item() > 0:\n",
    "            acc_bin = accuracies[in_bin].float().mean()\n",
    "            conf_bin = confidences[in_bin].float().mean()\n",
    "            ece += torch.abs(conf_bin - acc_bin) * prop\n",
    "    return ece.item()\n",
    "\n",
    "\n",
    "def kl_divergence(p_logits: torch.Tensor, q_logits: torch.Tensor, T: float = 1.0) -> float:\n",
    "    # KL(P||Q) with temperature T\n",
    "    p = F.log_softmax(p_logits / T, dim=1)\n",
    "    q = F.softmax(q_logits / T, dim=1)\n",
    "    return F.kl_div(p, q, reduction='batchmean').item()\n",
    "\n",
    "\n",
    "def cosine_similarity_logits(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    a_flat = a.flatten(1)\n",
    "    b_flat = b.flatten(1)\n",
    "    a_n = F.normalize(a_flat, p=2, dim=1)\n",
    "    b_n = F.normalize(b_flat, p=2, dim=1)\n",
    "    return (a_n * b_n).sum(dim=1).mean().item()\n",
    "\n",
    "\n",
    "def avg_entropy_from_logits(logits: torch.Tensor) -> float:\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    entropy = -(probs * (probs.clamp_min(1e-12).log())).sum(dim=1)\n",
    "    return entropy.mean().item()\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "metrics = [\n",
    "    'acc', 'loss', 'ece', 'agree_t', 'kl_to_t', 'cos_logits', 'entropy'\n",
    "]\n",
    "\n",
    "# First collect teacher outputs\n",
    "teacher_out = None\n",
    "if 'teacher' in loaded:\n",
    "    teacher_out = collect_metrics(loaded['teacher'], test_loader, DEVICE)\n",
    "\n",
    "for name, model in loaded.items():\n",
    "    out = collect_metrics(model, test_loader, DEVICE)\n",
    "    probs = F.softmax(out['logits'], dim=1)\n",
    "    ece = expected_calibration_error(probs, out['labels'])\n",
    "\n",
    "    # Comparisons to teacher (only if teacher available and current is not teacher)\n",
    "    if teacher_out is not None and name != 'teacher':\n",
    "        agree = (out['logits'].argmax(1) == teacher_out['logits'].argmax(1)).float().mean().item()\n",
    "        kl = kl_divergence(out['logits'], teacher_out['logits'])\n",
    "        cos = cosine_similarity_logits(out['logits'], teacher_out['logits'])\n",
    "    else:\n",
    "        agree, kl, cos = np.nan, np.nan, np.nan\n",
    "\n",
    "    ent = avg_entropy_from_logits(out['logits'])\n",
    "\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'acc': round(out['acc'], 4),\n",
    "        'loss': round(out['loss'], 4),\n",
    "        'ece': round(ece, 4),\n",
    "        'agree_t': round(agree, 4) if not np.isnan(agree) else np.nan,\n",
    "        'kl_to_t': round(kl, 4) if not np.isnan(kl) else np.nan,\n",
    "        'cos_logits': round(cos, 4) if not np.isnan(cos) else np.nan,\n",
    "        'entropy': round(ent, 4),\n",
    "    })\n",
    "\n",
    "# Build results DataFrame (sorted by model order above)\n",
    "df = pd.DataFrame(results)\n",
    "# Optional: reorder rows to keep teacher first\n",
    "order = [k for k in models.keys() if k in df['model'].values]\n",
    "df['order_idx'] = df['model'].apply(lambda m: order.index(m) if m in order else 999)\n",
    "df = df.sort_values('order_idx').drop(columns=['order_idx']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n===== Bảng kết quả (metrics) =====\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Ranking table: for each metric, compute rank (best rank = 1).\n",
    "# For metrics where lower is better (loss, ece, kl, entropy), rank ascending. For higher-better (acc, agree_t, cos), rank descending.\n",
    "rank_prefs = {\n",
    "    'acc': 'desc',\n",
    "    'loss': 'asc',\n",
    "    'ece': 'asc',\n",
    "    'agree_t': 'desc',\n",
    "    'kl_to_t': 'asc',\n",
    "    'cos_logits': 'desc',\n",
    "    'entropy': 'asc',\n",
    "}\n",
    "\n",
    "rank_df = df.copy()\n",
    "for col, pref in rank_prefs.items():\n",
    "    series = rank_df[col]\n",
    "    if series.isna().all():\n",
    "        rank_df[col + '_rank'] = np.nan\n",
    "        continue\n",
    "    # For NaN values (e.g., teacher comparisons), assign worst rank\n",
    "    fill_val = series.max() + 1 if pref == 'asc' else series.min() - 1\n",
    "    series_filled = series.fillna(fill_val)\n",
    "    ascending = (pref == 'asc')\n",
    "    rank_df[col + '_rank'] = series_filled.rank(method='min', ascending=ascending)\n",
    "\n",
    "# Keep only rank columns and model name\n",
    "rank_cols = ['model'] + [c + '_rank' for c in rank_prefs.keys()]\n",
    "rank_table = rank_df[rank_cols]\n",
    "rank_table['avg_rank'] = rank_table[[c for c in rank_cols if c != 'model']].mean(axis=1)\n",
    "\n",
    "print(\"\\n===== Bảng xếp hạng (rank) =====\")\n",
    "print(rank_table.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4097.628062,
   "end_time": "2025-09-13T16:43:17.910425",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-13T15:35:00.282363",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
