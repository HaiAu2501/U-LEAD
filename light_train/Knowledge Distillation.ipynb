{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f2a85b",
   "metadata": {
    "id": "e1f2a85b"
   },
   "source": [
    "# Knowledge Distillation on CIFAR-10\n",
    "\n",
    "Mục tiêu: Cài đặt nhiều phương pháp Knowledge Distillation (KD) khác nhau cho bài toán phân loại CIFAR-10.\n",
    "\n",
    "Yêu cầu chính:\n",
    "- Dataset: CIFAR-10 (train/test chuẩn của torchvision)\n",
    "- Model teacher: pretrained trên CIFAR-10; model student nhỏ hơn, chưa train.\n",
    "- Một biến chung `KD_EPOCHS` xác định số epoch train cho TẤT CẢ phương pháp KD.\n",
    "- Trước mỗi phương pháp có một cell markdown ghi tên phương pháp.\n",
    "- Mỗi phương pháp in ra: tổng thời gian train + accuracy trên train và test.\n",
    "\n",
    "Các phần dưới đây cung cấp phần setup dùng chung (dataloader, model, util, teacher), sau đó là từng phương pháp KD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d503ed3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d503ed3",
    "outputId": "7894f04e-bf6d-4fe1-e465-76c1ae67d25e"
   },
   "outputs": [],
   "source": [
    "# %% Shared Setup: dependencies, config, data, models, utils\n",
    "import os, time, math, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import copy\n",
    "\n",
    "# Reproducibility\n",
    "SEED = int(os.environ.get(\"SEED\", 42))\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Global epochs shared by all KD methods\n",
    "KD_EPOCHS = int(os.environ.get(\"KD_EPOCHS\", 30))  # chỉnh tại đây nếu muốn\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "NUM_CLASSES = 10\n",
    "VAL_RATIO = float(os.environ.get(\"VAL_RATIO\", 0.1))  # 10% train -> val split\n",
    "\n",
    "# Teacher training epochs if training from scratch\n",
    "TEACHER_EPOCHS = int(os.environ.get(\"TEACHER_EPOCHS\", 30))\n",
    "\n",
    "# Ensure checkpoints directory exists\n",
    "CKPT_DIR = os.environ.get(\"CKPT_DIR\", \"./checkpoints\")\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# Data: CIFAR-10\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "train_tf = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# Build full train set twice to allow different transforms for train vs validation\n",
    "train_full_aug = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "train_full_plain = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=False, transform=test_tf)\n",
    "\n",
    "# Create reproducible train/val split\n",
    "N = len(train_full_aug)\n",
    "val_size = max(1, int(VAL_RATIO * N))\n",
    "train_size = N - val_size\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "perm = torch.randperm(N, generator=gen)\n",
    "val_idx = perm[:val_size].tolist()\n",
    "train_idx = perm[val_size:].tolist()\n",
    "\n",
    "train_set = Subset(train_full_aug, train_idx)\n",
    "val_set = Subset(train_full_plain, val_idx)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Student model (smaller than ResNet18)\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Teacher model: minimal ResNet18 for CIFAR-10 (3x3 stem, no initial maxpool)\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # layers: 2,2,2,2\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * BasicBlock.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_teacher(num_classes=10):\n",
    "    # Build a CIFAR-10 style ResNet18 from scratch\n",
    "    return ResNet18(num_classes)\n",
    "\n",
    "# Point teacher checkpoint to checkpoints/kd_teacher.pth\n",
    "TEACHER_CKPT = os.path.join(CKPT_DIR, \"kd_teacher.pth\")\n",
    "\n",
    "# Train/eval utilities\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        out = model(x)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "        loss = criterion(logits, y)\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    acc = correct / total\n",
    "    avg_loss = loss_sum / total\n",
    "    return acc, avg_loss\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    train_acc: float\n",
    "    test_acc: float\n",
    "    train_time_sec: float\n",
    "\n",
    "\n",
    "def top1_acc(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "\n",
    "def train_ce(model: nn.Module, loader: DataLoader, optimizer, device: torch.device, scaler: Optional[GradScaler] = None):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                out = model(x)\n",
    "                logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "                loss = criterion(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model(x)\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    return total_correct / max(1, total_samples)\n",
    "\n",
    "def flatten_features(x):\n",
    "    return torch.flatten(x, 1)\n",
    "\n",
    "# Hooks to grab intermediate features (for feature distillation methods)\n",
    "class FeatureHook:\n",
    "    def __init__(self, module: nn.Module):\n",
    "        self.feat = None\n",
    "        module.register_forward_hook(self.hook)\n",
    "    def hook(self, module, input, output):\n",
    "        self.feat = output\n",
    "\n",
    "# Teacher build/load\n",
    "teacher = build_teacher(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "TEACHER_OPT_LR = float(os.environ.get(\"TEACHER_OPT_LR\", 0.1))\n",
    "TEACHER_WD = float(os.environ.get(\"TEACHER_WD\", 5e-4))\n",
    "\n",
    "if os.path.isfile(TEACHER_CKPT):\n",
    "    teacher.load_state_dict(torch.load(TEACHER_CKPT, map_location=DEVICE))\n",
    "    print(f\"Loaded teacher weights from {TEACHER_CKPT}\")\n",
    "else:\n",
    "    print(\"Teacher checkpoint not found. Training ResNet18 teacher from scratch on CIFAR-10 (select best by val acc).\")\n",
    "    optimizer_t = optim.SGD(teacher.parameters(), lr=TEACHER_OPT_LR, momentum=0.9, weight_decay=TEACHER_WD)\n",
    "    scheduler_t = optim.lr_scheduler.CosineAnnealingLR(optimizer_t, T_max=TEACHER_EPOCHS)\n",
    "    scaler_t = GradScaler(enabled=torch.cuda.is_available())\n",
    "    best_val, best_state = 0.0, None\n",
    "    start_t = time.time()\n",
    "    for e in range(TEACHER_EPOCHS):\n",
    "        acc_train = train_ce(teacher, train_loader, optimizer_t, DEVICE, scaler=scaler_t)\n",
    "        acc_val, _ = evaluate(teacher, val_loader, DEVICE)\n",
    "        if acc_val > best_val:\n",
    "            best_val = acc_val\n",
    "            best_state = copy.deepcopy(teacher.state_dict())\n",
    "            torch.save(best_state, TEACHER_CKPT)\n",
    "            tag = \" (saved best)\"\n",
    "        else:\n",
    "            tag = \"\"\n",
    "        scheduler_t.step()\n",
    "        print(f\"Teacher epoch {e+1}/{TEACHER_EPOCHS} - train_acc: {acc_train:.4f} - val_acc: {acc_val:.4f}{tag}\")\n",
    "    elapsed_t = time.time() - start_t\n",
    "    if best_state is not None:\n",
    "        teacher.load_state_dict(best_state)\n",
    "    print(f\"Saved best teacher to {TEACHER_CKPT}. Best val_acc: {best_val:.4f}. Training time: {elapsed_t:.1f}s\")\n",
    "\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "teacher.eval()\n",
    "\n",
    "acc_test, _ = evaluate(teacher, test_loader, DEVICE)\n",
    "print(f\"Teacher test acc: {acc_test:.4f}\")\n",
    "\n",
    "# Save teacher checkpoint into checkpoints directory (always ensure it exists)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "teacher_ckpt_path = TEACHER_CKPT\n",
    "torch.save(teacher.state_dict(), teacher_ckpt_path)\n",
    "print(f\"Saved teacher checkpoint to {teacher_ckpt_path}\")\n",
    "\n",
    "print(\"KD_EPOCHS =\", KD_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b9274",
   "metadata": {
    "id": "795b9274"
   },
   "source": [
    "## So sánh mô hình teacher và student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe070d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eefe070d",
    "outputId": "ae9ec1ef-2829-4629-b0f5-d3da32cc2207"
   },
   "outputs": [],
   "source": [
    "  # %% So sánh teacher vs student: tham số, layer, FLOPs/MACs, Latency\n",
    "from collections import defaultdict\n",
    "\n",
    "# Choose the canonical student used by most methods\n",
    "student_ref = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "def count_params(m: nn.Module):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "def count_layers(m: nn.Module):\n",
    "    # count conv + linear layers as \"layers\"\n",
    "    return sum(isinstance(mod, (nn.Conv2d, nn.Linear)) for mod in m.modules())\n",
    "\n",
    "\n",
    "# Lightweight FLOPs/MACs estimation using hooks (counts MACs ~ multiply-adds)\n",
    "def estimate_macs(model: nn.Module, input_size=(1, 3, 32, 32)):\n",
    "    macs = 0\n",
    "\n",
    "    def conv_hook(self, inp, out):\n",
    "        nonlocal macs\n",
    "        # inp[0]: N,Cin,Hin,Win ; out: N,Cout,Hout,Wout\n",
    "        x = inp[0]\n",
    "        N, Cin, Hin, Win = x.shape\n",
    "        Cout, Hout, Wout = out.shape[1:]\n",
    "        kH, kW = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
    "        # MACs per output element: Cin * kH * kW\n",
    "        macs += N * Cout * Hout * Wout * Cin * kH * kW\n",
    "\n",
    "    def linear_hook(self, inp, out):\n",
    "        nonlocal macs\n",
    "        # inp[0]: N, in_features ; out: N, out_features\n",
    "        N, in_f = inp[0].shape\n",
    "        out_f = out.shape[1]\n",
    "        macs += N * in_f * out_f\n",
    "\n",
    "    hooks = []\n",
    "    for mod in model.modules():\n",
    "        if isinstance(mod, nn.Conv2d):\n",
    "            hooks.append(mod.register_forward_hook(conv_hook))\n",
    "        elif isinstance(mod, nn.Linear):\n",
    "            hooks.append(mod.register_forward_hook(linear_hook))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(*input_size, device=DEVICE)\n",
    "        _ = model(dummy)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # FLOPs ~ 2 * MACs if counting MUL+ADD as two ops. Report both.\n",
    "    flops = 2 * macs\n",
    "    return macs, flops\n",
    "\n",
    "\n",
    "def pretty(n):\n",
    "    # format large numbers with units\n",
    "    for unit in [\"\", \"K\", \"M\", \"B\", \"T\"]:\n",
    "        if abs(n) < 1000:\n",
    "            return f\"{n:.2f}{unit}\"\n",
    "        n /= 1000\n",
    "    return f\"{n:.2f}P\"\n",
    "\n",
    "\n",
    "def measure_latency_ms_per_image(model: nn.Module, batch_size: int = 128, repeats: int = 30, warmup: int = 10):\n",
    "    model.eval()\n",
    "    x = torch.randn(batch_size, 3, 32, 32, device=DEVICE)\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    # Timed runs\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeats):\n",
    "            t0 = time.perf_counter()\n",
    "            _ = model(x)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            t1 = time.perf_counter()\n",
    "            times.append((t1 - t0))\n",
    "    avg_s = sum(times) / len(times)\n",
    "    ms_per_image = (avg_s / batch_size) * 1000.0\n",
    "    return ms_per_image\n",
    "\n",
    "\n",
    "def report_model_stats(name: str, m: nn.Module):\n",
    "    total, trainable = count_params(m)\n",
    "    layers = count_layers(m)\n",
    "    macs, flops = estimate_macs(m)\n",
    "    latency = measure_latency_ms_per_image(m)\n",
    "    print(\n",
    "        f\"[{name}]\\n\"\n",
    "        f\"  Params: {total} (trainable: {trainable}) -> {pretty(total)} params\\n\"\n",
    "        f\"  Layers (Conv+FC): {layers}\\n\"\n",
    "        f\"  MACs (32x32): {pretty(macs)}  |  FLOPs~ {pretty(flops)}\\n\"\n",
    "        f\"  Latency: ~{latency:.3f} ms / image (batch=128, avg over repeats)\\n\"\n",
    "    )\n",
    "\n",
    "print(\"So sánh mô hình teacher và student (đầu vào 32x32):\\n\")\n",
    "report_model_stats(\"Teacher (ResNet18)\", teacher)\n",
    "report_model_stats(\"Student (SmallNet)\", student_ref)\n",
    "\n",
    "# Cleanup\n",
    "del student_ref\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6e5cb",
   "metadata": {
    "id": "81a6e5cb"
   },
   "source": [
    "## Phương pháp 1: Vanilla KD (Hinton et al. 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9950493c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9950493c",
    "outputId": "41b0cf57-a916-48b3-fb0c-dd1a651d53e4"
   },
   "outputs": [],
   "source": [
    "# %% Train: Vanilla KD\n",
    "# Hyperparams for KD\n",
    "T = 4.0\n",
    "ALPHA = 0.7\n",
    "LR = 0.1\n",
    "\n",
    "# KD loss function for Vanilla KD\n",
    "def kd_loss_vanilla(logits_s, logits_t, y, T=4.0, alpha=0.5):\n",
    "    ce = F.cross_entropy(logits_s, y)\n",
    "    p_s = F.log_softmax(logits_s / T, dim=1)\n",
    "    p_t = F.softmax(logits_t / T, dim=1)\n",
    "    kd = F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "    return alpha * kd + (1 - alpha) * ce\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = kd_loss_vanilla(logits_s, logits_t, y, T=T, alpha=ALPHA)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 1\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v1.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v1) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Vanilla KD\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb52880",
   "metadata": {
    "id": "afb52880"
   },
   "source": [
    "## Phương pháp 2: Hard-label Distillation (CEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403fd21e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "403fd21e",
    "outputId": "c6e0b59b-97a2-47a1-b050-bf9fd850b797"
   },
   "outputs": [],
   "source": [
    "# %% Train: Hard-label Distillation (CEE)\n",
    "BETA = 0.7\n",
    "LR = 0.1\n",
    "\n",
    "# CEE loss function\n",
    "def cee_loss(logits_s, logits_t, y, beta=0.7):\n",
    "    # CEE: combine CE(student, y) and CE(student, teacher_label)\n",
    "    ce_y = F.cross_entropy(logits_s, y)\n",
    "    pseudo = logits_t.argmax(dim=1)\n",
    "    ce_t = F.cross_entropy(logits_s, pseudo)\n",
    "    return beta * ce_t + (1 - beta) * ce_y\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = cee_loss(logits_s, logits_t, y, beta=BETA)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 2\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v2.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v2) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Hard-label Distillation (CEE)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6604d1",
   "metadata": {
    "id": "4d6604d1"
   },
   "source": [
    "## Phương pháp 3: Feature Distillation (MSE on penultimate features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2de962",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e2de962",
    "outputId": "9322bf2f-8e29-4e06-f2f1-f8bb5422e915"
   },
   "outputs": [],
   "source": [
    "# %% Train: Feature Distillation (penultimate features)\n",
    "# We'll tap student at its final conv output (before average pool) and teacher at layer4 output.\n",
    "LR = 0.1\n",
    "W_FEAT = 50.0  # lower weight; we will ramp it up during training\n",
    "\n",
    "# Keep student architecture unchanged; just expose features\n",
    "class StudentExposeFeat(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)  # N,128,4,4\n",
    "        logits = self.classifier(f)\n",
    "        return logits, f\n",
    "\n",
    "student = StudentExposeFeat(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Teacher hook at last conv block (layer4)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Separate projection head (FitNet-style regressor) outside the student\n",
    "proj_s = nn.Sequential(\n",
    "    nn.Conv2d(128, 512, kernel_size=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace=True),\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.SGD(list(student.parameters()) + list(proj_s.parameters()), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# channel-wise L2 normalization helper\n",
    "def norm_channel(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    return x / (x.pow(2).sum(dim=1, keepdim=True).sqrt().clamp_min(eps))\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train(); proj_s.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    # linear ramp-up for feature loss (first half epochs)\n",
    "    ramp = min(1.0, (epoch + 1) / max(1, KD_EPOCHS // 2))\n",
    "    feat_w = W_FEAT * ramp\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook_t.feat\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            f_s_proj = proj_s(f_s)  # project to 512 channels\n",
    "            # Align spatial dims if needed using adaptive avgpool to teacher spatial size\n",
    "            if f_t is None:\n",
    "                raise RuntimeError(\"Teacher feature hook not captured.\")\n",
    "            if f_s_proj.shape[-2:] != f_t.shape[-2:]:\n",
    "                f_s_resized = F.adaptive_avg_pool2d(f_s_proj, f_t.shape[-2:])\n",
    "            else:\n",
    "                f_s_resized = f_s_proj\n",
    "            # Normalize features along channel dimension to reduce scale mismatch\n",
    "            nf_s = norm_channel(f_s_resized)\n",
    "            nf_t = norm_channel(f_t)\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            loss_feat = F.mse_loss(nf_s, nf_t.detach())\n",
    "            loss = loss_ce + feat_w * loss_feat\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    student.eval(); proj_s.eval()\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = {\n",
    "            'student': copy.deepcopy(student.state_dict()),\n",
    "            'proj_s': copy.deepcopy(proj_s.state_dict()),\n",
    "        }\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f} - feat_w: {feat_w:.1f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state['student'])\n",
    "    proj_s.load_state_dict(best_state['proj_s'])\n",
    "\n",
    "# Save student checkpoint for method 3\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v3.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v3) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Feature Distillation (MSE)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb25fc4",
   "metadata": {
    "id": "bfb25fc4"
   },
   "source": [
    "## Phương pháp 4: Attention Transfer (AT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bc1cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c10bc1cb",
    "outputId": "9b99cce7-8dbc-439e-d55f-319d44f92554"
   },
   "outputs": [],
   "source": [
    "# %% Train: Attention Transfer\n",
    "LR = 0.05\n",
    "W_AT = 250.0\n",
    "\n",
    "# Attention Transfer loss function\n",
    "def attention_transfer_loss(f_s, f_t, w=1.0, eps=1e-6):\n",
    "    # Attention Transfer (Zagoruyko & Komodakis): match normalized spatial attention maps\n",
    "    def att_map(f):\n",
    "        # f: N, C, H, W -> N, H, W\n",
    "        am = f.pow(2).mean(dim=1)\n",
    "        am = am / (am.flatten(1).norm(p=2, dim=1, keepdim=True).clamp_min(eps).view(-1,1,1))\n",
    "        return am\n",
    "    a_s, a_t = att_map(f_s), att_map(f_t)\n",
    "    return w * F.mse_loss(a_s, a_t.detach())\n",
    "\n",
    "class StudentWithFeatAT(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        logits = self.classifier(f)\n",
    "        return logits, f\n",
    "\n",
    "student = StudentWithFeatAT(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            if f_t is None:\n",
    "                raise RuntimeError(\"Teacher feature hook not captured.\")\n",
    "            if f_s.shape[-2:] != f_t.shape[-2:]:\n",
    "                f_s_resized = F.adaptive_avg_pool2d(f_s, f_t.shape[-2:])\n",
    "            else:\n",
    "                f_s_resized = f_s\n",
    "            loss = F.cross_entropy(logits_s, y) + attention_transfer_loss(f_s_resized, f_t, w=W_AT)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 4\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v4.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v4) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Attention Transfer\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60690a71",
   "metadata": {
    "id": "60690a71"
   },
   "source": [
    "## Phương pháp 5: Logit Matching (L2) + CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fb456",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "698fb456",
    "outputId": "84ef92ac-0829-40e3-8200-4d573e330ee7"
   },
   "outputs": [],
   "source": [
    "# %% Train: Logit Matching (L2) + CE\n",
    "LR = 0.1\n",
    "W_LOGIT = 1.0\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            loss = F.cross_entropy(logits_s, y) + W_LOGIT * F.mse_loss(logits_s, logits_t)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 5\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v5.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v5) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Logit Matching (L2) + CE\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcc591",
   "metadata": {
    "id": "ebfcc591"
   },
   "source": [
    "## Phương pháp 6: Soft-only KD (chỉ loss KD, bỏ CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880452c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e880452c",
    "outputId": "ee25de2a-38be-48f7-880b-b139844f4a1f"
   },
   "outputs": [],
   "source": [
    "# %% Train: Soft-only KD\n",
    "T = 4.0\n",
    "LR = 0.1\n",
    "\n",
    "student = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s = student(x)\n",
    "            # Only KD loss (alpha=1)\n",
    "            p_s = F.log_softmax(logits_s / T, dim=1)\n",
    "            p_t = F.softmax(logits_t / T, dim=1)\n",
    "            loss = F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 6\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v6.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v6) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Soft-only KD\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552435b",
   "metadata": {
    "id": "9552435b"
   },
   "source": [
    "## Phương pháp 7: Relational Knowledge Distillation (RKD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb670f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55cb670f",
    "outputId": "04ce6059-0c1c-4436-fe81-35e7c6353938"
   },
   "outputs": [],
   "source": [
    "# %% Train: Relational Knowledge Distillation (RKD)\n",
    "# Reference: Park et al., CVPR 2019. We implement RKD with both distance and angle losses.\n",
    "# Idea: match relational structures between samples (pairwise distances and triplet angles) in feature space.\n",
    "\n",
    "LR = 0.05\n",
    "W_RKD_DIST = 25.0\n",
    "W_RKD_ANGLE = 50.0\n",
    "\n",
    "# Student wrapper to expose features before global pooling\n",
    "class StudentWithFeatRKD(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)         # N, 128, H, W\n",
    "        logits = self.classifier(f)  # N, C\n",
    "        return logits, f\n",
    "\n",
    "student = StudentWithFeatRKD(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Teacher feature hook (layer4 output)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Helper: global-average pool to vectors and L2-normalize\n",
    "def to_vec_norm(fm: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # fm: N, C, H, W -> N, C\n",
    "    v = F.adaptive_avg_pool2d(fm, 1).flatten(1)\n",
    "    v = F.normalize(v, dim=1, eps=eps)\n",
    "    return v\n",
    "\n",
    "# RKD: Distance loss (pairwise)\n",
    "def rkd_distance(z_s: torch.Tensor, z_t: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # pairwise Euclidean distances\n",
    "    with torch.no_grad():\n",
    "        # teacher pairwise distances normalized by mean\n",
    "        d_t = torch.cdist(z_t, z_t, p=2)\n",
    "        mean_t = d_t[d_t>0].mean().clamp_min(eps)\n",
    "        d_t = d_t / mean_t\n",
    "    d_s = torch.cdist(z_s, z_s, p=2)\n",
    "    mean_s = d_s[d_s>0].mean().clamp_min(eps)\n",
    "    d_s = d_s / mean_s\n",
    "    return F.smooth_l1_loss(d_s, d_t)\n",
    "\n",
    "# RKD: Angle loss (triplet angles)\n",
    "def rkd_angle(z_s: torch.Tensor, z_t: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # For each anchor i, compute vectors to j and k: v_ij, v_ik, then their angle via cosine\n",
    "    def angle_matrix(z: torch.Tensor) -> torch.Tensor:\n",
    "        # z: N, D\n",
    "        n = z.size(0)\n",
    "        # compute pairwise differences: v_ij = z_j - z_i -> shape (N,N,D)\n",
    "        diff = z.unsqueeze(1) - z.unsqueeze(0)\n",
    "        # normalize along D\n",
    "        diff = F.normalize(diff, dim=2, eps=eps)\n",
    "        # cosine between v_ij and v_ik for all (j,k): cos = v_ij · v_ik\n",
    "        # angle tensor A where A[i,j,k] = cos(angle_jik)\n",
    "        A = torch.einsum('ijd,ikd->ijk', diff, diff)\n",
    "        return A\n",
    "    with torch.no_grad():\n",
    "        A_t = angle_matrix(z_t)\n",
    "    A_s = angle_matrix(z_s)\n",
    "    return F.smooth_l1_loss(A_s, A_t)\n",
    "\n",
    "optimizer = optim.SGD(student.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook\n",
    "            f_t = hook_t.feat\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)\n",
    "            # CE term for classification\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            # RKD on pooled features\n",
    "            z_s = to_vec_norm(f_s)\n",
    "            z_t = to_vec_norm(f_t)\n",
    "            loss_dist = rkd_distance(z_s, z_t)\n",
    "            loss_ang = rkd_angle(z_s, z_t)\n",
    "            loss = loss_ce + W_RKD_DIST * loss_dist + W_RKD_ANGLE * loss_ang\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = copy.deepcopy(student.state_dict())\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state)\n",
    "\n",
    "# Save student checkpoint for method 7 (RKD)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v7.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v7 RKD) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Relational Knowledge Distillation (RKD)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dx7AI67eYJwI",
   "metadata": {
    "id": "Dx7AI67eYJwI"
   },
   "source": [
    "## Phương pháp 8: Contrastive Representation Distillation (CRD, in-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zzaGwMl_UrqN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzaGwMl_UrqN",
    "outputId": "c046f154-df8a-4f47-fd2f-f2c633309eaa"
   },
   "outputs": [],
   "source": [
    "# %% Train: Contrastive Representation Distillation (CRD, in-batch)\n",
    "# Ref: Tian et al., ICLR 2020. We implement a lightweight in-batch CRD:\n",
    "#   - Take penultimate conv features from student and teacher\n",
    "#   - Project to a shared embedding space with small MLP heads\n",
    "#   - Use InfoNCE with in-batch negatives (z_s vs z_t of all samples)\n",
    "#   - Optimize CE + W_CRD * CRD\n",
    "\n",
    "LR = 0.1\n",
    "W_CRD = 1.0\n",
    "TAU = 0.07   # temperature for contrastive logits\n",
    "EMB_DIM = 128\n",
    "\n",
    "# Student wrapper to expose features\n",
    "class StudentWithFeatCRD(SmallNet):\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)         # N, 128, H, W\n",
    "        logits = self.classifier(f)  # N, C\n",
    "        return logits, f\n",
    "\n",
    "student = StudentWithFeatCRD(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Teacher feature hook (layer4)\n",
    "if not hasattr(teacher, 'layer4'):\n",
    "    raise RuntimeError(\"Teacher doesn't have layer4; choose a ResNet-like teacher.\")\n",
    "\n",
    "hook_t = FeatureHook(teacher.layer4)\n",
    "\n",
    "# Projection heads (trainable). Teacher backbone is frozen, but this head is trainable.\n",
    "proj_s = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128, EMB_DIM),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(EMB_DIM, EMB_DIM),\n",
    ").to(DEVICE)\n",
    "\n",
    "proj_t = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, EMB_DIM),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(EMB_DIM, EMB_DIM),\n",
    ").to(DEVICE)\n",
    "\n",
    "# Optimizer includes student + projection heads\n",
    "optimizer = optim.SGD(list(student.parameters()) + list(proj_s.parameters()) + list(proj_t.parameters()),\n",
    "                      lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=KD_EPOCHS)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# Contrastive loss (InfoNCE) using in-batch negatives\n",
    "def crd_loss(emb_s: torch.Tensor, emb_t: torch.Tensor, tau: float = 0.07) -> torch.Tensor:\n",
    "    # Normalize\n",
    "    zs = F.normalize(emb_s, dim=1)\n",
    "    zt = F.normalize(emb_t, dim=1)\n",
    "    # Similarity logits: N x N\n",
    "    logits = (zs @ zt.t()) / tau\n",
    "    targets = torch.arange(logits.size(0), device=logits.device)\n",
    "    return F.cross_entropy(logits, targets)\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "start = time.time()\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student.train(); proj_s.train(); proj_t.train()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            _ = teacher(x)  # populate hook\n",
    "            f_t = hook_t.feat  # N, 512, Ht, Wt\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits_s, f_s = student(x)  # f_s: N, 128, Hs, Ws\n",
    "            # Project to embeddings (vector)\n",
    "            z_s = proj_s(f_s)\n",
    "            z_t = proj_t(f_t.detach())\n",
    "            loss_ce = F.cross_entropy(logits_s, y)\n",
    "            loss_con = crd_loss(z_s, z_t, tau=TAU)\n",
    "            loss = loss_ce + W_CRD * loss_con\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_correct += (logits_s.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    scheduler.step()\n",
    "    # Eval on validation using classifier head only\n",
    "    student.eval(); proj_s.eval(); proj_t.eval()\n",
    "    train_acc_epoch = total_correct / max(1, total_samples)\n",
    "    val_acc_epoch, _ = evaluate(student, val_loader, DEVICE)\n",
    "    if val_acc_epoch > best_val:\n",
    "        best_val = val_acc_epoch\n",
    "        best_state = {\n",
    "            'student': copy.deepcopy(student.state_dict()),\n",
    "            'proj_s': copy.deepcopy(proj_s.state_dict()),\n",
    "            'proj_t': copy.deepcopy(proj_t.state_dict()),\n",
    "        }\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} - train_acc: {train_acc_epoch:.4f} - val_acc: {val_acc_epoch:.4f}{tag}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if best_state is not None:\n",
    "    student.load_state_dict(best_state['student'])\n",
    "    proj_s.load_state_dict(best_state['proj_s'])\n",
    "    proj_t.load_state_dict(best_state['proj_t'])\n",
    "\n",
    "# Save student checkpoint for method 8\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"kd_student_v8.pth\")\n",
    "torch.save(student.state_dict(), ckpt_path)\n",
    "print(f\"Saved student checkpoint (v8) to {ckpt_path}\")\n",
    "\n",
    "train_acc_final, _ = evaluate(student, train_loader, DEVICE)\n",
    "val_acc_final, _ = evaluate(student, val_loader, DEVICE)\n",
    "test_acc_final, _ = evaluate(student, test_loader, DEVICE)\n",
    "print({\n",
    "    \"method\": \"Contrastive Representation Distillation (CRD)\",\n",
    "    \"train_time_sec\": round(elapsed, 2),\n",
    "    \"train_acc\": round(train_acc_final, 4),\n",
    "    \"val_acc\": round(val_acc_final, 4),\n",
    "    \"test_acc\": round(test_acc_final, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e2d2e",
   "metadata": {},
   "source": [
    "## Đánh giá toàn diện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4604ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Comprehensive Evaluation: Load checkpoints and compare models\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Expect checkpoints in CKPT_DIR\n",
    "ckpt_dir = CKPT_DIR if 'CKPT_DIR' in globals() else './checkpoints'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# Build teacher & student architectures (same as training)\n",
    "teacher_eval = build_teacher(NUM_CLASSES).to(DEVICE)\n",
    "student_eval = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Utility: evaluate metrics over a dataloader\n",
    "@torch.no_grad()\n",
    "def collect_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    logits_list, labels_list = [], []\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        out = model(x)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "        loss_sum += criterion(logits, y).item()\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        logits_list.append(logits.detach().cpu())\n",
    "        labels_list.append(y.detach().cpu())\n",
    "    logits_all = torch.cat(logits_list, dim=0)\n",
    "    labels_all = torch.cat(labels_list, dim=0)\n",
    "    acc = correct / max(1, total)\n",
    "    avg_loss = loss_sum / max(1, total)\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'loss': avg_loss,\n",
    "        'logits': logits_all,\n",
    "        'labels': labels_all,\n",
    "    }\n",
    "\n",
    "# Expected checkpoints\n",
    "models = OrderedDict([\n",
    "    (\"teacher\", os.path.join(ckpt_dir, \"kd_teacher.pth\")),\n",
    "    (\"v1_vanilla\", os.path.join(ckpt_dir, \"kd_student_v1.pth\")),\n",
    "    (\"v2_cee\", os.path.join(ckpt_dir, \"kd_student_v2.pth\")),\n",
    "    (\"v3_feat\", os.path.join(ckpt_dir, \"kd_student_v3.pth\")),\n",
    "    (\"v4_at\", os.path.join(ckpt_dir, \"kd_student_v4.pth\")),\n",
    "    (\"v5_logit\", os.path.join(ckpt_dir, \"kd_student_v5.pth\")),\n",
    "    (\"v6_soft\", os.path.join(ckpt_dir, \"kd_student_v6.pth\")),\n",
    "    (\"v7_rkd\", os.path.join(ckpt_dir, \"kd_student_v7.pth\")),\n",
    "    (\"v8_crd\", os.path.join(ckpt_dir, \"kd_student_v8.pth\")),\n",
    "])\n",
    "\n",
    "# Load teacher\n",
    "loaded = {}\n",
    "if os.path.isfile(models['teacher']):\n",
    "    teacher_eval.load_state_dict(torch.load(models['teacher'], map_location=DEVICE))\n",
    "    loaded['teacher'] = teacher_eval\n",
    "else:\n",
    "    print(f\"[WARN] Teacher checkpoint not found: {models['teacher']}\")\n",
    "\n",
    "# Load students into dict (same arch SmallNet)\n",
    "for name, path in models.items():\n",
    "    if name == 'teacher':\n",
    "        continue\n",
    "    if os.path.isfile(path):\n",
    "        m = SmallNet(NUM_CLASSES).to(DEVICE)\n",
    "        m.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "        loaded[name] = m\n",
    "    else:\n",
    "        print(f\"[WARN] Student checkpoint not found: {path}\")\n",
    "\n",
    "# Metrics to compute\n",
    "# - acc_test: accuracy on test set\n",
    "# - loss_test: CE loss on test set\n",
    "# - ece: Expected Calibration Error (10-bin)\n",
    "# - agree_t: agreement rate between student and teacher predictions\n",
    "# - kl_to_t: KL(student || teacher) on test logits (softmax distributions)\n",
    "# - cos_logits: cosine similarity between student and teacher logits\n",
    "# - entropy: average predictive entropy (uncertainty)\n",
    "\n",
    "\n",
    "def expected_calibration_error(probs: torch.Tensor, labels: torch.Tensor, n_bins: int = 10) -> float:\n",
    "    # probs: N,C ; labels: N\n",
    "    confidences, predictions = probs.max(dim=1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "    bins = torch.linspace(0, 1, steps=n_bins + 1)\n",
    "    ece = torch.zeros(1)\n",
    "    for i in range(n_bins):\n",
    "        in_bin = (confidences > bins[i]) & (confidences <= bins[i + 1]) if i < n_bins - 1 else (confidences > bins[i]) & (confidences <= bins[i + 1])\n",
    "        prop = in_bin.float().mean()\n",
    "        if prop.item() > 0:\n",
    "            acc_bin = accuracies[in_bin].float().mean()\n",
    "            conf_bin = confidences[in_bin].float().mean()\n",
    "            ece += torch.abs(conf_bin - acc_bin) * prop\n",
    "    return ece.item()\n",
    "\n",
    "\n",
    "def kl_divergence(p_logits: torch.Tensor, q_logits: torch.Tensor, T: float = 1.0) -> float:\n",
    "    # KL(P||Q) with temperature T\n",
    "    p = F.log_softmax(p_logits / T, dim=1)\n",
    "    q = F.softmax(q_logits / T, dim=1)\n",
    "    return F.kl_div(p, q, reduction='batchmean').item()\n",
    "\n",
    "\n",
    "def cosine_similarity_logits(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    a_flat = a.flatten(1)\n",
    "    b_flat = b.flatten(1)\n",
    "    a_n = F.normalize(a_flat, p=2, dim=1)\n",
    "    b_n = F.normalize(b_flat, p=2, dim=1)\n",
    "    return (a_n * b_n).sum(dim=1).mean().item()\n",
    "\n",
    "\n",
    "def avg_entropy_from_logits(logits: torch.Tensor) -> float:\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    entropy = -(probs * (probs.clamp_min(1e-12).log())).sum(dim=1)\n",
    "    return entropy.mean().item()\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "metrics = [\n",
    "    'acc', 'loss', 'ece', 'agree_t', 'kl_to_t', 'cos_logits', 'entropy'\n",
    "]\n",
    "\n",
    "# First collect teacher outputs\n",
    "teacher_out = None\n",
    "if 'teacher' in loaded:\n",
    "    teacher_out = collect_metrics(loaded['teacher'], test_loader, DEVICE)\n",
    "\n",
    "for name, model in loaded.items():\n",
    "    out = collect_metrics(model, test_loader, DEVICE)\n",
    "    probs = F.softmax(out['logits'], dim=1)\n",
    "    ece = expected_calibration_error(probs, out['labels'])\n",
    "\n",
    "    # Comparisons to teacher (only if teacher available and current is not teacher)\n",
    "    if teacher_out is not None and name != 'teacher':\n",
    "        agree = (out['logits'].argmax(1) == teacher_out['logits'].argmax(1)).float().mean().item()\n",
    "        kl = kl_divergence(out['logits'], teacher_out['logits'])\n",
    "        cos = cosine_similarity_logits(out['logits'], teacher_out['logits'])\n",
    "    else:\n",
    "        agree, kl, cos = np.nan, np.nan, np.nan\n",
    "\n",
    "    ent = avg_entropy_from_logits(out['logits'])\n",
    "\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'acc': round(out['acc'], 4),\n",
    "        'loss': round(out['loss'], 4),\n",
    "        'ece': round(ece, 4),\n",
    "        'agree_t': round(agree, 4) if not np.isnan(agree) else np.nan,\n",
    "        'kl_to_t': round(kl, 4) if not np.isnan(kl) else np.nan,\n",
    "        'cos_logits': round(cos, 4) if not np.isnan(cos) else np.nan,\n",
    "        'entropy': round(ent, 4),\n",
    "    })\n",
    "\n",
    "# Build results DataFrame (sorted by model order above)\n",
    "df = pd.DataFrame(results)\n",
    "# Optional: reorder rows to keep teacher first\n",
    "order = [k for k in models.keys() if k in df['model'].values]\n",
    "df['order_idx'] = df['model'].apply(lambda m: order.index(m) if m in order else 999)\n",
    "df = df.sort_values('order_idx').drop(columns=['order_idx']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n===== Bảng kết quả (metrics) =====\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Ranking table: for each metric, compute rank (best rank = 1).\n",
    "# For metrics where lower is better (loss, ece, kl, entropy), rank ascending. For higher-better (acc, agree_t, cos), rank descending.\n",
    "rank_prefs = {\n",
    "    'acc': 'desc',\n",
    "    'loss': 'asc',\n",
    "    'ece': 'asc',\n",
    "    'agree_t': 'desc',\n",
    "    'kl_to_t': 'asc',\n",
    "    'cos_logits': 'desc',\n",
    "    'entropy': 'asc',\n",
    "}\n",
    "\n",
    "rank_df = df.copy()\n",
    "for col, pref in rank_prefs.items():\n",
    "    series = rank_df[col]\n",
    "    if series.isna().all():\n",
    "        rank_df[col + '_rank'] = np.nan\n",
    "        continue\n",
    "    # For NaN values (e.g., teacher comparisons), assign worst rank\n",
    "    fill_val = series.max() + 1 if pref == 'asc' else series.min() - 1\n",
    "    series_filled = series.fillna(fill_val)\n",
    "    ascending = (pref == 'asc')\n",
    "    rank_df[col + '_rank'] = series_filled.rank(method='min', ascending=ascending)\n",
    "\n",
    "# Keep only rank columns and model name\n",
    "rank_cols = ['model'] + [c + '_rank' for c in rank_prefs.keys()]\n",
    "rank_table = rank_df[rank_cols]\n",
    "rank_table['avg_rank'] = rank_table[[c for c in rank_cols if c != 'model']].mean(axis=1)\n",
    "\n",
    "print(\"\\n===== Bảng xếp hạng (rank) =====\")\n",
    "print(rank_table.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
